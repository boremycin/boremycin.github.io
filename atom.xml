<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Boremycin&#39;s Blog</title>
  
  
  <link href="http://boremycin.github.io/atom.xml" rel="self"/>
  
  <link href="http://boremycin.github.io/"/>
  <updated>2025-12-20T07:22:08.834Z</updated>
  <id>http://boremycin.github.io/</id>
  
  <author>
    <name>Boremycin</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>数据集分布相似度量化</title>
    <link href="http://boremycin.github.io/2025/12/20/code7/"/>
    <id>http://boremycin.github.io/2025/12/20/code7/</id>
    <published>2025-12-20T07:19:54.563Z</published>
    <updated>2025-12-20T07:22:08.834Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">dataset similarity measure调研与简单复现</p><br><span id="more"></span></p><h1 id="基于metafeature的距离与相似度度量"><a href="#基于metafeature的距离与相似度度量" class="headerlink" title="基于metafeature的距离与相似度度量"></a>基于metafeature的距离与相似度度量</h1><p>Feuer et al.[1]提出使用meta-learning来有效初始化Sequential Model-Based Bayesian Optimization(SMBO)过程. 所设定的前提是在待度量的数据集集合中的每个数据集$D^{(i)}$都可以用K个metafeature描述, 即 $m^i = (m^i_1, m^i_2, … \ ,m_k^i)$. 作者进而提出两种距离度量方法, 一种是直接度量metafeature集的距离作为数据集之间的距离:</p><script type="math/tex; mode=display">D_{Fq}(D^{(i)}, D^{(j)}) = \| m^i - m^j \|_q \tag{1}</script><p>第二种度量方法通过negative Spearman correlation度量不同超参数设定的模型性能并将其作为metafeature集合, 对应表达形式如下:</p><script type="math/tex; mode=display">D_{Fc}(D^{(i)}, D^{(j)}) = 1 - \mathcal{Cor}([g^{D^{(i)}}(\theta_1), g^{D^{(i)}}(\theta_2), ... , g^{D^{(i)}}(\theta_n)],[g^{D^{(j)}}(\theta_1), g^{D^{(j)}}(\theta_2), ... , g^{D^{(j)}}(\theta_n)]) \tag{2}</script><p>上式中$g^{D^{(i)}}(\cdot)$代表含参目标函数. 当需要在多个数据集中匹配最相似的数据集时, 由于参数无法固定下来, 因此需要借助mapping function在metafeature对$(m^i, m^j)$和$D_{Fc}(D^{(i)}, D^{(j)})$之间进行回归. 常用的metafeature可以归为以下几个大类:</p><ul><li>用于描述或者量化数据集的简单特征</li><li>PCA metafeatures</li><li>数据集的熵值</li><li>统计特征, 包括数据集的峰度(kurtosis)或者标签的分布宽度(dispersion of label distribution)</li><li>landmarking metafeatures(通过快速的ML算法获得, 如linear separability)</li></ul><p>上述中的峰度kurtosis是指用于描述概率分布曲线在平均值处峰值高低以及尾部厚度的统计量, 用于衡量数据特征的离散程度和异常值的存在情况. 对应数学表达如下:</p><script type="math/tex; mode=display">K = \frac{\mu_4}{\sigma^4} = \frac{E[X - \mu]^4}{(E[(X-\mu)^2])^2} \tag{3}</script><p>上式中X为特征变量, $\mu$是均值, $E$是期望算子, $\sigma$是标准差, $\mu_4$是4阶中心矩. </p><p>标签分布宽度则描述了类别分布的平衡程度或离散程度, 可以用类别概率标准差衡量, 对应数学表达形式如下:</p><script type="math/tex; mode=display">\sigma_{labels} = \sqrt{\frac{1}{C}\sum_{i=1}^C(p_i - \frac{1}{C})^2} \tag{4}</script><p>上式中C是类别总数, $p_i$是第i类的样本比例. </p><p>此外, 式(2)中所用的Spearman相关系数为范围在[-1,1]间的统计值, 用于度量两组排序(ranks)结果的相关强度, 该系数可以用于衡量数据和模型的契合程度, 对于n对原始分数对$(X_i, Y_i)$, 将其转换为ranks$R[X_i], R[Y_i]$, 令$R[X_i], R[Y_i] = (R_i, S_i)$, 对应的相关系数计算如下:</p><script type="math/tex; mode=display">r_s = \rho[R[X], R[Y]] = \frac{cov[R[X], R[Y]]}{\sigma_{R[X]} \cdot \sigma_{R[Y]}} = \frac{\frac{1}{n} \sum_{i=1}^n R_i S_i - \bar{R}\bar{S}}{\sigma_{R[X]} \cdot \sigma_{R[Y]}}      \tag{5}</script><p>上式中的rank不是矩阵的秩, 给定$\{X_1, X_2, …, X_n \}$, 对其进行排序后rank的定义为$R(X_i)$为在排序序列中$X_i$的位置. 当不存在并列的情况时, 表达式可简化为:</p><script type="math/tex; mode=display">r_s = 1 - \frac{6 \sum_{i=1}^{n}d_i^2}{n(n^2-1)}\tag{6}</script><p>以SAMPLE数据集以及光学的CIFAR10为例, 通过以下代码可以初步计算real和synth子数据集和光学CIFAR10数据集的峰度和标签分布差异.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ImageClassificationDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root_dir, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.root_dir = root_dir</span><br><span class="line">        <span class="variable language_">self</span>.transform = transform</span><br><span class="line">        <span class="variable language_">self</span>.classes = <span class="built_in">sorted</span>(os.listdir(root_dir))</span><br><span class="line">        <span class="variable language_">self</span>.class_to_idx = &#123;cls: idx <span class="keyword">for</span> idx, cls <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="variable language_">self</span>.classes)&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="variable language_">self</span>.samples = []</span><br><span class="line">        <span class="keyword">for</span> class_name <span class="keyword">in</span> <span class="variable language_">self</span>.classes:</span><br><span class="line">            class_dir = os.path.join(root_dir, class_name)</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(class_dir):</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            <span class="keyword">for</span> file_name <span class="keyword">in</span> os.listdir(class_dir):</span><br><span class="line">                file_path = os.path.join(class_dir, file_name)</span><br><span class="line">                <span class="variable language_">self</span>.samples.append((file_path, <span class="variable language_">self</span>.class_to_idx[class_name]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(<span class="variable language_">self</span>.samples)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        img_path, label = <span class="variable language_">self</span>.samples[idx]</span><br><span class="line">        image = Image.<span class="built_in">open</span>(img_path).convert(<span class="string">&#x27;RGB&#x27;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.transform:</span><br><span class="line">            image = <span class="variable language_">self</span>.transform(image)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> image, label</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_features_from_images</span>(<span class="params">dataset, batch_size=<span class="number">32</span>, num_workers=<span class="number">4</span></span>):</span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    all_pixels = []</span><br><span class="line">    all_labels = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">        pixels = images.view(images.size(<span class="number">0</span>), -<span class="number">1</span>) <span class="comment"># 将图像展平为像素值向量</span></span><br><span class="line">        all_pixels.append(pixels)</span><br><span class="line">        all_labels.append(labels)</span><br><span class="line">    </span><br><span class="line">    pixel_data = torch.cat(all_pixels, dim=<span class="number">0</span>)  <span class="comment"># [n_samples, n_pixels]</span></span><br><span class="line">    label_data = torch.cat(all_labels, dim=<span class="number">0</span>)  <span class="comment"># [n_samples]</span></span><br><span class="line">    <span class="keyword">return</span> pixel_data, label_data</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calculate_kurtosis</span>(<span class="params">tensor</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    tensor: shape [n_samples, n_features]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    mean = torch.mean(tensor, dim=<span class="number">0</span>)</span><br><span class="line">    std = torch.std(tensor, dim=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    standardized = (tensor - mean) / (std + <span class="number">1e-8</span>)</span><br><span class="line">    fourth_moment = torch.mean(standardized ** <span class="number">4</span>, dim=<span class="number">0</span>)</span><br><span class="line">    kurtosis = fourth_moment - <span class="number">3</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> kurtosis</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">analyze_label_distribution</span>(<span class="params">labels</span>):</span><br><span class="line">    unique_labels = torch.unique(labels)</span><br><span class="line">    num_unique_labels = <span class="built_in">len</span>(unique_labels)</span><br><span class="line">    label_counts = torch.bincount(labels)</span><br><span class="line"></span><br><span class="line">    distribution_std = torch.std(label_counts.<span class="built_in">float</span>())</span><br><span class="line">    </span><br><span class="line">    probabilities = label_counts.<span class="built_in">float</span>() / torch.<span class="built_in">sum</span>(label_counts)</span><br><span class="line">    entropy = -torch.<span class="built_in">sum</span>(probabilities * torch.log(probabilities + <span class="number">1e-8</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;num_unique_labels&#x27;</span>: num_unique_labels,</span><br><span class="line">        <span class="string">&#x27;label_counts&#x27;</span>: label_counts,</span><br><span class="line">        <span class="string">&#x27;distribution_std&#x27;</span>: distribution_std,</span><br><span class="line">        <span class="string">&#x27;entropy&#x27;</span>: entropy,</span><br><span class="line">        <span class="string">&#x27;probabilities&#x27;</span>: probabilities</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">analyze_dataset</span>(<span class="params">dataset_path, img_size=<span class="number">224</span>, batch_size=<span class="number">256</span></span>):</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((img_size, img_size)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">    ])</span><br><span class="line">    dataset = ImageClassificationDataset(dataset_path, transform=transform)</span><br><span class="line">    </span><br><span class="line">    pixel_data, label_data = extract_features_from_images(dataset, batch_size)  <span class="comment"># 提取特征和标签</span></span><br><span class="line">    </span><br><span class="line">    sample_indices = torch.randperm(pixel_data.shape[<span class="number">1</span>])[:<span class="number">10000</span>]  <span class="comment"># 计算峰度 (采样部分像素以减少计算量)</span></span><br><span class="line">    sampled_pixels = pixel_data[:, sample_indices]</span><br><span class="line">    kurtosis_values = calculate_kurtosis(sampled_pixels)</span><br><span class="line"></span><br><span class="line">    label_stats = analyze_label_distribution(label_data)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;kurtosis_mean&#x27;</span>: torch.mean(kurtosis_values),</span><br><span class="line">        <span class="string">&#x27;kurtosis_std&#x27;</span>: torch.std(kurtosis_values),</span><br><span class="line">        <span class="string">&#x27;label_distribution_std&#x27;</span>: label_stats[<span class="string">&#x27;distribution_std&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;label_entropy&#x27;</span>: label_stats[<span class="string">&#x27;entropy&#x27;</span>],</span><br><span class="line">        <span class="string">&#x27;num_classes&#x27;</span>: <span class="built_in">len</span>(dataset.classes),</span><br><span class="line">        <span class="string">&#x27;total_samples&#x27;</span>: <span class="built_in">len</span>(dataset)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    synth_dataset_path = <span class="string">&quot;./synth&quot;</span>  </span><br><span class="line">    real_dataset_path = <span class="string">&quot;./real&quot;</span>  </span><br><span class="line">    cifar10_dataset_path = <span class="string">&#x27;./cifar10&#x27;</span>  </span><br><span class="line">    synth_stats = analyze_dataset(synth_dataset_path)</span><br><span class="line">    real_stats = analyze_dataset(real_dataset_path)</span><br><span class="line">    cifar10_stats = analyze_dataset(cifar10_dataset_path)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;result&#x27;</span>:&lt;<span class="number">20</span>&#125;</span> <span class="subst">&#123;<span class="string">&#x27;Synth&#x27;</span>:&lt;<span class="number">15</span>&#125;</span> <span class="subst">&#123;<span class="string">&#x27;Real&#x27;</span>:&lt;<span class="number">15</span>&#125;</span> <span class="subst">&#123;<span class="string">&#x27;CIFAR10&#x27;</span>:&lt;<span class="number">15</span>&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-&quot;</span> * <span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;kurtosis_mean&#x27;</span>:&lt;<span class="number">20</span>&#125;</span> <span class="subst">&#123;synth_stats[<span class="string">&#x27;kurtosis_mean&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span> <span class="subst">&#123;real_stats[<span class="string">&#x27;kurtosis_mean&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span> <span class="subst">&#123;cifar10_stats[<span class="string">&#x27;kurtosis_mean&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;label_std&#x27;</span>:&lt;<span class="number">20</span>&#125;</span> <span class="subst">&#123;synth_stats[<span class="string">&#x27;label_distribution_std&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span> <span class="subst">&#123;real_stats[<span class="string">&#x27;label_distribution_std&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span> <span class="subst">&#123;cifar10_stats[<span class="string">&#x27;label_distribution_std&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;<span class="string">&#x27;label_entropy&#x27;</span>:&lt;<span class="number">20</span>&#125;</span> <span class="subst">&#123;synth_stats[<span class="string">&#x27;label_entropy&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span> <span class="subst">&#123;real_stats[<span class="string">&#x27;label_entropy&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span> <span class="subst">&#123;cifar10_stats[<span class="string">&#x27;label_entropy&#x27;</span>]:&lt;<span class="number">15.4</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>特征结果如下所示<br><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">result               Synth           Real            CIFAR10        </span></span><br><span class="line"><span class="section">-------------------------------------------------------------</span></span><br><span class="line">kurtosis<span class="emphasis">_mean        1.1260          0.9982          -0.7343        </span></span><br><span class="line"><span class="emphasis">label_std            30.2811         30.2811         0.0000</span></span><br><span class="line"><span class="emphasis">label_</span>entropy        2.2800          2.2800          2.3026</span><br></pre></td></tr></table></figure></p><h1 id="Gromov-Wasserstein距离"><a href="#Gromov-Wasserstein距离" class="headerlink" title="Gromov-Wasserstein距离"></a>Gromov-Wasserstein距离</h1><p>文献[2]提出基于Gromov-Wasserstein metric来度量数据集之间的相关程度, 对应数学表达如下所示:</p><script type="math/tex; mode=display">d_{GW}(\mathcal{X}, \mathcal{Y}) = \inf_{\pi \in \Pi(\mu_{\mathcal{X}}, \mu_{\mathcal{Y}})}   \int_{\mathcal{X}^2 \times \mathcal{Y}^2} |d_{\mathcal{X}}(x, x') -d_{\mathcal{Y}}(y, y')|^q \cdot d\pi(x,y)d\pi(x',y') \tag{7}</script><p>上式实际度量了两个概率空间$\mathcal{X}$和$\mathcal{Y}$之间的距离, 其中的$\mathcal{X}$和$\mathcal{Y}$不仅包含距离结构$d_{\mathcal{X}}$, $d_{\mathcal{Y}}$, 还带有概率测度$\mu_{\mathcal{X}}$和$\mu_{\mathcal{Y}}$. $\pi$为最优传输理论中的联合概率测度, 对应的$\Pi(\mu_{\mathcal{X}}, \mu_{\mathcal{Y}})$是所有满足边缘约束的耦合集合. 当用于度量给定的数据集$\mathcal{X}=\{x_1, …, x_n \}$和$\mathcal{Y}=\{y_1, …, y_m\}$时, 对于度量空间$(\mathcal{X}, d_{\mathcal{X}})$的定义,  $d_{\mathcal{X}}:\mathcal{X} \times \mathcal{X} \rightarrow \mathbb{R}_+$可以表示为如下形式:</p><script type="math/tex; mode=display">d_{\mathcal{X}}(x_i, x_j) =  \| \phi(x_i) - \phi(x_j) \|_2 \tag{8}</script><p>其中的$\phi(\cdot)$为特征提取器. 对于概率测度$\mu_{\mathcal{X}}$和$\mu_{\mathcal{Y}}$, 在有限数据集上使用经验分布:</p><script type="math/tex; mode=display">\mu_{\mathcal{X}} = \frac{1}{n}\sum_{i=1}^n \delta_{x_i}, \quad \mu_{\mathcal{Y}} = \frac{1}{m}\sum_{j=1}^m \delta_{y_j} \tag{9}</script><p>其中的$\delta$为狄拉克测度, 可以简单理解为每个$x_i$在数据集中的比例. 联合概率测度$\pi \in \Pi(\mu_{\mathcal{X}}, \mu_{\mathcal{Y}})$则表示一个软匹配关系, 对应表达形式如下:</p><script type="math/tex; mode=display">\pi_{ij} \geq 0, \quad  \sum_j \pi_{ij} = \frac{1}{n}, \quad \sum_i \pi_{ij} = \frac{1}{m} \tag{10}</script><p>GW距离并非衡量数据集中的点是否接近, 而是两个数据集的内部集合结构是否一致. 同上, 借助POT库(python optimal transport)可以实现GW的计算与可视化, 同样分别比较real-synth和real-cifar10, 代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader, Dataset</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy <span class="keyword">as</span> sp</span><br><span class="line"><span class="keyword">import</span> ot</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> TSNE</span><br><span class="line"><span class="keyword">from</span> method1 <span class="keyword">import</span> ImageClassificationDataset <span class="comment">#同上</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_features_from_images</span>(<span class="params">dataset, batch_size=<span class="number">32</span>, num_workers=<span class="number">4</span></span>):</span><br><span class="line">    dataloader = DataLoader(dataset, batch_size=batch_size, </span><br><span class="line">                          shuffle=<span class="literal">False</span>, num_workers=num_workers)</span><br><span class="line">    all_pixels = []</span><br><span class="line">    all_labels = []</span><br><span class="line">    <span class="keyword">for</span> images, labels <span class="keyword">in</span> dataloader:</span><br><span class="line">        pixels = images.view(images.size(<span class="number">0</span>), -<span class="number">1</span>)  <span class="comment"># 将图像展平为像素值向量</span></span><br><span class="line">        all_pixels.append(pixels)</span><br><span class="line">        all_labels.append(labels)</span><br><span class="line">    pixel_data = torch.cat(all_pixels, dim=<span class="number">0</span>)  <span class="comment"># [n_samples, n_pixels]</span></span><br><span class="line">    label_data = torch.cat(all_labels, dim=<span class="number">0</span>)  <span class="comment"># [n_samples]</span></span><br><span class="line">    <span class="keyword">return</span> pixel_data.numpy(), label_data.numpy() </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_dataset_features</span>(<span class="params">dataset_path, img_size=<span class="number">32</span>, batch_size=<span class="number">256</span></span>):</span><br><span class="line">    transform = transforms.Compose([</span><br><span class="line">        transforms.Resize((img_size, img_size)),</span><br><span class="line">        transforms.ToTensor(),</span><br><span class="line">    ])</span><br><span class="line">    dataset = ImageClassificationDataset(dataset_path, transform=transform)</span><br><span class="line">    features, labels = extract_features_from_images(dataset, batch_size)</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_and_visualize_gw_with_features</span>(<span class="params">real_path, synth_path, feature_method=<span class="string">&#x27;pixel&#x27;</span>, img_size=<span class="number">128</span></span>):</span><br><span class="line">    real_features, real_labels = load_dataset_features(real_path, img_size)</span><br><span class="line">    synth_features, synth_labels = load_dataset_features(synth_path, img_size)</span><br><span class="line">    </span><br><span class="line">    C1 = sp.spatial.distance.cdist(real_features, real_features)</span><br><span class="line">    C2 = sp.spatial.distance.cdist(synth_features, synth_features)</span><br><span class="line">    C1 /= C1.<span class="built_in">max</span>()</span><br><span class="line">    C2 /= C2.<span class="built_in">max</span>()</span><br><span class="line">    </span><br><span class="line">    n_real, n_synth = <span class="built_in">len</span>(real_features), <span class="built_in">len</span>(synth_features)</span><br><span class="line">    p = ot.unif(n_real)</span><br><span class="line">    q = ot.unif(n_synth)</span><br><span class="line">    </span><br><span class="line">    gw, log = ot.gromov.gromov_wasserstein(</span><br><span class="line">        C1, C2, p, q, <span class="string">&quot;square_loss&quot;</span>, verbose=<span class="literal">True</span>, log=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    plt.figure(figsize=(<span class="number">8</span>, <span class="number">6</span>))</span><br><span class="line">    plt.imshow(gw, cmap=<span class="string">&#x27;Blues&#x27;</span>)</span><br><span class="line">    plt.title(<span class="string">f&#x27;GW Transport Matrix (<span class="subst">&#123;feature_method&#125;</span>)\nGW Distance: <span class="subst">&#123;log[<span class="string">&quot;gw_dist&quot;</span>]:<span class="number">.6</span>f&#125;</span>&#x27;</span>)</span><br><span class="line">    plt.colorbar()</span><br><span class="line">    plt.show()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">        <span class="string">&#x27;distance&#x27;</span>: log[<span class="string">&quot;gw_dist&quot;</span>],</span><br><span class="line">        <span class="string">&#x27;transport_plan&#x27;</span>: gw,</span><br><span class="line">        <span class="string">&#x27;real_features&#x27;</span>: real_features,</span><br><span class="line">        <span class="string">&#x27;synth_features&#x27;</span>: synth_features</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    results = compute_and_visualize_gw_with_features(<span class="string">&quot;./real&quot;</span>, <span class="string">&quot;./synth&quot;</span>)</span><br><span class="line">    results2 = compute_and_visualize_gw_with_features(<span class="string">&quot;./real&quot;</span>, <span class="string">&quot;./cifar10&quot;</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;\n&quot;</span> + <span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;Gromov-Wasserstein Distance Results:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;=&quot;</span>*<span class="number">50</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GW distance: <span class="subst">&#123;results[<span class="string">&#x27;distance&#x27;</span>]:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;GW distance: <span class="subst">&#123;results2[<span class="string">&#x27;distance&#x27;</span>]:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><p>在分析实验结果之前, 首先对图像统计特征进行可视化. 将特征提取器设定为对图形简单一维展平, 获取特征集合的特征值分布图像和不同维度的方差分布, 结果从上至下分别为real-synth和real-cifar10, 下同. 对应可视化结果如下:</p><figure style="text-align: center;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_1.png" alt="Figure_1" style="display: block; margin: 0 auto; width: 700px; height: auto;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_1_cifar.png" alt="Figure_1_cifar" style="display: block; margin: 0 auto; width: 700px; height: auto;"></figure><p>接着, 使用PCA对图像进行降维可视化, 分别按数据集和按类别进行着色区分, 可视化结果如下:</p><figure style="text-align: center;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_2.png" alt="Figure_1" style="display: block; margin: 0 auto; width: 700px; height: auto;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_2_cifar.png" alt="Figure_1_cifar" style="display: block; margin: 0 auto; width: 700px; height: auto;"></figure><p>最终, 用t-SNE对展平的一维特征进行可视化处理, 对应结果如下所示:</p><figure style="text-align: center;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_3.png" alt="Figure_1" style="display: block; margin: 0 auto; width: 700px; height: auto;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_3_cifar.png" alt="Figure_1_cifar" style="display: block; margin: 0 auto; width: 700px; height: auto;"></figure><p>接着使用GW距离进行数据集相似度统计, 首先可以对代价矩阵(即由$d_{\mathcal{X}}$或$d_{\mathcal{Y}}$构成的元数据点两两之间距离匹配矩阵)进行可视化, 对应结果如下:</p><figure style="text-align: center;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_4.png" alt="Figure_1" style="display: block; margin: 0 auto; width: 700px; height: auto;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/Figure_4_cifar10.png" alt="Figure_1_cifar" style="display: block; margin: 0 auto; width: 700px; height: auto;"></figure><p>运行算法可得, 最终不同数据集对之间的GW距离结果如下:</p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">==================================================</span><br><span class="line">Gromov-Wasserstein Distance Results:</span><br><span class="line">==================================================</span><br><span class="line">GW distance: 0.003756</span><br><span class="line">GW distance: 0.011917</span><br></pre></td></tr></table></figure><h1 id="Deep-Dataset-Dissimliarity-Measures-DeDiMs"><a href="#Deep-Dataset-Dissimliarity-Measures-DeDiMs" class="headerlink" title="Deep Dataset Dissimliarity Measures(DeDiMs)"></a>Deep Dataset Dissimliarity Measures(DeDiMs)</h1><p>文献[3]提出了一种融合数据和标签分布以及半监督深度学习的数据分布匹配评估方法, 主要通过设计4个特殊的距离函数实现, 给定两个数据集$D^a$和$D^b$, 对应步骤如下:</p><ol><li>对数据集进行随机采样, 在系数$\tau$的约束下获得两个对应的子集, 称为$D^{a,\tau}$和$D^{b,\tau}$</li><li>通过特征提取器(feature extractor)获得子数据集对应的特征向量集$H^{a,\tau}$和$H^{b,\tau}$</li></ol><p>完成上述两个步骤后, 首先需要计算 Minkowski-based distance set, 对应的过程如下:</p><ol><li>选取Manhattan距离或欧式距离(q-norm取1或2)计算距离$d_i=min_k |h_i - h_k |_q$, $h_i$为从$H^{a, \tau}$中采集C个样本构成, $h_k$是$H^{b, \tau}$中与之最接近的向量, 通过该步骤获取一个距离列表$d_{l_q}(D^a,D^b, \tau, C) = \{ \hat{d_1}, …,\hat{d_C}\}$</li><li>类似步骤1, 计算$D^a$的组内距离, 获得参考距离列表(reference distance list)$d_{l_q}(D^a,D^a, \tau, C) = \{ \check{d_1}, …,\check{d_C}\}$</li><li>计算上述两个距离列表的差值绝对值, 同时计算各个距离列表与自身均值的离散分布以及Wilcoxon test的p-value</li></ol><p>上述方法可以视为基于比较密度方程(comparing density function)的数据集相似度评估算法, 计算过程同时评估的数据集内和不同数据集之间的距离差异, 可以更好地反映数据集之间的分布差异并提升半监督深度学习(semi-supervised deep learning ,SSDL)的性能.</p><p>此外, 上述的Minkowski distance, 也被称为Minkowski metric, 是一种正则化向量空间, 可以视为曼哈顿距离和欧式距离的推广. 对于两个向量集合$X = (x_1, x_2, …, x_n)$和$Y = (y_1, y_2, …, y_n)$, 对应计算公式如下:</p><script type="math/tex; mode=display">D(X,Y)=(\sum_{i=1}^n |x_i - y_i|^{p})^{\frac{1}{p}} \tag{11}</script><p>该距离的设计启发来自Minkowski不等式, 通常p被设定为1或2, 分别对应曼哈顿距离和欧式距离. 闵氏距离可以理解为两个向量集合之间的能量平均. 对于Wilcoxson test, 有两种常见的检验定义, 分别为Wilcoxon Signed-Rank Test(符号秩检验)和Wilcoxon Rank-Sum Test(秩和检验), 前者基于成对样本, 后者则适用于独立样本, 没有一一对应关系. 此处采用的是Wilcoxon符号检验.  </p><p>Wilcoxon符号秩检验是一种非参数假设检验方法, 用于比较两个配对样本是否有显著差异. 这里的秩是指数据绝对值大小所带来的排名. 对于两组距离列表, 首先计算每对数据的差值$D_i = x_i - y_i$, 当$D_i = 0$时会将该数据从分析中提出并对样本总数-1; 接着计算绝对值的秩, 对所有非零差值绝对值进行排序并从小至大赋予秩次$R_i$.</p><script type="math/tex; mode=display">R_i = Rank(|D_i|) \tag{12}</script><p>此外需要定义相应的符号函数$sgn(\cdot)$以规定差值的方向, 对应表达形式如下:</p><script type="math/tex; mode=display">sgn(D_i) = \begin{cases}+1 \quad if D_i > 0 \\-1 \quad if D_i < 0\end{cases}\tag{13}</script><p>最终将符号秩总和作为检验结果:</p><script type="math/tex; mode=display">W_{sum} = \sum_{i=1}^n sgn(D_i)\cdot R_i \tag{14}</script><p>对于上述假设检验或其他任意假设检验, 还可以用p值刻画分布的内涵. 其统一的定义为:</p><script type="math/tex; mode=display">p = \mathbb{P}_{H_0}(T(X) \sim T(x_{obs})) \tag{15}</script><p>其中X为随机样本, $x_{obs}$为观测样本, $T(\cdot)$为检验统计量, $\mathbb{P}_{H_0}$为在原假设$H_0$成立条件下的概率. 对于Wilcoxon检验, 原假设为差值的中值为0, 即数据分布呈现中心对称.</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>[1]: Initializing Bayesian Hyperparameter Optimization via Meta-Learning, AAAI</p><p>[2]: Modern approaches to discrete curvature, Book</p><p>[3]: Dataset Similarity to Assess Semisupervised Learning Under Distribution Mismatch Between the Labeled and Unlabeled Datasets, IEEE TRANSACTIONS ON ARTIFICIAL INTELLIGENCE</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
dataset similarity measure调研与简单复现
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="笔记" scheme="http://boremycin.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="review" scheme="http://boremycin.github.io/tags/review/"/>
    
    <category term="代码复现" scheme="http://boremycin.github.io/tags/%E4%BB%A3%E7%A0%81%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>PyTorch多卡训练初探</title>
    <link href="http://boremycin.github.io/2025/11/24/code6/"/>
    <id>http://boremycin.github.io/2025/11/24/code6/</id>
    <published>2025-11-24T11:05:17.876Z</published>
    <updated>2025-11-24T11:06:28.612Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">在自己当前的分类模型集成框架里加入了多卡训练, 借此梳理相关的概念. </p><br><span id="more"></span></p><p>PyTorch DDP(Distributed Data Parallel)包括了三个核心概念, 分别为<code>rank</code>, <code>local_rank</code>和<code>node_rank</code>. </p><ul><li>rank: 即Gobal Rank, 为物理GPU的编号</li><li>local_rank: 当前节点的进程编号. 每个节点GPU都从0开始编号</li><li>node_rank: 当前节点的编号, 在多节点训练时使用</li></ul><p>需要注意的是, <code>local_rank</code>是<code>torchrun</code>自动传入的环境变量, 可以通过以下方式获取:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">local_rank = <span class="built_in">int</span>(os.environ.get(<span class="string">&#x27;LOCAL_RANK&#x27;</span>, <span class="number">0</span>))</span><br></pre></td></tr></table></figure><p>此外pytorch的DDP中还有一些需要初始化的参数, 包括<code>world_size</code>, 为全局进程总数, 是所有节点上所有GPU进程的总数量; <code>nproc_per_node</code>是单节点进程数, 表示单个物理机上启动的训练进程的数量, <code>master_addr</code>是主节点IP地址, 用于初始化分布式进程组的rank=0进程所在机器的IP地址; <code>master_port</code>为主节点端口号, 用于初始化分布式进程组的协调进程所监听的TCP端口号, 与<code>master_addr</code>共同指定了协调服务的位置; <code>backend</code>为通信后端, 用于指定进程间通信所用协议, 常用nccl来进行GPU之间的高速通信. </p><p>结合<code>torchrun</code>启动分布式训练的命令行如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=1,2 torchrun --nproc_per_node=2 train.py</span><br></pre></td></tr></table></figure><p>启动后首先需要对模型进行DDP场景下的加载, 对应代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> config.train.distributed:</span><br><span class="line">    <span class="comment"># 启用 SyncBatchNorm</span></span><br><span class="line">    model = nn.SyncBatchNorm.convert_sync_batchnorm(model) </span><br><span class="line">    model = model.to(config.device) </span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 使用 DistributedDataParallel 封装模型</span></span><br><span class="line">    model = nn.parallel.DistributedDataParallel(</span><br><span class="line">        model,</span><br><span class="line">        device_ids=[<span class="built_in">int</span>(config.device.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>])],</span><br><span class="line">        output_device=<span class="built_in">int</span>(config.device.split(<span class="string">&quot;:&quot;</span>)[<span class="number">1</span>]) </span><br><span class="line">    )</span><br></pre></td></tr></table></figure><p>以上代码中的<code>nn.SyncBatchNorm.convert_sync_batchnorm(model)</code>的核心作用是跨所有GPU聚合批次的统计信息, 用于共享GPU之间不同数据的计算结果. <code>nn.parallel.DistributedDataParallel(...)</code>在每个进程/GPU 上拥有一个完整的模型副本, 独立进行前向和反向传播. 在反向传播完成后, 它会自动协调(使用 All-Reduce 操作)所有副本的梯度, 并同步更新模型参数, 确保所有 GPU 上的模型保持一致. </p><p>此外, 在进行分布式训练时需要对数据的加载进行对应处理, 使不同GPU读取到不同的训练或测试数据, 对应的代码实现如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> config.train.distributed:</span><br><span class="line">    train_sampler = torch.utils.data.distributed.DistributedSampler(train_dataset) <span class="comment"># 创建分布式数据采样器</span></span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    train_sampler = <span class="literal">None</span> </span><br><span class="line"><span class="comment"># train_loader = DataLoader(train_dataset, sampler=train_sampler, ...)</span></span><br></pre></td></tr></table></figure><p>以上代码中的<code>torch.utils.data.distributed.DistributedSampler</code>为分布式采样器, 根据当前的rank和world_size自动将整个数据集逻辑地分割成world_size个不重叠的子集, 从而保证每个GPU在每个epoch中都能获取唯一且互不重叠的数据批次. </p><p>在训练进行中, 同样需要对数据采集器进行必要的同步操作, 对应代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> config.train.distributed:</span><br><span class="line">    <span class="comment"># 在每个 epoch 开始时调用</span></span><br><span class="line">    train_loader.sampler.set_epoch(epoch)</span><br></pre></td></tr></table></figure><p>以上代码中的epoch为当前轮次的编号, 该函数的核心功能是重新打乱数据, 确保每个epoch的数据划分都是随机且均匀的.</p><p>对于模型的运行与训练结果保存, 在模型训练层面, DDP模式下训练循环和单卡类似, 在梯度反向传播中DDP机制会自动在所有进程之间同步梯度, 无需额外干预和对代码的修改. 对于训练权重的保存, DDP模式只在rank=0的进程上执行保存操作. </p><p>通过以上操作即可实现多卡的模型训练与保存. </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
在自己当前的分类模型集成框架里加入了多卡训练, 借此梳理相关的概念. 
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="笔记" scheme="http://boremycin.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="工程开发" scheme="http://boremycin.github.io/tags/%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：LoRa-PGD Attack</title>
    <link href="http://boremycin.github.io/2025/11/19/paper11/"/>
    <id>http://boremycin.github.io/2025/11/19/paper11/</id>
    <published>2025-11-19T12:21:57.480Z</published>
    <updated>2025-11-19T12:53:50.635Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">关于PGD攻击LoRa形式初始化改进算法的笔记</p><br><span id="more"></span></p><h1 id="发表去向"><a href="#发表去向" class="headerlink" title="发表去向"></a>发表去向</h1><p>arxiv版本, 暂未发表</p><h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><h2 id="PGD优化与PGD-Attack"><a href="#PGD优化与PGD-Attack" class="headerlink" title="PGD优化与PGD Attack"></a>PGD优化与PGD Attack</h2><p>投影梯度下降(Projection Gradient Descent)原本是一种鲁棒优化算法, 其核心思想是设定一个梯度下降的范围边界, 当每次下降的结果超出边界后就将其投影到边界上距离最近的位置, 其应用目标是解决带约束的优化问题. PGD的对应数学形式如下所示:</p><script type="math/tex; mode=display">x^{t+1} = \Pi_C(x^t - \alpha_t \nabla f(x^t)) \tag{1}</script><p>上式中的$\alpha_t$是更新步长, $\Pi_C(\cdot)$是到集合C的欧氏距离投影函数, 如果C是简单的集合(如盒子, 球或非负正交等), 则投影有显示表达式, 投影函数的数学表达如下:</p><script type="math/tex; mode=display">\Pi_C(z) = \arg \min_{y \in C} \| y - z \|_2 \tag{2}</script><p>以$L_2$球约束为例, 相应的函数形式为:</p><script type="math/tex; mode=display">\Pi_{B_2(x_0, \epsilon)}(z) = x_0 + \epsilon \frac{z - x_0}{\| z - x_0\|_2} \tag{3}</script><p>在PGD Attack算法中, 将目标函数设定为模型的损失函数, 采用的投影算子除上述$L_2$外还有$L_\infty$, 对应的数学表达形式如下:</p><script type="math/tex; mode=display">\Pi_{B_{\infty}(x,\epsilon)}(z) =  clip(z, x - \epsilon, x + \epsilon) \tag{4}</script><p>对于untargeted attack的设定, 相应的PGD优化形式如下:</p><script type="math/tex; mode=display">x^{t+1} = \Pi_{B_p(x, \epsilon)}(x^t + \alpha \cdot g^t)</script><script type="math/tex; mode=display">g^t = \nabla_x \mathbb{l}(f_\theta(x^t),y) \tag{5}</script><p>在PGD Attack算法中, 对于初始矩阵的选取较为敏感. 如果直接选取图像$x$, 训练时的计算开销较少但更容易陷入局部最优, 弱化最终的攻击效果. 常用随机均匀初始化(在x附近加入随机均匀扰动)或采用球面采样; 此外也有用单步FGSM作为目标引导的初始化方法.</p><h2 id="SVD与LoRA分解"><a href="#SVD与LoRA分解" class="headerlink" title="SVD与LoRA分解"></a>SVD与LoRA分解</h2><h3 id="SVD"><a href="#SVD" class="headerlink" title="SVD"></a>SVD</h3><p>关于矩阵奇异值(Singular Value)的定义如下: 设A是 $m \times n$的实矩阵, 如果存在非负实数$\sigma$以及n维非零实数向量$\alpha$, m维非零实数向量$\beta$, 使</p><script type="math/tex; mode=display">A\alpha = \sigma \beta , \quad A'\beta = \sigma \alpha \tag{6}</script><p>则称$\sigma$是A的奇异值, $\alpha$和$\beta$分别称为A关于$\sigma$的左奇异和右奇异向量. </p><p>相应的奇异值分解(SVD)的定义如下, 对应矩阵A存在以下分解:</p><script type="math/tex; mode=display">A = U \Sigma V^T \tag{7}</script><p>上式中的$U \in R^{m \times m}$为正交矩阵, $V \in R^{n \times n}$为正交矩阵, $\Sigma \in R^{m \times n}$为对角矩阵, 对角线上的非负实数均为奇异值. 若A的秩为r, 则只有前r个奇异值严格大于0. Eckart-Young则说明了SVD分解下的低秩最佳近似, 对任意的$k &lt; r$, 令$M_k = U_k \Sigma_k V^T_k$(取前k个奇异值), Echart-Young定理说明:</p><ul><li><p>$M_k$是所有秩不超过k的矩阵中还Frobenius范数意义下与M最接近的:</p><script type="math/tex; mode=display">M_k = \arg\min_{rank(X) \leq k}  \| M - X \|_F \tag{8}</script></li><li><p>相应误差则是奇异值的尾部</p></li></ul><p>因此截断SVD是做低秩近似的最优解.</p><h3 id="奇异谱"><a href="#奇异谱" class="headerlink" title="奇异谱"></a>奇异谱</h3><p>奇异谱刻画了矩阵的能量分布和结构复杂度. 对于矩阵A而言, 对应的奇异谱Frobenius Norm为:</p><script type="math/tex; mode=display">\| A \|^2_F = \sum_{i=1}^r \sigma_i^2 \tag{9}</script><p>奇异值越大, 表明矩阵的能量集中在对应方向. 此外, 奇异值谱的核范数形式如下:</p><script type="math/tex; mode=display">\| A \|_*  = \sum_i \sigma_i \tag{10}</script><p>上式为奇异值谱的L1范数, 核范数强调低秩, 会惩罚较大的奇异值; 是秩函数的凸松弛; 作为一种结构约束常用于低秩恢复, 矩阵补全等.</p><h3 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h3><p>由于大模型的训练权重往往较大, LoRA(Low-Rank Adaptation)的动机是冻结预训练参数$W_0$, 仅仅学习一个低秩增量$\Delta W$, 使最终的权重为$W_0 + \Delta W$, 同时强制$\Delta W$为秩为r的矩阵, 其对应表达形式如下:</p><script type="math/tex; mode=display">\Delta W = B \cdot A \tag{11}</script><p>其中$A \in R^{r \times d_{in}}$, $B \in R^{d_{out} \times r}$. 对于A和B的初始化, 常见的作法为使B初始化为全0矩阵, A随机为一个较小的矩阵. </p><h1 id="创新点总结"><a href="#创新点总结" class="headerlink" title="创新点总结"></a>创新点总结</h1><p>作者提出PGD方法会主要攻击目标图像奇异值谱(singular value spectrum)中的低频部分(lower part), 因此作者提出了PGD攻击的变化形式, 重点对输入图像的low rank部分进行攻击优化的计算, 结果表明该方法可以等同甚至在部分场景下优于full-rank PGD Attack, 同时显著降低了对显存的占用.</p><h1 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h1><p>对于一般的分类器对抗攻击场景, 优化目标的数学表达形式如下:</p><script type="math/tex; mode=display">\left\{\begin{aligned}&\delta X^* \in \arg\max_{\delta X \in R^{C \times N \times M}} \mathbb{l}(f_\theta (X + \delta X), Y) \\\\&\|\delta X \|_w \leq \tau, \quad X + \delta X \in C\end{aligned} \right. \tag{12}</script><p>上式中的C指代$\mathbb{R}^{C \times N \times M}$的输入格式, Y为误分类目标向量, $f_\theta$为分类器与对应参数, $\mathbb{l}$为损失函数. 对于经典的PGD攻击算法, 其扰动迭代过程如下:</p><script type="math/tex; mode=display">\delta X_{k+1} = \Pi (\delta X_k + \tau \frac{\nabla_{\delta X} \mathbb{l}}{ \| \nabla_{\delta X} \mathbb{l} \|}) \tag{13}</script><p>上式中的$\Pi(\cdot)$为投影函数. 对于low-rank攻击, 相应的攻击优化目标的数学表达形式如下:</p><script type="math/tex; mode=display">\left\{\begin{aligned}&\delta X^* \in \arg\max\limits_{\delta X} \mathbb{l}(f_\theta (X + \delta X), Y) \\\\&\|\delta X \|_w \leq \tau, \quad X + \delta X \in C, \quad rank(\delta) \leq r\end{aligned}\right. \tag{14}</script><p>鉴于秩约束本身包含复杂的几何特性, 因此一个简单的解决方案在扰动矩阵上叠加(superimpose)低秩表征, 对应的优化形式如下:</p><script type="math/tex; mode=display">\left\{\begin{aligned}& (\delta U^*, \delta V^*) \in \arg \max_{\substack{\delta U \in \mathbb{R}^{C \times N \times r} \\ \delta V \in \mathbb{R}^{ C \times r \times M}}} l(f_{\theta}(X + \delta U \otimes_C \delta V), Y) \\\\&\| \delta U \otimes_C \delta V\|_w \leq \tau, \quad X + \delta U \otimes_C \delta V\end{aligned}\right. \tag{15}</script><p>上式中的$\otimes_C$代表channel-wise的张量乘法. 相应的对扰动矩阵U和V的迭代公式如下:</p><script type="math/tex; mode=display">\delta U_{k+1} = \delta U_{k} + \frac{\nabla_{\delta U}\mathbb{l}}{\|\nabla_{\delta U}\mathbb{l}\|}, \quad \delta V_{k+1} = \delta V_{k} + \frac{\nabla_{\delta V}\mathbb{l}}{\|\nabla_{\delta V}\mathbb{l}\|} \tag{16}</script><p>在原始图像上的扰动部署形式如下:</p><script type="math/tex; mode=display">X = clamp(X + \tau \cdot {normalize}(\delta U_{k+1} \otimes_{C} \delta V_{k+1})) \tag{17}</script><p>上式中的clamp部分扮演了优化过程中的投影函数. 对于扰动幅度$\tau$, 其值的确定主要参考$l_p$ norm, 为了从秩的角度确定扰动的上限, 作者考虑了扰动奇异值的核范数结果, 设定$R = min(N, M)$, 对应的范数矩阵设定如下:</p><script type="math/tex; mode=display">\|X\|_* = \frac{1}{C} \sum_i^C\sum_j^R \sigma_i(X_{i,:,:}) = \frac{1}{C} \sum_i^C \|X_{i,:,:}\|_* \tag{18}</script><p>最终, LoRa-PGD Attack算法的伪代码流程如下表所示:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251117133131.png" style = "height: 350px width: auto"></figure><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>首先, 作者提出了三种实验的初始化方式:</p><ul><li><p>Random Initialization: 与原始LoRa方法的初始化方式相同, 从Gaussian distribution中进行随机采样作为初始化的扰动矩阵, 同时其他部分设为0, 该方式可以确保优化过程从原始图像X开始.</p></li><li><p>Transfer Initialization: 对于固定的数据集, 首先计算full-rank FGSM攻击的结果并将其作为PGD和LoRa-PGD攻击的输入样本.</p></li><li><p>Warm-up Initialization: 该方式与Transfer Initialization类似, 即对每一个特定的模型和数据集都进行专门的FGSM计算并将结果对抗样本作为攻击算法的优化起始点.</p></li></ul><p>初步的实验结果如下:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251117134617.png" style = "height: 350px width: auto"></figure><p>此外, 笔者也对该算法进行了简单的复现(主要参考原作者的开源代码), 在MSTAR数据集上对VGG模型进行白盒攻击测试, 基于原始代码的参数设定和攻击结果如下:</p><ul><li>参数设置: 迭代轮次300, eps=50/255, eps_division=1e-10, rank=0.2</li><li>攻击结果: Benign样本分类ACC为0.9641, 对抗样本分类ACC为0.6103</li></ul><p>生成的对抗样本如下图所示:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251119203140.png" style = "height: 350px width: auto"></figure><p>原始的攻击效果与PGD算法相比(eps=32/255, alpha=32/255,steps=300, ACC结果0.2154), 改进的声称SOTA的算法并没有显示出优越的攻击效果, 后续实验中即使将参数设置为较为极端的情况也难以实现理想的攻击. 通过对PGD和LoRa-PGD产生的对抗样本进行比较发现, 现有算法很难产生较大幅度的扰动, 而对MSTAR数据集而言难以获得类似光学图像上的理想效果. 限制算法在极端参数下仍无法有效产生大幅度的扰动矩阵的原因主要是对于低秩矩阵进行了全0初始化, 同时对扰动矩阵进行了较强的归一化. 针对上述两点, 分别对两个低秩矩阵进行随机初始化, 同时省略最终扰动矩阵的归一化过程. 进行上述改进后与PGD算法对比的对抗样本图像如下所示, 最终的对抗样本ACC为0.1949:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251119204142.png" style = "height: 350px width: auto"></figure><p>实验的复现代码如下, 以供读者参考:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LoRa_PGDAttack</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, model, image=<span class="literal">None</span>, epsilon = <span class="number">8</span> / <span class="number">255</span>, eps_division = <span class="number">1e-10</span> ,rank = <span class="built_in">int</span>(<span class="params"><span class="number">128</span> * <span class="number">0.2</span></span>), steps = <span class="number">10</span> ,target=<span class="literal">None</span>, random_start = <span class="literal">True</span>, init = <span class="string">&#x27;lora&#x27;</span>, device=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="variable language_">self</span>.model = model</span><br><span class="line">        <span class="variable language_">self</span>.device = device <span class="keyword">if</span> device <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">else</span> torch.device(<span class="string">&#x27;cuda:0&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> image <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:  <span class="comment"># 单图像模式</span></span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(image, np.ndarray):</span><br><span class="line">                <span class="variable language_">self</span>.x = torch.from_numpy(image).<span class="built_in">float</span>().to(<span class="variable language_">self</span>.device)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="variable language_">self</span>.x = image.clone().to(<span class="variable language_">self</span>.device)</span><br><span class="line">            <span class="variable language_">self</span>.x = <span class="variable language_">self</span>.x/<span class="number">255</span></span><br><span class="line">        <span class="variable language_">self</span>.eps = epsilon</span><br><span class="line">        <span class="variable language_">self</span>.eps_for_division = eps_division</span><br><span class="line">        <span class="variable language_">self</span>.rank = rank</span><br><span class="line">        <span class="variable language_">self</span>.steps = steps</span><br><span class="line">        <span class="variable language_">self</span>.target = target</span><br><span class="line">        <span class="variable language_">self</span>.random_start = random_start</span><br><span class="line">        <span class="variable language_">self</span>.init = init</span><br><span class="line">        <span class="variable language_">self</span>.model.<span class="built_in">eval</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#<span class="doctag">TODO:</span> To be continued </span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">attack</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;处理单个图像&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">perturb_batch</span>(<span class="params">self, images: torch.Tensor, original_labels: torch.Tensor = <span class="literal">None</span></span>) -&gt; torch.Tensor:</span><br><span class="line">        <span class="string">&quot;&quot;&quot;处理批量图像&quot;&quot;&quot;</span></span><br><span class="line">        images = images.to(<span class="variable language_">self</span>.device)</span><br><span class="line">        labels = original_labels.to(<span class="variable language_">self</span>.device)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.target != <span class="literal">None</span>:</span><br><span class="line">            target_labels = outputs[:, <span class="variable language_">self</span>.target]</span><br><span class="line"></span><br><span class="line">        loss_f = nn.CrossEntropyLoss()</span><br><span class="line">        outputs = <span class="variable language_">self</span>.model(images)</span><br><span class="line"></span><br><span class="line">        images.requires_grad = <span class="literal">True</span></span><br><span class="line">        bi_shape = images.shape <span class="comment">#batch, 3, n, n</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> <span class="variable language_">self</span>.init == <span class="string">&#x27;lora&#x27;</span>:</span><br><span class="line">            u_im = torch.randn([bi_shape[<span class="number">0</span>], bi_shape[<span class="number">1</span>], bi_shape[<span class="number">2</span>], <span class="variable language_">self</span>.rank], device=<span class="variable language_">self</span>.device) <span class="comment">#batch, 3, n, r</span></span><br><span class="line">            v_im = torch.randn([bi_shape[<span class="number">0</span>], bi_shape[<span class="number">1</span>], <span class="variable language_">self</span>.rank, bi_shape[<span class="number">3</span>]], device = <span class="variable language_">self</span>.device) <span class="comment">#batch, 3, r, n</span></span><br><span class="line">            <span class="comment">#norm_u = torch.norm(u_im.view(bi_shape[0], -1), p=2, dim=1)</span></span><br><span class="line">            <span class="comment">#u_im = (u_im / (norm_u.view(bi_shape[0], 1, 1, 1))).detach()</span></span><br><span class="line">            <span class="comment">#v_im = torch.zeros([bi_shape[0], bi_shape[1], self.rank, bi_shape[3]], device = self.device) #batch, 3, r, n</span></span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;start lora_pgd interation&#x27;</span>)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> tqdm(<span class="built_in">range</span>(<span class="variable language_">self</span>.steps)):</span><br><span class="line">            u_im = u_im.detach()</span><br><span class="line">            v_im = v_im.detach()</span><br><span class="line"></span><br><span class="line">            u_im.requires_grad = <span class="literal">True</span></span><br><span class="line">            v_im.requires_grad = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> <span class="variable language_">self</span>.eps == <span class="number">0.</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            delta = torch.einsum(<span class="string">&#x27;bcik,bckj-&gt;bcij&#x27;</span>, u_im, v_im)</span><br><span class="line">            delta_norm = torch.linalg.vector_norm(delta.reshape(bi_shape[<span class="number">0</span>], -<span class="number">1</span>), <span class="built_in">ord</span>=<span class="number">2</span>, dim=<span class="number">1</span>) + <span class="variable language_">self</span>.eps_for_division</span><br><span class="line">            im_per = torch.clamp(images + <span class="variable language_">self</span>.eps * delta/delta_norm.view(bi_shape[<span class="number">0</span>], <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="built_in">min</span>=<span class="number">0.</span>, <span class="built_in">max</span>=<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">            cur_output = <span class="variable language_">self</span>.model(im_per)</span><br><span class="line">            loss = loss_f(cur_output, labels)</span><br><span class="line">            data_grad = torch.autograd.grad(loss, inputs=[u_im, v_im], retain_graph=<span class="literal">False</span>, create_graph=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">            data_grad_u = data_grad[<span class="number">0</span>].detach()</span><br><span class="line">            data_grad_v = data_grad[<span class="number">1</span>].detach()</span><br><span class="line"></span><br><span class="line">            norm_grad_u = torch.linalg.vector_norm(data_grad_u.reshape(bi_shape[<span class="number">0</span>],-<span class="number">1</span>), <span class="built_in">ord</span>=<span class="number">2</span>, dim=<span class="number">1</span>) + <span class="variable language_">self</span>.eps_for_division</span><br><span class="line">            norm_grad_v = torch.linalg.vector_norm(data_grad_v.reshape(bi_shape[<span class="number">0</span>],-<span class="number">1</span>), <span class="built_in">ord</span>=<span class="number">2</span>, dim=<span class="number">1</span>) + <span class="variable language_">self</span>.eps_for_division</span><br><span class="line"></span><br><span class="line">            u_im = u_im + data_grad_u/(norm_grad_u.view(bi_shape[<span class="number">0</span>],<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line">            v_im = v_im + data_grad_v/(norm_grad_v.view(bi_shape[<span class="number">0</span>],<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># u_im = u_im.detach()</span></span><br><span class="line">            <span class="comment"># v_im = v_im.detach()</span></span><br><span class="line">        </span><br><span class="line">        delta = torch.einsum(<span class="string">&#x27;bcik,bckj-&gt;bcij&#x27;</span>, u_im, v_im)</span><br><span class="line">        delta_norm = torch.linalg.vector_norm(delta.detach().reshape(bi_shape[<span class="number">0</span>], -<span class="number">1</span>), <span class="built_in">ord</span> = <span class="number">2</span>, dim = <span class="number">1</span>)</span><br><span class="line">        <span class="comment">#delta = self.eps * delta / (delta_norm.view(bi_shape[0], 1, 1, 1) + self.eps_for_division)</span></span><br><span class="line">        delta = <span class="variable language_">self</span>.eps * delta</span><br><span class="line">        adv_imgs = torch.clamp(images + delta, <span class="built_in">min</span>=<span class="number">0.</span>, <span class="built_in">max</span>=<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> adv_imgs</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>作者针对PGD Attack, 从矩阵值的角度进行了优化, 但核心的观察仍然是出自图像的频率域. 通过频率变化相信可以获得自然图像中人眼和人脑(包括模型)感知学习到而不自知难察觉的信息. 但是遗憾的是, 原文并没有使用频率分析工具(FFT, DCT等)对改进前后的结果进行定量或者可视化的比较, SVD在代码实现中的体现也不够充分, 重点是参考了LoRa的初始化思路. </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
关于PGD攻击LoRa形式初始化改进算法的笔记
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="对抗样本" scheme="http://boremycin.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：SAR-VLF</title>
    <link href="http://boremycin.github.io/2025/11/04/paper10/"/>
    <id>http://boremycin.github.io/2025/11/04/paper10/</id>
    <published>2025-11-04T05:49:48.021Z</published>
    <updated>2025-11-04T05:54:02.312Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">针对PolSAR和SAR的小样本Few-Shot场景改进CLIP模型框架</p><br><span id="more"></span></p><h1 id="发表去向"><a href="#发表去向" class="headerlink" title="发表去向"></a>发表去向</h1><p>论文发表于2025 TCSVT</p><h1 id="创新点总结"><a href="#创新点总结" class="headerlink" title="创新点总结"></a>创新点总结</h1><p>在真实的SAR-ATR场景中, 对于非合作目标, 往往无法获得大量数据用于模型的训练, 因此需要关注Few-shot条件下SAR-ATR模型的性能. 随着CLIP等pre-trained模型在可见光图像zero-shot和few-shot任务上取得了优异的性能, 如何将其与SAR图像结合以解决跨域任务下(可见光-&gt;SAR)的少样本识别准确率, 作者结合预训练的CLIP模型提出了VLF-SAR(Vision Language Framework SAR)的少样本识别框架, 论文的主要创新点如下:</p><ol><li><p>设计了频域嵌入模块(frequency embedded module, FEM)来提取图像中的频率域信息. 论文将所要识别的SAR图像分为了PolSAR和Traditional SAR两类, 并提出了相应的VLF-SAR-P和VLF-SAR-T.</p></li><li><p>对于VLF-SAR-P, 论文提出了名为polarimetric feature selector(PFS)的模块, 在图像结构相似度的基础上用于提取最合适的测定偏振向量, 为了实现不同偏振方向的向量之间的动态选取, 作者进而提出了adaptive multimodal triple attention mechanism(AMTAM). 此外, 作者引入了对比学习的方法, 通过将上述feature 划分为positive和negative来增强对于feature的区分和选取.</p></li><li><p>对于VLF-SAR-T, 由于一般的SAR图像只包含反映了幅值信息, 作者针对此提出了multimodal fusion attention mechanism(MFAM)处理信息量相对较少的SAR图像.</p></li></ol><h1 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h1><p>VLF-SAR的整体架构如下图所示:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251019200059.png" style = "height: 400px width: auto"></figure><p>由于基于电磁散射的特殊成像机理, SAR图像呈现出包含大量散斑噪声, 高对比度和缺少自然色彩信息等特点, 也导致SAR图像和可见光图像之间存在巨大的domain gap. 在SAR成像过程中, 电磁波后向散射的信息会包含在成像结果的频率域当中, 因此作者提出了physical information guidance module(PIGM)来提取和利于图像频率域中所包含的物理信息.</p><h2 id="FEM模块"><a href="#FEM模块" class="headerlink" title="FEM模块"></a>FEM模块</h2><p>该模块选择DCT进行空域SAR图像的频域转换, DCT变化的数学表达形式如下:</p><script type="math/tex; mode=display">D(u, v) = \alpha(u) \cdot \alpha(v) \sum_{x = 0}^{M-1} \sum_{y = 0}^{N-1} \mathbf{I}(x, y) \cdot cos[\frac{(2x+1)u\pi}{2M}] \cos[\frac{(2y+1)v\pi}{2N}] \tag{1}</script><p>上式中的$\mathbf{I}(x,y)$为M*N的输入图像, $\alpha(\cdot)$为不同方向上的尺度缩放因子. 在进行DCT变换之后, 作者设定了全局阈值T来对频域矩阵进行阈值分割处理, T的表达形式如下:</p><script type="math/tex; mode=display">T = n \times mean(|D|) \tag{2}</script><p>上式中的n被设定为3, 来区分SAR图像的前景和背景. 根据T作者将DCT矩阵分为两份, 保留大于T部分的矩阵称为$D_{denoised}$, 另一个矩阵称为$D_{noise}$, 其中前者压缩了原始图像中的低频信息(denoise的对象则为高频的speckle noise), 后者则保留了高频信息. 此外, 作者提出了一种将单通道灰度PolSAR图像通过IDCT重建为3通道空域图像的方法, 其中各个通道的组成成分如下:</p><script type="math/tex; mode=display">I_{DCT-3}(x, y ,1) = I(x, y)</script><script type="math/tex; mode=display">I_{DCT-3}(x, y ,2) = IDCT(D_{denoised})</script><script type="math/tex; mode=display">I_{DCT-3}(x, y ,3) = IDCT(D_{noised}) \tag{3}</script><p>该重建操作主要目的是对齐CLIP模块三通道输入的要求, 同时对于PolSAR和SAR图像都可以提取到基础的图像信息.</p><h2 id="VLF-SAR-P模块"><a href="#VLF-SAR-P模块" class="headerlink" title="VLF-SAR-P模块"></a>VLF-SAR-P模块</h2><p>在PolSAR图像的解译过程中, 极化特征向量包含着成像目标区域的物理特性信息. 因此, 在进行PolSAR图像的小样本分类时需要重点关注极化特征向量的提取与选择(因为polar feature 通常多余3个). 论文提出了极化特征向量选取算法, 基于SSIM(structural similarity index measure)进行改进, 提出了名为Combined Similarity Index Measure的特征向量评估方法. 对于SSIM, 数学表达形式如下: </p><script type="math/tex; mode=display">SSIM(I_1, I_2) = \frac{(2 \mu_{I_1} \mu_{I_2} + C_1) (2\sigma_{I_1I_2} + C_2)}{(\mu_{I_1}^2 +  \mu_{I_2}^2 + C_1)(\sigma_{I_1}^2 + \sigma_{I_2}^2 + C_2)} \tag{4}</script><p>上式中$\mu$为图像的均值, $\sigma$为图像的方差, 参数$C_1$和$C_2$被用于平衡分子和分母. 对于PolSAR图像, 背景散斑噪声占据了图像的主体部分, 会严重影响图像的均值与方差, 使不同图像间的差异被缩小或忽略. 因此作者提出使用Ostu图像分割算法提取PolSAR图像的主体部分并产生对应的mask掩膜矩阵, 再统计不同图像主体部分间的结构相似度, 这里所用于比较的图像为原始的PolSAR图像和所提取出的polar features. 掩膜矩阵M的定义如下:</p><script type="math/tex; mode=display">m = \arg \max_{\tau}[\sigma_B^2(\tau)]</script><script type="math/tex; mode=display">M(x, y) = \left\{ \begin{array}{rcl}1, & \ if \ I(x,y)  \geq m \\0, & \ if \ I(x,y)  \leq m \tag{5}\end{array}\right.</script><p>上式中的$\sigma_B^2(\tau)$为类间差额阈值(between class variance threshold), $M(x,y)$为对应的掩码矩阵.由此可以获得如下表达式:</p><script type="math/tex; mode=display">S_M(P_i, O) = SSIM(P_{iM}, O_{M}) \tag{6}</script><p>在此基础上, 作者引入了另一种度量矩阵FSIM, 会根据图像的梯度信息和相位一致性计算相似度, 对应的表达形式如下:</p><script type="math/tex; mode=display">F(P_i, O) = \frac{\sum_xPC_x \cdot GM_x \cdot S(P_i(x), O(x))}{\sum_x PC_x \cdot GM_x} \tag{7}</script><p>其中的$PC_x$为相位一致性, 其内涵为人眼观察下图像中的感知特征, 更能反映图像中例如边缘, 角点等显著的局部结构(体现在傅里叶频谱中的相位对齐); $GM_x$为像素点x的梯度值, 用于衡量局部区域的亮度变化率. 对于前者的计算基于局部能量模型, 使用一组具有不同尺度和方向的Log-Gabor小波滤波器对图像进行滤波. Log-Gabor是Gabor滤波器的变种, 在频域的传递函数(Transfor Function)服从对数高斯函数的分布. Log-Gabor的优势是可以在保持零交流分量的前提下构造具有任意带宽的滤波器, 滤波器对应的频率传递函数是两个函数的乘积:</p><script type="math/tex; mode=display">G(f) = G_r(r) \cdot G_\theta(\theta) \tag{8}</script><p>其中$r = |f|$是径向频率(频谱中距离原点的距离), $\theta$是方向角度. $G_r(r)$的计算公式如下:</p><script type="math/tex; mode=display">G_r(r) = exp(-\frac{|log(r/r_0)|^2}{2|log(\sigma / r_0)|^2}) \tag{9}</script><p>上式中的r是径向频率, $\sigma$是带宽参数, 在图像经过FFT后, 频谱图中的每个点(u,v)都代表一个二维坐标频率, r则是从原点到(u, v)的欧氏距离, 即 $r = \sqrt{u^2 + v^2}$ , 反映了图像中空间变化的快慢. 径向频率越高, 图像对应区域的纹理或边缘变化越密集精细. 而方向相应, 其决定了滤波器对哪个方向的结构最敏感, 数学表达形式如下:</p><script type="math/tex; mode=display">G_{\theta}(\theta) = exp(- \frac{(\theta - \theta_0)^2}{2\sigma_{\theta}^2}) \tag{10}</script><p>上式中$\theta$为当前频率点的角度, $\theta_0$为中心方向, 决定了滤波器相应最大的方向, $\sigma_{\theta}$为方向带宽参数, 控制方向相应的宽度. 原始的Gabor滤波器则是高斯包络与复正弦平面波的乘积, 数学表达形式如下:</p><script type="math/tex; mode=display">g(x, y; \lambda, \theta, \psi, \sigma, \gamma) = exp(-\frac{x'^2 + \gamma^2 y'^2}{2\sigma^2}) \cdot exp(i(2\pi\frac{x'}{\lambda}+\psi)) \tag{11}</script><p>其中的$\lambda$为正弦波波长(与中心频率成反比), $\theta$为滤波器方向, $\psi$为相位偏移, $\sigma$为高斯包络的标准差, $\gamma$为空间纵横比(控制包络的椭圆度), $x’, y’$为旋转后坐标, 用于实现$\theta$.</p><p>回到FSIM的计算定义, 对于相位一致性$PC_x$, 其计算会使用一组具有不同尺度和方向(M个尺度, N个方向)的Log-Gabor小波滤波器对图像进行滤波, 对于每个尺度m和方向n, 滤波结果都包含实部$E_{m,n}^R(x)$和虚部$E_{m,n}^I(x)$, 通过根据这个复数结果即可获取幅值信息和相位信息, 局部幅值与相位的数学形式如下:</p><script type="math/tex; mode=display">A_{m, n}(x) = \sqrt{[E_{m,n}^R(x)]^2 + [E_{m,n}^I(x)]^2}</script><script type="math/tex; mode=display">\Phi_{m,n}(x) = atan2(E_{m,n}^I(x), E_{m,n}^R(x)) \tag{12}</script><p>则PC的计算公式为:</p><script type="math/tex; mode=display">PC(x) = \frac{\sum_n\sum_m A_{m,n}(x) \cdot \Delta \Phi_{m,n}(x) - \epsilon}{\sum_n \sum_m A_{m,n}(x) + \epsilon} \tag{13}</script><p>上式中的$\Delta \Phi_{m,n}(x)$ 用于衡量不同尺度上的相位偏离其加权平均相位的程度, $\sum_n \sum_m A_{m,n}(x) \cdot \Delta \Phi_{m,n}(x)$为局部能量, 表示所有尺度和方向上的小波幅值和, $\epsilon$为取值较小的常数, 用于放置除零. $\Delta \Phi_{m,n}(x)$的数学表达形式如下:</p><script type="math/tex; mode=display">\Delta \Phi_{m,n}(x) = |\Phi_{m,n}(x) - \bar{\Phi}(x)| \tag{14}</script><p>其中$\bar{\Phi}(x)$为加权平均相位. PC值越高, 表示该像素所有尺度和方位上的小波相位越一致, 结构越显著. 对于梯度幅值的计算则较为简单, 在空域中使用标准的梯度算子(如Sobel算子或Prewitt算子)即可完成, 梯度幅值为水平梯度矩阵和垂直梯度矩阵的几何平均数.</p><script type="math/tex; mode=display">GM(x) = \sqrt{G_x(x)|^2 + |G_y(x)|^2} \tag{15}</script><p>FSIM算法在计算出参考图像X和给定图像Y的PC和GM图后, 会进而计算他们在每个像素上的相似度, PC相似度和GM相似度的计算公式如下图所示:</p><script type="math/tex; mode=display">S_{PC}(x) = \frac{2 \cdot PC_{X}(x) \cdot PC_{Y}(x) + C_1}{[PC_{X}(x)]^2 + [PC_{Y}(x)]^2 + C_1}</script><script type="math/tex; mode=display">S_{GM}(x) = \frac{2 \cdot GM_{X}(x) \cdot GM_{Y}(x) + C_2}{[GM_{X}(x)]^2 + [GM_{Y}(x)]^2 + C_2} \tag{16}</script><p>对两个相似度进行融合即可得到局部相似度, 用$\alpha$和$\beta$进行加权, 通常直接求和, 表达形式如下: </p><script type="math/tex; mode=display">S_L(x) = [S_{PC}(x)]^\alpha + [S_{GM}(x)]^\beta \tag{17}</script><p>最终的FSIM分数为对局部相似度图进行加权平均得来, 权重使用两幅图像中的PC值的最大值:</p><script type="math/tex; mode=display">FSIM = \frac{\sum_{x \in \Omega} S_L(x) \cdot PC_M(x)}{\sum_{x \in \Omega} PC_M(x)}</script><script type="math/tex; mode=display">PC_M(x) = \max \{ PC_X(x), PC_Y(x) \} \tag{18}</script><p>在论文设定中, FSIM不需要使用由Ostu所提取的mask矩阵, 最终的混合相似度CSIM表达式为:</p><script type="math/tex; mode=display">CSIM(P_i, O) = \sqrt{S_M(P_i, O) \cdot F(P_i, O)} \tag{19}</script><p>作者通过该结果同时考量了极化特征向量与原图的结构相似度和感知相似度. 最终作者挑选CSIM最大的极化特征作为结果. </p><p>此外, 为了向CLIP模型中引入领域知识(如PolSAR图像中的物理信息), 作者提出了自适应多模态三重注意力机制(Adaptive Multimodal Triple Attention Mechanism, AMTAM), 对应框架图如下所示:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251027210201.png" style = "height: 400px width: auto"></figure><p>对于图像特征I和文本特征T, CLIP模型进行的注意力机制操作主要如下:</p><script type="math/tex; mode=display">Q_T = W_Q^T  T</script><script type="math/tex; mode=display">K_I = W_K  I, \ \ V_I = W_V  I</script><script type="math/tex; mode=display">T_{attentionI} = softmax(\frac{Q_T \cdot K_I^T}{\sqrt{d}}) \cdot V_I \tag{20}</script><p>作者上文中通过PFS模块引入了多个额外极化特征矩阵作为SAR图像特殊的物理信息, 因此将其多为额外的模态, 引导模型动态的平衡不同模态间的输入对最终决策的贡献, 最终的合成文本向量代表了原始的文本向量和额外的被注意图像向量的加权和, 权重w为可训练参数, 使用softmax确保其归一性, 表达形式如下:</p><script type="math/tex; mode=display">T_{fused} = w_1T'' + w_2T_{atttended\_I1} + w_3T_{atttended\_I2} \tag{21}</script><p>其中$T’’$的计算表达形式如下, 上文中注意力计算所用T矩阵为该输出结果:</p><script type="math/tex; mode=display">T' = \sigma(W_T^1T + b_T^1), \ \ T'' = W_T^2T' + b_T^2 \tag{22}</script><p>上式中的T为文本的embedding feature, $\sigma$代表ReLU等激活函数. 以文本表征(text representation)为例, 最终的融合结果如下:</p><script type="math/tex; mode=display">T_{final} = W_{F}^{T}T_{fused} + b_F^T \tag{23}</script><p>综上, 对于AMTAM模块, 其主要功能是对不同模态的特征进行了动态和自适应的融合.</p><h2 id="对比学习"><a href="#对比学习" class="headerlink" title="对比学习"></a>对比学习</h2><p>研究表明, 对比学习(contrastive learning)在少样本学习的分类任务中具有显著的有效性. 基于此, 作者提出了针对SAR图像少样本识别的改进对比学习模块. 基于上述的$AMTAM-I_{DCT-3}$ 和相应的极化特征结果, 结合对应的相同样本作为positive pairs, 其余的$I_{DCT-3}$的samples作为negative samples, 对应的对比学习损失函数如下:</p><script type="math/tex; mode=display">L_c = \sum_i max(0, d(f_i^D, f_i^P)- d(d_i^D, f_j^D) + margin) \tag{24}</script><p>上式中的$f_i^D$和$f_i^P$分别代表$I_{DCT-3}$特征和对应的极化特征, $d(\cdot, \cdot)$是欧氏距离. margin项确保模型可以在不破坏特征空间的情况下高效区分出不同的样本. 结合分类器原有的交叉熵损失, 最终VLF-SAR-P的损失函数的数学形式如下:</p><script type="math/tex; mode=display">L_{total} = L_{CE} + \lambda L_c \ , \quad L_{CE} = -\sum_{i=1}^C y_i log(p_i) \tag{25}</script><h2 id="VLF-SAR-T"><a href="#VLF-SAR-T" class="headerlink" title="VLF-SAR-T"></a>VLF-SAR-T</h2><p>与PolSAR图像不同, 一般的SAR图像仅包括强度信息(即幅值信息), 因此需要对已有架构进行进化, 对应的VLF-SAR-T的模型架构仅包括FEM和MFAM, 前者已在上文中说明. 对于MFAM, 其目的在于优化文本信息和SAR图像信息的融合(而不设计极化特征向量), 相应的模型结构如下图所示:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251103124843.png" style = "height: 400px width:auto"></figure><p>上图中起始的输入特征由冻结的CLIP模型的encoders提取获得, 经过简单的MLP和交叉注意力模块处理后获得互相关特征, 接着与原有的图像特征和文本特征进行拼接融合, 选择的方法是简单的拼接操作(concatenation), 对应的数学表达如下所示:</p><script type="math/tex; mode=display">T_{final} = L(T \copyright T_{attended}) \ , \quad I_{final} = L(I \copyright I_{attended}) \tag{26}</script><p>上式中的$\copyright$表示concatenation操作, $L(\cdot)$代表线性变换.   </p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>作者选用了多个数据集, 包括包含PolSAR图像的opensarship2.0数据集和普通SAR图像的FuSAR-ship数据集, 并进行了包括消融实验在内的多项完备实验对比, 此处仅展示核心实验结果. 对于PolSAR和SAR图像, 模型fewshot的结果如下图所示:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251103180641.png" style = "height:400px width:auto"></figure><p>由于作者在文中引入了多个改进模块, 关于不同模块有效性的消融实验结果如下:</p><figure style = "text-align: center">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251103181034.png" style = "height:400px width:auto"></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文作者提出了针对PolSAR和SAR图像的few-shot场景下的优化框架VLF-SAR, 并以CLIP模型为主体, 通过加入多个优化模块实现更好的少样本分类性能. 然而当前模型仍存在以下不足: FEM模块进行极化特征提取时依赖先验的阈值设定; 没有展开对于未知类别分类性能的测试.</p><h1 id="代码复现"><a href="#代码复现" class="headerlink" title="代码复现"></a>代码复现</h1><p>对于论文中的FEM模块, 即用DCT与IDCT进行特征矩阵提取, 笔者进行了简单的复现, 针对batch形式的tensor场景, 选用torch-dct库, 对应的复现代码和可视化结果如下所示:</p><p>复现代码:<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 </span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch_dct <span class="keyword">as</span> dct</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">polsar = cv2.imread(<span class="string">&quot;sarship.jpg&quot;</span>)</span><br><span class="line">polsar_rgb = cv2.cvtColor(polsar, cv2.COLOR_BGR2RGB)</span><br><span class="line">polsar_torch = torch.from_numpy(polsar_rgb).permute(<span class="number">2</span>,<span class="number">0</span>,<span class="number">1</span>).<span class="built_in">float</span>()</span><br><span class="line">polsar_batch = torch.unsqueeze(polsar_torch, dim = <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">polsar_dct = dct.dct_2d(polsar_batch)</span><br><span class="line"></span><br><span class="line">channels_mean = torch.<span class="built_in">abs</span>(polsar_dct).mean(dim=(<span class="number">2</span>,<span class="number">3</span>)) <span class="comment"># (batch_size, channels)</span></span><br><span class="line"></span><br><span class="line">T = <span class="number">3</span> * channels_mean[..., <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line"></span><br><span class="line">D_denoise = polsar_dct * (torch.<span class="built_in">abs</span>(polsar_dct) &lt; T).<span class="built_in">float</span>()</span><br><span class="line">D_noise = polsar_dct * (torch.<span class="built_in">abs</span>(polsar_dct) &gt;= T).<span class="built_in">float</span>()</span><br><span class="line"></span><br><span class="line">I_denoise = dct.idct_2d(D_denoise)</span><br><span class="line">I_noise = dct.idct_2d(D_noise)</span><br><span class="line"></span><br><span class="line">I = polsar_batch</span><br><span class="line"></span><br><span class="line">I_denoise_np = I_denoise.cpu().detach().numpy()[<span class="number">0</span>]  <span class="comment"># 选择第一个batch的第一个通道</span></span><br><span class="line">I_noise_np = I_noise.cpu().detach().numpy()[<span class="number">0</span>]      <span class="comment"># 选择第一个batch的第一个通道</span></span><br><span class="line">I_np = I.cpu().detach().numpy()[<span class="number">0</span>]     </span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_255</span>(<span class="params">img_np</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(img_np.shape) == <span class="number">2</span>:</span><br><span class="line">        img_normalized = (img_np - img_np.<span class="built_in">min</span>()) / (img_np.<span class="built_in">max</span>() - img_np.<span class="built_in">min</span>() + <span class="number">1e-8</span>)</span><br><span class="line">        <span class="keyword">return</span> (img_normalized * <span class="number">255</span>).astype(np.uint8)</span><br><span class="line">    <span class="keyword">elif</span> <span class="built_in">len</span>(img_np.shape) == <span class="number">3</span> <span class="keyword">and</span> img_np.shape[<span class="number">2</span>] == <span class="number">3</span>:</span><br><span class="line">        channel_mins = img_np.<span class="built_in">min</span>(axis = (<span class="number">0</span>,<span class="number">1</span>), keepdims = <span class="literal">True</span>)</span><br><span class="line">        channel_maxs = img_np.<span class="built_in">max</span>(axis = (<span class="number">0</span>,<span class="number">1</span>), keepdims = <span class="literal">True</span>)</span><br><span class="line">        channel_ranges = channel_maxs - channel_mins</span><br><span class="line">        channel_ranges[channel_ranges == <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">        img_normalized = (img_np - channel_mins) / channel_ranges</span><br><span class="line">        <span class="keyword">return</span> (img_normalized * <span class="number">255</span>).astype(np.uint8)</span><br><span class="line"></span><br><span class="line">I_denoise_vis = normalize_255(np.transpose(I_denoise_np, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">I_noise_vis = normalize_255(np.transpose(I_noise_np, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line">I_vis = normalize_255(np.transpose(I_np, (<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">1</span>)</span><br><span class="line">plt.imshow(I_vis)</span><br><span class="line">plt.title(<span class="string">&quot;original image&quot;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">2</span>)</span><br><span class="line">plt.imshow(I_denoise_vis )</span><br><span class="line">plt.title(<span class="string">&quot;denoise feature&quot;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">1</span>,<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line">plt.imshow(I_noise_vis)</span><br><span class="line">plt.title(<span class="string">&quot;noise feature&quot;</span>)</span><br><span class="line">plt.axis(<span class="string">&#x27;off&#x27;</span>)</span><br><span class="line"></span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure></p><p>PolSAR图像的可视化结果(采用OpenSARShip2.0数据集)如下:</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/polsar_result2.png" alt="polsar_result2"></p><p>SAR图像的可视化结果如下:</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/sar_result2.png" alt="sar_result2"></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
针对PolSAR和SAR的小样本Few-Shot场景改进CLIP模型框架
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="小样本Few-Shot" scheme="http://boremycin.github.io/tags/%E5%B0%8F%E6%A0%B7%E6%9C%ACFew-Shot/"/>
    
    <category term="CLIP" scheme="http://boremycin.github.io/tags/CLIP/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：Adversarial Robust under Long-tailed distribution</title>
    <link href="http://boremycin.github.io/2025/10/07/paper9/"/>
    <id>http://boremycin.github.io/2025/10/07/paper9/</id>
    <published>2025-10-07T02:29:52.331Z</published>
    <updated>2025-10-07T02:35:45.460Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">关于结合LT分布与AT(Adversarial Training)的论文笔记</p><br><span id="more"></span></p><h1 id="发表情况"><a href="#发表情况" class="headerlink" title="发表情况"></a>发表情况</h1><p>2021年CVPR oral论文</p><h1 id="摘要与创新点总结"><a href="#摘要与创新点总结" class="headerlink" title="摘要与创新点总结"></a>摘要与创新点总结</h1><p>对抗样本的提出解释了CNN架构为代表的神经网络模型具有特殊的脆弱性, 而对抗鲁棒性则重点关注提升模型对对抗样本的识别准确率, 使模型在处于攻击的场景下仍能保持性能, 同时拓展对模型性能的评估角度, 进一步解释神经网络模型的内在机理. 然而, 从数据角度出发, 大部分模型训练所用数据均是均匀分布的, 而在现实场景中数据的分布常为长尾分布(Long-tailed Distribution). 在论文中, 作者从长尾分布数据的角度出发, 揭示了该数据分布对模型的识别性能和对抗鲁棒性造成的负面影响. 在此基础上, 作者提出了针对长尾分布数据集的高效对抗训练框架(可以理解为defense手段), 实现了该场景下的SOTA效果. 对于论文的场景, 作者定义了两种识别准确率 $A_{nat}$ 和 $A_{rob}$ , 前者指natural, 会在默认训练和加入对抗训练的两种模型上进行评估, 而后者指robust, 仅在对抗训练的分类模型上进行评估. 对于二者的区别, 可以用如下公式表示. </p><script type="math/tex; mode=display">A_{rob} = A_{nat} - R_{bdy} \tag{1}</script><p>上式中的$R_{bdy}$指那些干净和被正确分类的输入的features与决策边界的$\epsilon$范围的距离. 为了使$A_{nat}$较大且$A_{rob}$较小, 作者认为可以从长尾分布数据再平衡的角度着手. 综上, 本文的创新点可总结如下:</p><ol><li>首次提出从数据长尾分布的角度研究模型对抗鲁棒性的问题</li><li>结合针对长尾分布的rebalance方法和对抗训练方法</li><li>提出了SOTA的针对长尾分布的对抗鲁棒训练框架</li></ol><h1 id="核心方法"><a href="#核心方法" class="headerlink" title="核心方法"></a>核心方法</h1><h2 id="现有方法及其实验结果分析"><a href="#现有方法及其实验结果分析" class="headerlink" title="现有方法及其实验结果分析"></a>现有方法及其实验结果分析</h2><p>首先需要对<strong>Adversarial Robustness</strong>的定义进行阐释, 该定义主要指通过Adversarial Defence或Adversarial Training(AT)的方法提升模型在受到对抗样本攻击时的分类性能. 针对LT策略和AT框架结合后对模型的影响, 作者进行了详尽的实验, 结果如下:</p><figure style="text-align: center;">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251005212230.png" style = "width: auto; height: 350px;"></figure><p>对于对抗训练的流程, 可以用如下数学形式表示:</p><script type="math/tex; mode=display">\min_{\theta} \mathbb{E}_{(x,y) \sim D} [L_{T}(\theta ; x + \delta, y)] \ , \\where \ \  \delta = \arg \max_{\delta \in \mathcal{B}(\epsilon) } L_{A}(\theta ; x + \delta, y) \tag{2}</script><p>上式中的内部优化目标时找到最有效的对抗扰动, 外部目标是通过损失函数的梯度下降使模型对对抗样本也保持鲁棒性. 两个损失函数都可以用CE损失. 而对于LT分布下的模型优化方法, 可以大致分为三类: <strong>training</strong>, <strong>fine-tuning</strong>和<strong>inference</strong>. 同时对以下表达进行规定: $f(x)$为图像$x$输入模型后提取深层特征向量, $W$为分类器的权重向量; $\tilde{f}(x)$ 和 $\tilde{W}$ 则为对上述向量进行归一化后的结果. </p><p>在训练结果, 作者结合多种针对长尾分布的重采样和代价敏感学习(cost-sensitive learning)的方法, 提出了改进的基于CE的损失函数, 具体数学表达形式如下:</p><script type="math/tex; mode=display">L'_{CE}(W; f(x),y) = -r_{w}(y) \cdot log(\frac{e^{z_y}}{\sum_{i}e^{z_i}})</script><script type="math/tex; mode=display">where \ \ z_i = g_i(W_i, f(x)) \tag{3}</script><p>上式中的$g(\cdot)$代表分类模型进行softmax前的logit值, $r_w(y)$则代表对于类别y的re-weighting因子, 相当于对这类样本的采样频率. $g$函数由多种表达形式, 常用线性分类形式: </p><script type="math/tex; mode=display">g_i(W_i, f(x)) = W_i^T \cdot f(x) + b_i \tag{4}</script><p>此外, $L’_{CE}$还可用于加入到对抗训练中(即模型更新所用梯度和对抗样本生成所有梯度均来自作为标量的$L’_{CE}$损失函数). 在fine-tuning阶段, 核心的做法是将模型的backbone冻结, 通过数据re-balancing的技术对模型进行微调, 实验结果表明经过一轮微调即可显著提升$A_{nat}$, 但后续微调轮次增加的效果则不明显. 在inference阶段, 作者比较了权重正则化($\tau-norm$), classifier re-scaling 和 feature disentangling等解决LT问题的算法. 结果均已在上述表格中. </p><p>在实验中作者发现, 当前的LT methods对于$A_{nat}$ 和 $A_{rob}$ 间性能存在差距的问题都没有较好的解决手段. 然而, cosine classifier相较于传统的linear classifier则在这个问题上有更好的表现. 对于linear classifier, 可以简单理解为通过CNN提取出特征向量后, 在执行分类层时近似为线性运算, 进而获得模型输出的logits, 相应的表达形式如下:</p><script type="math/tex; mode=display">z_j = W_j^T \cdot f(x) + b_j \tag{5}</script><p>上式中f(x)表示CNN模型的backbone, $z_j$表示第j类的logits值. 而在cosine classifier中, 对特征向量$f(x)$ 和 对应权重$W_j$进行归一化操作, 这样两者的点乘结果即为两向量间的余弦值, 因此避免了向量长度对预测分类结果的影响, 提高对LT分布数据集的分类鲁棒性. </p><h2 id="RoBal训练框架"><a href="#RoBal训练框架" class="headerlink" title="RoBal训练框架"></a>RoBal训练框架</h2><p>结合上述实验结果, 作者总结了长尾分布下的对抗鲁棒性面临着两个主要问题:</p><ol><li>合适的特征向量与嵌入手段的选择</li><li>如何更好地将LT方法与AT对抗训练框架相结合</li></ol><p>针对以上两个问题, 作者提出了包含尺度不变分类器(scale-invariant classifier)和二阶段再平衡(re-balancing)的Robust and Balanced训练框架, 即RoBal.</p><h3 id="scale-invariant-classifier"><a href="#scale-invariant-classifier" class="headerlink" title="scale invariant classifier"></a>scale invariant classifier</h3><p>经典的线性分类器的logit计算可以写为如下的数学形式:</p><script type="math/tex; mode=display">W_i^T \cdot f(x) + b_i = \| W_i \| \cdot \| f(x) \| \cdot cos\theta_i + b_i \tag{6}</script><p>针对其中的$W$项作者进行了尺度不变的思路延申. 在AT的框架下, 最初可以被正确分类的样本可以根据是否产生有效的对抗样本而分为两类. 通过计算scaling ratio(如下式(7)所示), 可以观察这两类不同图像的分布情况, 结果表明可以产生有效对抗样本的类别有更高的scaling ratio.</p><script type="math/tex; mode=display">\frac{\| f(x + \delta) \|}{\| f(x) \|} \tag{7}</script><p>对于不同类别的scaling ratio分布如下图所示, 由上至下分别是原始样本和加入扰动后的对抗样本的scaling ratio和图像数目的分布情况:</p><figure style="text-align: center;">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251006182337.png" style = "height: 400px; width: 300"></figure><p>对于尺度不变的自然想法就是引入cosine classifier, 降低权重向量和特征向量的模长对分类的影响. </p><h3 id="two-stage-re-balancing"><a href="#two-stage-re-balancing" class="headerlink" title="two-stage re-balancing"></a>two-stage re-balancing</h3><p>在从AT的角度出发引入cosine classifier后, 针对训练数据的长尾分布, 作者对$cos\theta_i$ 和 $b_i$这两个变量提出了对应的改进形式. 一个被广泛使用的对偏置项的改进就是加入类别特定(class-specific)的偏置项, 即 $b_i = \tau_b log(n_i)$ , 而改进后的CE损失函数如下所示:</p><script type="math/tex; mode=display">L_0 = -log(\frac{e^{z_y + b_y}}{\sum_i e^{z_i+b_i}}) = log(1 + \sum_{i \neq y} e^{z_i - z_y + \tau_b log(n_i / n_y)}) \tag{8}</script><p>上式中的$\tau_b$使超参数, 用于控制偏置值的计算. $n_i$则为第i类的样本数量. 对于上述的引入样本数量的偏置值的设定, 由于模型训练过程中对损失函数的优化目标使使之最小化, 以CE为例的损失函数需要模型输出的logits和真实标签构成的one-hot向量, 而当构建上述偏置项后, 在损失函数的计算过程中相当于直接在logits上加入一个固定的向量, 使数量多的样本的logits天然偏大, 而真实标签是那些少量样本时, 在面对模型天然倾向误判为大样本的前提下会在训练中促使模型输出更大的原始logits的小样本项的值, 从损失函数反向传播的贡献上即小样本项会带来更大的梯度矩阵, 从而解决LT的问题. </p><p>然而, 从类间差距的角度出发(在cosine classifier的背景下, 这个差距就是不同类别在球空间中被分配的角度范围间的角度边界距离), 当 $n_y$ 明显大于 $n_i$时, CE损失有被缩小的倾向, 加入的偏置项有negative的作用. 为了解决这个问题, 作者提出了一个类别感知的间隔, 为头部的样本数量大的类别分配了更大的margin value作为补偿. 对应的margin value的数学表达形式如下:</p><script type="math/tex; mode=display">m_i = \frac{\tau_m}{s} log \frac{n_i}{n_{min}} + m_0 \tag{9}</script><p>上式中的$\tau_m$为用于控制m变化的超参数, s代表temperature, 用于拓展cosine形式下的输出值域. 结合margin value的CE损失函数的数学表达形式如下所示:</p><script type="math/tex; mode=display">L_1 = -log(\frac{e^{s(cos\theta_y - m_y) + b_y}}{e^{s(cos\theta_y - m_y) + b_y} + \sum_{i \neq y} e^{s \cos \theta_i + b_i } }) \tag{10}</script><p>以笔者个人的理解来看, 本文作者在这里的改进的核心就是高级版的复杂版的<strong>水多加面, 面多加水</strong>, 由于softmax和交叉熵损失的计算固定, 仅就对模型输出的干净logits值的改动来看作者的改进. 原始的偏置项的作用可用下式表示:</p><script type="math/tex; mode=display">Logits_{final} = Logits_{original} + \mathbf{b} \tag{11}</script><p>其中的向量$\mathbf{b}$的分布特征是类别的数量越多, 对应位置分配的值越大, 却恒为正数. 在cosine classifier的背景下Logits的计算过程如下:</p><script type="math/tex; mode=display">Logits_{original} = \cos \theta_i = \cos(f(x), W) \tag{12}</script><p>而作者的改进相当于对原始的Logits先减去一项, 再在后面加上一项, 反复进行拉扯来平衡最终的Logits的效果, 且当$i \neq y$ 时不执行减去的操作. 最终从Logits的视角模型的输出的数学形式如下所示:</p><script type="math/tex; mode=display">Logits = \cos \theta_i - s \cdot m_i + b_i = \cos \theta_i - \tau_m log(\frac{n_i}{n_{min}}) - m_0 + \tau_b log(n_i) \tag{13}</script><p>只能说作者从一个刁钻的角度(AT训练)证实了加入采样偏置项带来的trade-off, 但提出的解决方案很难说有多么高明. 最后, 受AT训练框架的启发, 作者提出了在最终的训练损失中加入KL散度, 来制约对抗样本和干净图像之间的分布差距(可以理解为一种高级的范数约束, 散度是用来衡量两个概率分布间的相似情况). 最终的损失函数的数学表达形式如下:</p><script type="math/tex; mode=display">L = L_1(x+\delta, y) + \alpha \cdot KL(\tilde{W} \cdot \tilde{f}(x + \delta), \tilde{W} \cdot \tilde{f}(x)) \tag{14}</script><p>对抗样本的产生则是基于vanilla CE loss. 此外, 对于two-stage的定义, 则是作者在推理阶段也对模型所用的偏置项进行了改进, 具体为减去一个采样偏置项来放大对小样本量类别的注意, 对应的数学表达形式如下:</p><script type="math/tex; mode=display">\arg \max_{i \in [C]} s \cdot \cos \theta_i - \tau_p log(\frac{n_i}{\sum_j n_j}) \tag{15}</script><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>作者进行了详尽的消融实验和结果分析. 这里仅展示相对重要的实验结果. 下图的N与R分别代表natural和robust, 对比结果如下:</p><figure style="text-align: center;">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251006205753.png" style = "width: auto; height: 270px"></figure><p>对于不同的数据长尾分布情况(即imbalance ratio, IR不同), 作者也进行了消融实验, 结果如下:</p><figure style="text-align: center;">    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20251006210043.png" style = "width: auto; height: 350px"></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文最核心的创新视角在于用对抗样本和对抗攻击漂亮地解释了长尾分布方法中偏置项设置可能带来的trade-off, 说明对抗样本作为解释工具的巨大应用前景.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
关于结合LT分布与AT(Adversarial Training)的论文笔记
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="对抗样本" scheme="http://boremycin.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
    <category term="对抗防御" scheme="http://boremycin.github.io/tags/%E5%AF%B9%E6%8A%97%E9%98%B2%E5%BE%A1/"/>
    
  </entry>
  
  <entry>
    <title>局域网多设备协同与文件迁移</title>
    <link href="http://boremycin.github.io/2025/09/30/code5/"/>
    <id>http://boremycin.github.io/2025/09/30/code5/</id>
    <published>2025-09-30T02:14:47.375Z</published>
    <updated>2025-10-04T02:15:06.482Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">如何优雅地在一个物理桌面空间中使用两台(win)电脑.</p><br><span id="more"></span></p><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>新学期开学后老师为我配备了新的主机(花了8k大洋买了i7-14700 + 3050 8G + &lt;500w整机电源的主机…), 然而我的重要的学习资料, 工作软件和代码文件等都已经高度依赖本科时期的笔记本, 且自己是严重的多屏用户, 工位桌面往往需要3块(及以上)的屏幕. 有了新主机后为了更优雅地使用两台独立的win电脑, 同时尽可能的便利文件备份与屏幕协同, 我探索了下面的各类方案, 且在使用体验上目前来看还算不错, 故与诸君分享之.</p><h1 id="配置与布局"><a href="#配置与布局" class="headerlink" title="配置与布局"></a>配置与布局</h1><p>两台电脑均为win系统, 通过一个路由器的两根网线组成LAN的局域网连接, 在物理布局上为主机配置一个单独显示器, 桌面中间放置多hdmi通道的显示器, 和笔记本和主机各连接一个hdmi信道, 左侧放置笔记本, 最终的桌面布局如下.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/5e3f241a6880a876040a6b1b04915f00.jpg" alt="5e3f241a6880a876040a6b1b04915f00"></p><h1 id="Act-as-One"><a href="#Act-as-One" class="headerlink" title="Act as One"></a>Act as One</h1><p>在部分场景下我还是会希望将笔记本作为唯一的主机并使另外两个屏幕作为拓展屏使用. 对于中间的屏幕, 只需要调整hdmi的通道; 如果想拓展连接到另一个主机的屏幕, 只需要借助ToDesk软件即可(这个软件在多设备协同方面真的非常完善). 将另一台设备加入到本地的ToDesk设备列表后, 使用基础连接中的拓展屏功能即可, 另一台屏幕就会作为通过网络连接进行映射的副屏, 在校园网环境下的延迟很低, 基本可以实现流畅使用. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930110818.png" alt="20250930110818"></p><h1 id="Make-Two-Act-as-One"><a href="#Make-Two-Act-as-One" class="headerlink" title="Make Two Act as One"></a>Make Two Act as One</h1><p>也有部分场景, 我希望两台电脑都保持开机并同时使用两个系统, 在同一个物理桌面中实现的关键在于两点: 1. 使用同一套键鼠在近乎无延迟的情况下操作两台电脑, 同时可以共享复制板; 2. 两台电脑的文件可以互相自由的访问, 实现局域网下的文件共同管理和操作. 幸运的是, 针对以上两个问题都有成熟且便捷的解决方案. </p><h2 id="一套键鼠-多个设备"><a href="#一套键鼠-多个设备" class="headerlink" title="一套键鼠, 多个设备"></a>一套键鼠, 多个设备</h2><p>微软针对上述场景开发了名为<a href="https://www.microsoft.com/en-us/download/details.aspx?id=35460">无界鼠标</a>(Mouse without Borders)的成熟软件, 在两个设备上同时下载后进行简单配置即可实现同一套键鼠的跨屏操作. 下载完成后首先会出现选择当前设备为主设备或从设备的选项界面(实测后续connect成功后可以流畅双向控制), 界面如下所示:</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930115522.png" alt="20250930115522"></p><p>如果选择yes作为主设备, 会跳出填写界面用于写入从设备的信息; 选择no后则会自动显示对应信息. 输入完成后跳转出如下界面, 简要介绍了mwb软件的功能:</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930115802.png" alt="20250930115802"></p><p>连接完成后会出下如下界面, 可以进一步通过拖拽来映射屏幕间的排序关系并管理其他高级功能.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930120003.png" alt="20250930120003"></p><h2 id="局域网下的文件共享"><a href="#局域网下的文件共享" class="headerlink" title="局域网下的文件共享"></a>局域网下的文件共享</h2><p>首先是文件的访问与共享, 在局域网环境下通过win的网络磁盘映射即可轻松实现. 首先找到例如桌面的目标文件夹, 右键文件夹-&gt;属性-&gt;共享, 由于局域网中可能有其他同学的多台设备, 出于安全考虑, 将<code>同时共享的用户数量</code>限制为1台, 设置权限为读取. 此外还需要在shell中通过<code>ipconfig</code>命令查找当前设备的ipv4地址用于另一台设备的连接. 在另一台设备中使用<code>win + R</code>调出运行框, 输入<code>\\IP</code>来打开对应设备文件夹, 其中的ip即为上述的ipv4地址. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930125858.png" alt="20250930125858"></p><p>完成以上操作后即可成功打开相应的文件夹并进行文件的跨设备管理. 此外, win11还提供了通过网络凭证进行局域网文件共享的方法, 在文件管理器的网络栏中选择目标设备(在初始化win电脑时建议起个名字, 会比默认编号更方便查找), 系统会提示输入网络凭证. 需要注意的是, 这里的凭证一般指本地凭证, 而非登入时所用的微软账号. 如果还没有创建本地凭证, 需要以管理员的方式打开cmd(在搜索栏中搜索cmd, 并选择管理员身份打开), 通过以下命令即可快速创建本机的网络凭证的用户名和密码, 其中的<code>xxxx</code>为设定的用户名, <code>yyyyyy</code>为对应密码. 连接成功后可以打开的仍仅为上述设置已共享的文件夹.</p><figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">net</span> user xxxx yyyyyy /add</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930133344.png" alt="20250930133344"></p><h2 id="局域网下的文件同步"><a href="#局域网下的文件同步" class="headerlink" title="局域网下的文件同步"></a>局域网下的文件同步</h2><p>在完成文件共享后, 对于部分常用文件夹(如paper, blog等), 仍然希望可以自动化地进行同步. 这里微软也给出了成熟优雅的软件解决方案(win means WIN!). 使用<code>SyncToy</code>软件可以快速实现局域网下的文件夹同步, 虽然微软已不再提供官方下载渠道(且该软件已超10年没有更新, 可见其功能之完备), 但互联网上有许多下载资源. 需要注意的是该软件依赖<code>.NET3.5</code>框架, win11系统可能需要单独设置打开: 首先打开<code>控制面板</code>, 选择<code>程序</code>中的<code>启用或关闭Windows功能</code>, 最后勾选.NET3.5即可. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930140150.png" alt="20250930140150"></p><p>在两台设备上的SyncToy均下载完成后, 启动软件并将本地文件夹和通过网络进行定位的目标文件夹创建为新的文件夹对(Folder Pair), 点击<code>Run</code>后就会自动进行配对和同步. 此外软件提供了三种文件同步的模式, 分别为镜像同步(即双向完全同步), 单向同步和增量同步(不进行删除操作), 一般选择镜像同步即可.  </p><p> <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930140921.png" alt="20250930140921"></p><p>至此, 文件夹的同步仍需要每次手动运行SyncToy软件, 如需更加自动化的设置, 则需要借助win系统提供的<code>任务计划程序</code>. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930141521.png" alt="20250930141521"></p><p>打开后创建基本任务, 触发器设定为<code>每天</code>. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930141643.png" alt="20250930141643"></p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930141654.png" alt="20250930141654"></p><p>设定<code>操作</code>为<code>启动程序</code>, 并加入SyncToy的exe可执行文件路径, 并在<code>添加参数</code>项中加入<code>-R</code>(会同步所有的Folder Pair), 至此即完成了自动化的设备文件夹同步.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250930143314.png" alt="20250930143314"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>经过上述的折腾, 笔者目前已经是可以流畅地同时使用两台设备了, 看起来后期的操作便利性值得前期的时间投入成本. </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
如何优雅地在一个物理桌面空间中使用两台(win)电脑.
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="笔记" scheme="http://boremycin.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="工程开发" scheme="http://boremycin.github.io/tags/%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：AI-GAN和Meta-GAN</title>
    <link href="http://boremycin.github.io/2025/09/16/paper8/"/>
    <id>http://boremycin.github.io/2025/09/16/paper8/</id>
    <published>2025-09-16T07:01:18.000Z</published>
    <updated>2025-09-16T08:37:12.165Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">关于2篇基于GAN网络的对抗攻击算法论文的笔记</p><br><span id="more"></span></p><h1 id="AI-GAN"><a href="#AI-GAN" class="headerlink" title="AI-GAN"></a>AI-GAN</h1><h2 id="发表去向"><a href="#发表去向" class="headerlink" title="发表去向"></a>发表去向</h2><p>发表于21年IEEE ICIP</p><h2 id="创新点总结"><a href="#创新点总结" class="headerlink" title="创新点总结"></a>创新点总结</h2><p>Attack-Inspired GAN在原始的GAN生成网络的基础上引入了额外的对抗器(attacker)加入到GAN网络的训练过程中, 使原有的生成模型可以高效地生成针对特定图像和特定类别的<strong>扰动矩阵</strong>, 即实现有目标攻击(target attack). 对于已有的利用GAN的对抗算法, 常常存在以下问题: 生成能力受限, 每次推理只能生成实现一种特定类别的攻击, 对于不同类别需要重复训练; 难以泛化到真实世界数据集中(测试的数据集局限在MNIST或CIFAR). 针对上述算法局限, AI-GAN实现了以下创新:</p><ol><li>改进GAN网络结构, 在传统的生成器G-判别器D结构外引入了攻击器Attacker, 三者同时训练</li><li>提升了对抗样本在更接近现实场景分布的数据集中的攻击效果</li><li>展现了一定的迁移性, 同时优于经典对抗算法</li></ol><h2 id="算法内容"><a href="#算法内容" class="headerlink" title="算法内容"></a>算法内容</h2><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250914192636.png" alt="20250914192636"></p><p>AI-GAN的整体架构如上图所示. 其中的判别器D被设计为双头判别器(可以理解为有两个分类器对应多个输入), 生成器G负责生成针对特定类别的扰动矩阵, 需要输入原始图像和目标攻击类别. 对于判别器, 除了干净图像和由生成器G获取的对抗样本外, 还会额外输入由攻击器A生成的另一个对抗样本, 文中A被设定为PGD算法. 因此在训练过程中, 判别器不仅需要判断输入图像是真实图像还是扰动后的图像, 还需要额外对扰动图像的来源进行分类. </p><p>对于判别器G的具体设置, 其包含了两个分支(branches), 其中一个被用来分辨干净图像和扰动图像, 另一个用来分辨扰动图像是由G产生还是由A产生. 此外, 为了提高生成器G的对抗性能, 作者还加入了对抗训练的流程. 此外, 设计上述的鲁棒的判别器D还可以帮助训练过程的稳定和加速收敛. 最终, 判别器D的损失函数包含了以下部分: 针对干净-扰动图像分类的$\mathcal{L}_S$, 针对扰动图像来源分类的$\mathcal{L}_{C}$, 完整的表达形式如下:</p><script type="math/tex; mode=display">\mathcal{L}_S = E[logP(S = clean |x)] + E[logP(S = perturbed | x')]</script><script type="math/tex; mode=display">\mathcal{L}_{C} = \mathcal{L}_{C(adv)} + \mathcal{L}_{C(pert)} = E[logP(C = y |x'')] + E[logP(C = y| x')]</script><script type="math/tex; mode=display">\mathcal{L}_{D} = \mathcal{L}_{S}  + \mathcal{L}_{C}  \tag{1}</script><p>上式中的y指输入图像的真实类别, 训练过程中判别器的优化目标是最大化上述损失函数.</p><p>对于生成器, 作者从提升其可拓展性(scalability)的角度出发, 提出了通过自监督方式对编码器进行预训练的方法(但论文中并未明确说明实现过程). 预训练编码器使模型近似于在特征空间进行扰动攻击, 某种方式上提升了对抗样本呢的迁移性, 此外额外引入的多头判别器D也使生成器G的对抗性能得到进一步的提升. 对于生成器G的损失函数, 包括以下部分: 度量对目标模型攻击效果的$\mathcal{L}_{target(adv)}$ , 对于判别器的攻击损失$\mathcal{L}_{D(adv)}$, 以及上述判别器D中的损失函数项$\mathcal{L}_{S}$, 具体的数学表达形式如下:</p><script type="math/tex; mode=display">\mathcal{L}_{target(adv)} = E[logP(C_1 = t|x')]</script><script type="math/tex; mode=display">\mathcal{L}_{D(adv)} = E[logP(C_2 = t|x')]</script><script type="math/tex; mode=display">\mathcal{L}_G = \mathcal{L}_{target(adv)} + \mathcal{L}_{D(adv)} - \mathcal{L}_S \tag{2}</script><p>上式中的t为对抗目标类别, $C_i$代表不同分类器的分类结果, 优化目标是最大化损失函数. </p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>模型主要比较了自身算法与较早提出的基于GAN的对抗攻击算法AdvGAN之间的攻击效果的差异(比较的方法较少), 仍然是在MNIST和CIFAR-10数据集上的实验结果如下. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250914205016.png" alt="20250914205016"></p><p>此外, 作者在对抗防御场景下将AI-GAN与众多经典算法进行了比较, 该场景的设置背景为使用对抗样本混合原始样本进行对抗训练, 该过程中的对抗训练的样本来源于FGSM和PGD等算法. 在AI-GAN的训练过程中由于需要用到待攻击目标模型进行G的训练, 在训练过程中则不加入对抗训练流程, 最终的实验结果如下. 可以看出在施加了对抗防御的背景下模型的攻击性能即使仍是最优也出现了明显的下降, 且作者标注了前二好的算法结果来夸大AI-GAN的SOTA性质.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250914205450.png" alt="20250914205450"></p><p>最后作者就可拓展性的问题进行了专门的实验, 其实质就是在更复杂的CIFAR-100数据集上进行了简单验证, 仅就AI-GAN的实验结果而言, 在白盒攻击场景下仍然实现了87.76%的ASR.</p><h2 id="论文总结"><a href="#论文总结" class="headerlink" title="论文总结"></a>论文总结</h2><p>论文里出现了较多定义不清和表述模糊的地方, 实验开展和性能比较部分也不够完备, 但在攻击角度下的GAN结构的改进, 尤其是额外的攻击器的引入, 仍有较强的启发性. </p><h1 id="Meta-GAN-attack"><a href="#Meta-GAN-attack" class="headerlink" title="Meta-GAN attack"></a>Meta-GAN attack</h1><h2 id="发表去向-1"><a href="#发表去向-1" class="headerlink" title="发表去向"></a>发表去向</h2><p>由2021年iccv拓展而来, 发表于IEEE TIFS, 论文原题为Robust and Generalized Physical Adversarial Attacks via Meta-GAN</p><h2 id="创新点总结-1"><a href="#创新点总结-1" class="headerlink" title="创新点总结"></a>创新点总结</h2><p>为实现数字域对抗样本图像在真实物理场景下的有效性, 当前的物理对抗攻击算法往往关注对各种物理变化的模拟并将其融合进对抗样本的生成过程, 然而这种方式往往较大的人力成本来针对新的数据和模型模拟物理变化, 同时算法迭代优化对抗扰动时的时间开销较大. 针对上述局限, 论文提出了一种鲁棒和可泛化的物理域对抗攻击算法, 名为Meta-GAN Attack, 通过GAN模型的引入, 该算法不仅能够生成可物理部署的对抗样本, 还融合了元学习的思想, 使模型在攻击未知的模型目标时仅仅通过少量的数字和物理图像实现攻击性能的泛化, 可以实现类别无关和模型无关(class-agnostic and model-agnostic)的元学习效果. 具体创新点总结如下:</p><ol><li>基于GAN模型完成D2P中的图像颜色与色彩失真的模拟过程, 生成物理可部署的对抗样本. 另外引入额外的CycleGAN网络实现数字图像到物理重采集图像的转换过程, 提升了算法的class-agnostic能力(难以获得未知数据集的真实物理图像)</li><li>定义class-agnostic and model-agnostic的few-shot场景, 设计了对应的meta learning算法(CMML)提升GAN网络的未知场景泛化性能</li><li>在两个不同数据集下开展了较为完备的验证实验</li></ol><h2 id="算法主要内容"><a href="#算法主要内容" class="headerlink" title="算法主要内容"></a>算法主要内容</h2><h3 id="算法背景"><a href="#算法背景" class="headerlink" title="算法背景"></a>算法背景</h3><p>本文所定义的物理域攻击场景主要是指数字域对抗样本生成-打印-相机重采集-部署攻击的流程, 如下图流程所示. 这种打印-重采集的顺序变换被称为digital-to-physical(D2P) transformation. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250915190529.png" alt="20250915190529"></p><p>由于相机与目标间的距离变化, 视角变化和打印材质的特征变化等因素, 物理域攻击往往比数字域攻击更不可控. 上述D2P的过程的核心是为重采集后的<strong>数字图像</strong>引入色彩和形状失真(distortion). 为了使对抗样本在D2P后仍能保持对抗性能, 物理域对抗算法往往在样本迭代过程中加入对D2P的模拟操作.</p><p>然而, 面对实际部署过程中可能出现的模型未知与获取的目标图像数据集的类别和每类图像的数量都受限, 因此该场景的攻击可以被定义为few-shot的攻击场景, 通过meta learning算法的引入可以使已被完整训练的模型和算法参数在较小数据量的基础上通过少数轮次的训练即可实现较为理想的攻击效果.</p><p>特别地, 对于D2P的转换过程, 作者根据不同的情况进行可能的失真原因的分解(失真可能使多种原因叠加导致的), 包括1:1 D2P, 意为重采集图像和原始数字图像间为大小等比关系, 没有发生形状上的改变, 但可能由于打印设备/采集设备的原因造成像素值的失真; 空间形变(Spacial Transformation), 即由于相对位置和视角变化导致的图像形变. 对于上述变化情况, 相应的示意图如下. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250915204048.png" alt="20250915204048"></p><h3 id="结构设计与算法核心"><a href="#结构设计与算法核心" class="headerlink" title="结构设计与算法核心"></a>结构设计与算法核心</h3><p>算法的核心架构为用于生成对抗扰动的GAN网络, 其中的生成器G会在生成对抗图像的同时模拟D2P过程中的颜色和形状失真. 对于元学习部分, 可以理解为在训练完成的GAN模型基础上利用少数新数据集的数据进行微调, 继而实现攻击性能的泛化. 对于GAN模型的损失函数设计部分, 论文从三个角度展开:</p><ol><li>满足数字域图像攻击</li><li>对1:1 D2P鲁棒</li><li>对Spacial Transformation鲁棒</li></ol><p>对于第一部分, 优化的目标函数如下. 式中的$\mathcal{L}_{adv}$是待攻击目标模型的交叉熵损失, $f$为对应分类模型, $y_t$为指定攻击类别.</p><script type="math/tex; mode=display">\min_G \mathcal{L}_{adv}(f(G(x)), y_t) \tag{1}</script><p>对于第二部分, 论文指出可以通过引导数字域的对抗样本与重采集后的物理图像的相似程度实现对1:1 D2P的鲁棒性, 因此可以将GAN作为域迁移的手段(从数字图像$\chi_d$到物理图像$\chi_p$), 对应的GAN网络损失函数如下, 式中的$D$为判别器, $G$为生成器.</p><script type="math/tex; mode=display">\mathcal{L}_{GAN} = \mathbb{E}_{x_p \in \chi_{p}}[logD(x_p)] + \mathbb{E}_{x \in \chi_{d}}[log(1 - D(G(x)))] \tag{2}</script><p>对于第三部分, 受EOT方法的启发, 论文引入了多个空间变换函数, 如旋转, 平移, 缩放和仿射变换等($t \in T$)来模拟物理场景下的形状失真, 然后基于一系列合成的伪变化图像进行训练来提升对空间变化失真的鲁棒性, 对应的优化方程如下(融合入式(1)):</p><script type="math/tex; mode=display">\min_G \mathbb{E}_{t \in T}[\mathcal{L}_{adv}(f(t(G(x))), y_t)] \tag{3}</script><p>特别的, 对于上述不同类型的函数t的构造, 都可以通过参数可变的统一齐次变换矩阵实现(不同的变换效果对应不同的参数), 统一后的变换表达式如下:</p><script type="math/tex; mode=display">\begin{bmatrix}w_i' \\h_i' \\1\end{bmatrix}=\mathbf{A} \cdot \begin{bmatrix}w_i \\h_i \\1\end{bmatrix}=\begin{bmatrix}a_1 & a_2 & a_3 \\a_4 & a_5 & a_6 \\0 & 0 & 1\end{bmatrix}\cdot\begin{bmatrix}w_i \\h_i \\1\end{bmatrix}\tag{4}</script><p>将EOT过程转换为上述的统一齐次变换表达形式的好处是确保了函数$t$的可导性质(变换本身是一种线性组合), 方便后续将图像矩阵作为自变量的梯度求解过程. 综合上述各部分损失函数, 最终的GAN网络损失函数表达形式如下:</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{GAN}(x, x_p; G, D) + \lambda \cdot \mathbb{E}_{t \in T}[\mathcal{L}_{adv}(f(t(G(x))), y_t)] + c \cdot \|G(x) - x_p\|_p \tag{5}</script><p>上式中的最后一项距离度量是为了限制对抗样本的形变幅度, 保持其视觉上的不可感知性. </p><p>基于上述损失函数的说明, 在训练过程中需要有足够的数字域图像和物理域图像构成的图像对来完成对GAN网络的训练, 而针对元学习的场景, 对于未知任务下的数据集, 需要为各个类别手动构造few-shot的图像对. 论文里元学习场景下的class-agnostic and model-agnostic可以理解为用于生成对抗样本的Attack GAN面对不同的任务$\tau_i$, 每个任务都由用于微调训练的少量样本数据集$D_s$, 用于测试生成对抗样本性能的测试数据集$D_q$和待攻击的目标分类器$f$, 不同任务之间上述变量各不相同. 从多个任务的宏观视角出发, 设定共有n个任务, 其中的n-1个任务都可以用于Attack GAN的微调, 最后一项任务作为彻底的测试项. 在该场景下的Attacker GAN的更新流程如下:</p><p>首先在单个任务$\tau$上微调Attack GAN中的G和D</p><script type="math/tex; mode=display">\theta'_{G,\tau_i} = \theta_{G} - \alpha \nabla_{\theta_G}\mathcal{L}_{\tau_i}(\theta_G, \theta_D, f_i, D_s^{\tau_i})</script><script type="math/tex; mode=display">\theta'_{D,\tau_i} = \theta_{D} + \alpha \nabla_{\theta_D}\mathcal{L}_{\tau_i}(\theta_G, \theta_D, f_i, D_s^{\tau_i}) \tag{6}</script><p>然后将所有任务的损失项求和作为总的meta损失, 表达形式如下:</p><script type="math/tex; mode=display">\mathcal{L}^{meta} = \sum_{\tau_i \in p(\tau)} \mathcal{L}_{\tau_i}(\theta_{G,\tau_i}',\theta_{D,\tau_i}', f_i, D_s^{\tau_i}) \tag{7}</script><p>然后对Attack GAN的整体进行参数更新的数学表达形式如下:</p><script type="math/tex; mode=display">\theta_G = \theta_G - \beta \nabla_{\theta_G} \mathcal{L}^{meta}</script><script type="math/tex; mode=display">\theta_D = \theta_D + \beta \nabla_{\theta_G} \mathcal{L}^{meta} \tag{8}</script><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250916153051.png" alt="20250916153051"></p><p>在上述场景中还存在另外的较为显著的限制, 即在各个任务的$D_s$中物理图像和数字图像不完全成对或由于获取难度的限制用于训练的物理图像的数量会显著少于真实图像. 为解决对应问题, 论文引入了额外的Cycle GAN作为数字图像到物理图像的域转换器, 应用流程如上图所示. 根据Cycle GAN的定义, 其模型中会包含两个生成器G和两个判别器D, 其结构的特征的通过循环迭代的方式来避免严格成对的训练数据的依赖, 相应的损失函数的表达形式如下:</p><script type="math/tex; mode=display">\begin{aligned}\mathcal{L}_{\text{CycleGAN}} &= \mathbb{E}[\log D_p(x_p)] + \mathbb{E}[\log (1 - D_p(G_{d \rightarrow p}(x_d)))] \\&\quad + \mathbb{E}[\log D_d(x_d)] + \mathbb{E}[\log (1 - D_d(G_{p \rightarrow d}(x_p)))] \\&\quad + \mathbb{E}\!\left[\,\|G_{p \rightarrow d}(G_{d \rightarrow p}(x_d)) - x_d\|_1 \right] \\&\quad + \mathbb{E}\!\left[\,\|G_{d \rightarrow p}(G_{p \rightarrow d}(x_p)) - x_p\|_1 \right]\end{aligned}\tag{9}</script><p>对于该额外框架内的应用在文中没有非常清晰的说明, 可以理解为对不同任务下的$D_s$的数据补齐, 最后, 全文的算法框架如下图所示:</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250916153135.png" alt="20250916153135"></p><h2 id="实验设计与验证结果"><a href="#实验设计与验证结果" class="headerlink" title="实验设计与验证结果"></a>实验设计与验证结果</h2><p>本文在实验部分设计了四个对照实验, 使用的数据集为ImageNet和GTSRB(German Traffic Sign Recognition Benchmark)</p><p>实验1为在白盒场景下攻击所有数据可获取的数据集和目标模型, 相应的实验结果如下.</p><figure>    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20250916154150.png" style = "width:auto; height = 150px"></figure><p>实验2,3分别为在已知模型和未知数据以及未知模型和已知数据上进行了相应测试, 此处不再赘述. 实验4为未知模型和未知数据上的测试, 对应结果如下图所示.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250916161852.png" alt="20250916161852"></p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Meta-GAN融合了GAN网络和物理攻击场景, 同时加入了meta learning的相应概念, 其中利用多个GAN模型分别进行对抗样本生成和不同分布下的图像域转换的思路也可以融合到SAR图像的仿真-实测的图像域转换场景中. </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
关于2篇基于GAN网络的对抗攻击算法论文的笔记
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>TPS代码完整复现与分析</title>
    <link href="http://boremycin.github.io/2025/09/09/code4/"/>
    <id>http://boremycin.github.io/2025/09/09/code4/</id>
    <published>2025-09-09T08:22:14.619Z</published>
    <updated>2025-09-11T12:35:21.570Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">TPS图像扭曲算法的代码完整复现与对所有代码的详细分析.</p><br><span id="more"></span></p><h1 id="算法背景"><a href="#算法背景" class="headerlink" title="算法背景"></a>算法背景</h1><p>TPS(Thin Plate Spline)薄板样条插值是图像插值方法的一种, 是常用的2D平面插值方法. 该形变算法输入两组成对的坐标源点和目标变化点, 其核心思想是在尽量满足所有成对变化点匹配的情况下是使图像的形变量最小(类比于将一个平直薄板进行弯折来对齐特定点坐标, 同时使薄板的弯折程度尽可能降低). TPS算法可以应用于几乎所有的生物姿态形变, 同时在人面部图像变化中也有相关应用.</p><figure>    <img src=https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911203507.png>    <figcaption style="text-align: center;">TSP形变示意图</figcaption></figure><p>TPS可以大致分为两部分, 分别是<strong>形变参数求解</strong>和<strong>图像坐标变换填充</strong>. 对于第一部分, 算法需要求解一个差值函数. 假设有N对坐标点作为控制点(src和dst, 每组中的坐标一一对应), 希望求解出一个函数可以满足上述的所有变换. 函数中选择了径向基函数来完成上述过程, 通过给定的控制点求解该函数的参数.</p><p>径向基函数(Radial Basis Function, RBF)是一种以<strong>点间距离</strong>为自变量的标量函数, 形式如下式(1). 下式(2)为对应的线性组合形式. 对于式(2), $c_j$为控制坐标对的第j个中心, $w_j$为线性权重, $\phi$为选定的径向基函数, $p(x)$为增加的低阶多项式. </p><script type="math/tex; mode=display">\phi(r) \ , \ r = \|x-c\|  \tag{1}</script><script type="math/tex; mode=display">f(x) = \sum_{j=1}^Nw_i \phi(\|x-c\|) + p(x) \tag{2}</script><p>在TPS算法中,径向基函数选择的是如下式的双调和方程. 该函数天然满足如式(4)的方程, 该径向基函数是双调和函数的基础解. </p><script type="math/tex; mode=display">U(r) = r^2 ln(r) \tag{3}</script><script type="math/tex; mode=display">\Delta^2 U = (\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2})^2 \propto \delta_{(0,0)} \tag{4}</script><p>上文提到, TPS算法希望在模拟弯曲的过程中整体形变量最小, 可以用弯曲能量的概念进行约束. 弯曲能量可以被定义为函数二阶导数的平方对二维实数域的积分, 形式如下. 优化的目标是使该能量积分最小.</p><script type="math/tex; mode=display">I[f(x,y)] = \int \int (f_{xx}^2 + 2 f_{xy}^2 + f_{yy}^2)dxdy \tag{5}</script><p>对于形变函数$f$, 由于2D平面的坐标变化可以理解为X方向和Y方向各自的独立变换, 因此TPS算法利用两个独立的形变函数(被称为样条函数)计算各个方向上坐标的位移. 样条函数的定义如下:</p><script type="math/tex; mode=display">f_{x'}(x,y) = a_1 + a_x x + a_yy + \sum_{i=1}^N w_iU(\|(x_i,y_i) - (x,y)\|)</script><script type="math/tex; mode=display">f_{y'}(x,y) = a_1 + a_x x + a_yy + \sum_{i=1}^N w_iU(\|(x_i,y_i) - (x,y)\|) \tag{6}</script><p>需要注意的是函数求解的结果不是2D平面下的绝对坐标, 而是基于初始点坐标的相对位移. 上式中的系数$a_i$用于线性空间拟合, 系数$w_i$用于对<strong>每个</strong>控制点进行约束, 约束对象是源点和目标点之间的位移标量, 通过函数$U$获取, 结合RBF函数的定义, 该函数作为一种Gaussian Kernel来用于衡量输入量之间的相似性.</p><figure>    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250910194751.png" style = "width: auto; height: 500px">    <figcaption style="text-align: center;">U(x)函数图像</figcaption></figure><h1 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h1><p>根据上文分析可知, 算法的输入部分是需要进行TPS形变和目标图像和成对的源点-目标点坐标对. 需要注意的是算法输入的是归一化的相对坐标(即假定图像的长宽均为1, 坐标以[0-1]之间的连续数进行表示), 且由于算法在输出形变图像时会对所有采样坐标都进行形变, 为了放置图像的边缘部分出现不合理的扭曲, 造成黑边等效果, 需要在输入时加入anchor(即位置在图像边缘或者中心, 源点和目标点保持一致)来进行改善. 此外, 算法输出图像的大小可以通过参数进行控制, 默认为原图尺寸. </p><figure>    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911121354.png" style = "width: auto; height: 500px">    <figcaption style="text-align: center;">不理想形变示意图</figcaption></figure><p>首先是插值方程的参数求解, 由于x, y方向的偏移量是分别计算的, 因此需要求出两组对应各个方向的插值参数. 对于式(6)的求解可以转化为如下的linear system表达形式, 通过<code>np.linalg.solve</code>进行参数求解.</p><script type="math/tex; mode=display">L = \begin{bmatrix}    K & P \\    P^T & O\end{bmatrix} \times\begin{bmatrix}    w \\    a\end{bmatrix}= \begin{bmatrix}    v \\    0\end{bmatrix} \tag{7}</script><p>结合式(6), K包含所有坐标组合的RBF函数$U(x,y)$, P则是输入源点坐标的齐次形式, w与a是对应的系数向量(也是求解目标), v是输入的位移向量(即目标点与源点之间的差值, 求解后的函数输出也是差值). 对于w与$P^T$的部分, 其对应的展开表达如下. </p><script type="math/tex; mode=display">\sum_{i=1}^n w_i = 0, \ \ \sum_{i=1}^n w_i c_i = 0 \tag{8}</script><p>上式子中n为选取的控制点的个数, $c_i$为源点坐标. 式(8)的作用是使双调和函数的能量有限且解位移, 是附加的正交约束. 上述矩阵的展开形式如下图所示. </p><figure>    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911131808.png" style = "width:auto; height: 450px"></figure><p>代码中对于参数<code>theta</code>的求解主要通过<code>theta = tps_theta_from_points(c_src, c_dst, reduced=True)</code>中的<code>tps_theta_from_points</code>函数, 以输入n个控制点为例, 其中的reduced参数作用是仅保留n-1个w参数, 再通过 $\sum w = 0$ 的约束求解被略去的w, 来保证算法的鲁棒性. 具体代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tps_theta_from_points</span>(<span class="params">c_src, c_dst, reduced=<span class="literal">False</span></span>):</span><br><span class="line">    delta = c_src - c_dst</span><br><span class="line">    </span><br><span class="line">    cx = np.column_stack((c_dst, delta[:, <span class="number">0</span>])) <span class="comment"># shape: (n, 3)</span></span><br><span class="line">    cy = np.column_stack((c_dst, delta[:, <span class="number">1</span>]))</span><br><span class="line">        </span><br><span class="line">    theta_dx = TPS.fit(cx, reduced=reduced)</span><br><span class="line">    theta_dy = TPS.fit(cy, reduced=reduced)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> np.stack((theta_dx, theta_dy), -<span class="number">1</span>) </span><br><span class="line">    <span class="comment"># the shape of this return is (n+3-1,2), contains results for x and y</span></span><br></pre></td></tr></table></figure><p>上式中的<code>cx</code>和<code>cy</code>分别是源点和各自方向上的差值组合获得的, 通过<code>fit</code>函数获得对应方向的参数, 并将x, y方向的参数拼接, 这样对特定位置的变量, 都有类似$w_i(x), \  w_i(y)$的参数分量. 上述的<code>fit</code>函数依赖两个子函数, 相应实现如下.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">d</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    Tring to calculate the L2 Loss. </span></span><br><span class="line"><span class="string">    Assume the number of the markpoints is N. The shape of a and b is supposed to be (N,2).</span></span><br><span class="line"><span class="string">    a[xxx] make it (N,1,2), and for b that&#x27;s (1,N,2)</span></span><br><span class="line"><span class="string">    The shape in square function is (N,N,2) </span></span><br><span class="line"><span class="string">    The return shape is (N,N)</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> np.sqrt(np.square(a[:, <span class="literal">None</span>, :<span class="number">2</span>] - b[<span class="literal">None</span>, :, :<span class="number">2</span>]).<span class="built_in">sum</span>(-<span class="number">1</span>))</span><br><span class="line">    <span class="comment"># shape of a:[n,1,2], shape of b:[1,n,2]. a and b is actually the same one.</span></span><br><span class="line">    <span class="comment"># the result is the sqrt of the (x1-x2)^2+(y1-y2)^2, with the shape of (n,n)</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">u</span>(<span class="params">r</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">    The result of U function</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">return</span> r**<span class="number">2</span> * np.log(r + <span class="number">1e-6</span>)</span><br></pre></td></tr></table></figure><p>分析以上代码可知, 函数<code>u</code>为算法中的径向基函数, 函数<code>d</code>以遍历的方式为输入的一维向量构建二维的L2距离范数矩阵. 在参数求解中输入的a与b均为矩阵c ($c_x$ 或 $c_y$), 对矩阵a进行<code>a[:, None, :2]</code>, None会在所在位置增加维度, 原始的a为(n, 3), 扩充后为(n, 1, 2); 矩阵b则变为(1, n, 2), 保留最后维度的前两位即保留源点坐标(x, y). 矩阵相减时利用np的广播机制, 矩阵a广播为(n, n, 2), 先考虑后两个维度, 由(1, 2)广播为(n, 2)实质是当前的坐标重复n次, 再与矩阵b在这个维度上相减时, b则是n个<strong>不同的</strong>坐标, 而b在第一个维度的广播则是考虑到a的第一维度, 将上述过程遍历到a的每一对坐标当中. 简单的示意图如下. </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911144856.png" alt="20250911144856"></p><p>最后再通过作差, 平方和开方的操作, 即获得了(n, n)的标量矩阵, 用于RBF函数的输入矩阵.介绍完<code>u</code>和<code>d</code>函数后, 完整的参数求解函数的代码如下.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">c, lambd=<span class="number">1e-2</span>, reduced=<span class="literal">False</span></span>): <span class="comment"># the input of the fit is lile [x,y,error]        </span></span><br><span class="line">    n = c.shape[<span class="number">0</span>] <span class="comment"># number of the locations </span></span><br><span class="line"></span><br><span class="line">    U = TPS.u(TPS.d(c, c))</span><br><span class="line">    K = U + np.eye(n, dtype=np.float32)*lambd</span><br><span class="line">    <span class="comment"># lambd is a trade-off parameter to make the deformation smoother</span></span><br><span class="line"></span><br><span class="line">    P = np.ones((n, <span class="number">3</span>), dtype=np.float32)</span><br><span class="line">    P[:, <span class="number">1</span>:] = c[:, :<span class="number">2</span>] <span class="comment"># here keep the (x,y) of the destination markpoints</span></span><br><span class="line"></span><br><span class="line">    v = np.zeros(n+<span class="number">3</span>, dtype=np.float32)</span><br><span class="line">    v[:n] = c[:, -<span class="number">1</span>] <span class="comment"># The delta x or y from c</span></span><br><span class="line">    <span class="comment"># shape of v is (n+3,)</span></span><br><span class="line"></span><br><span class="line">    A = np.zeros((n+<span class="number">3</span>, n+<span class="number">3</span>), dtype=np.float32)</span><br><span class="line">    A[:n, :n] = K</span><br><span class="line">    A[:n, -<span class="number">3</span>:] = P</span><br><span class="line">    A[-<span class="number">3</span>:, :n] = P.T</span><br><span class="line"></span><br><span class="line">    theta = np.linalg.solve(A, v) <span class="comment"># p has structure w,a</span></span><br><span class="line">    <span class="keyword">return</span> theta[<span class="number">1</span>:] <span class="keyword">if</span> reduced <span class="keyword">else</span> theta</span><br><span class="line">    <span class="comment"># reduced的作用是对w进行约束, w之和需要为0, 因此用n-1个w就可以知道被reduced的w值</span></span><br></pre></td></tr></table></figure><p>代码中的<code>U</code>为原始的矩阵K定义, 而加入的类单位矩阵的功能是使矩阵更好求解, 减少奇异值问题. 反映到图像变换结果上就是允许差值结果与目标点间出现偏差, 使图像变化更加平滑稳健. 变量<code>P</code>则是构建关于源点的齐次化坐标矩阵. 变量<code>v</code>保留了坐标差值部分用于参数求解. 最终通过矩阵<code>K</code>和<code>P</code>填充矩阵<code>A</code>, 通过np.linalg.solve(A, v)即可求解出参数矩阵, 舍弃 $w_0$ 后即获得输出的参数.  </p><p>回到函数<code>tps_theta_from_points</code>, 分别对x, y方向求解参数后进行拼接, 即为函数的范围值, shape为(n+3-1, 2).至此参数求解部分结束. 第二部分是对所有的原图坐标进行差值变化, 求出相对变化值后叠加在原坐标上获取输出坐标(将原坐标像素映射到输出坐标上即实现图像的扭曲). 算法通过<code>grid = tps_grid(theta, c_dst, dshape)</code>输出各个位置上的相对坐标, 函数的具体代码如下.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tps_grid</span>(<span class="params">theta, c_dst, dshape</span>):    </span><br><span class="line">    ugrid = uniform_grid(dshape)</span><br><span class="line"></span><br><span class="line">    reduced = c_dst.shape[<span class="number">0</span>] + <span class="number">2</span> == theta.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    dx = TPS.z(ugrid.reshape((-<span class="number">1</span>, <span class="number">2</span>)), c_dst, theta[:, <span class="number">0</span>]).reshape(dshape[:<span class="number">2</span>])</span><br><span class="line">    dy = TPS.z(ugrid.reshape((-<span class="number">1</span>, <span class="number">2</span>)), c_dst, theta[:, <span class="number">1</span>]).reshape(dshape[:<span class="number">2</span>])<span class="comment"># the input&#x27;s shape is (x,2), which is sent to function z</span></span><br><span class="line">    dgrid = np.stack((dx, dy), -<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    grid = dgrid + ugrid <span class="comment"># converted grid + original grid, to obtain the absolute coordinates</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> grid <span class="comment"># H&#x27;xW&#x27;x2 grid[i,j] in range [0..1]</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">uniform_grid</span>(<span class="params">shape</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Uniform grid coordinates.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Params</span></span><br><span class="line"><span class="string">    ------</span></span><br><span class="line"><span class="string">    shape : tuple</span></span><br><span class="line"><span class="string">        HxW defining the number of height and width dimension of the grid</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    points: HxWx2 tensor</span></span><br><span class="line"><span class="string">        Grid coordinates over [0,1] normalized image range.</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    H,W = shape[:<span class="number">2</span>]    <span class="comment"># shape is like (512,512), tuple type</span></span><br><span class="line">    c = np.empty((H, W, <span class="number">2</span>)) <span class="comment"># shape of c is (512,512,2)</span></span><br><span class="line">    c[..., <span class="number">0</span>] = np.linspace(<span class="number">0</span>, <span class="number">1</span>, W, dtype=np.float32)</span><br><span class="line">    <span class="comment"># from the viewpoint of second 512, fill the distribution of [0-1], and repeat 512 times for the fiest axis</span></span><br><span class="line">    c[..., <span class="number">1</span>] = np.expand_dims(np.linspace(<span class="number">0</span>, <span class="number">1</span>, H, dtype=np.float32), -<span class="number">1</span>) <span class="comment"># fullfill the [0-1] according to the first 512 axis, and for inner 512, just repeat </span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> c</span><br><span class="line"></span><br><span class="line"><span class="meta"> @staticmethod</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">z</span>(<span class="params">x, c, theta</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    x :ugrid.reshape((-1, 2)), shape is (xxx,2)</span></span><br><span class="line"><span class="string">    c: c_dst, shape is (n,2)</span></span><br><span class="line"><span class="string">    theta: theta[:, 1], shape is (n+3-1,)</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = np.atleast_2d(x)</span><br><span class="line">    U = TPS.u(TPS.d(x, c))</span><br><span class="line">    w, a = theta[:-<span class="number">3</span>], theta[-<span class="number">3</span>:] <span class="comment"># separate the w and a in the theta vector</span></span><br><span class="line">    reduced = theta.shape[<span class="number">0</span>] == c.shape[<span class="number">0</span>] + <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> reduced:</span><br><span class="line">        w = np.concatenate((-np.<span class="built_in">sum</span>(w, keepdims=<span class="literal">True</span>), w)) <span class="comment"># make w unabridged through the constrain of sum equalling zero</span></span><br><span class="line">    b = np.dot(U, w) <span class="comment">#shape of U: take 512*512 for example ,(262214,6)</span></span><br><span class="line">    <span class="keyword">return</span> a[<span class="number">0</span>] + a[<span class="number">1</span>]*x[:, <span class="number">0</span>] + a[<span class="number">2</span>]*x[:, <span class="number">1</span>] + b <span class="comment"># the shape of output result is (262144,)</span></span><br></pre></td></tr></table></figure><p>不妨设输出的图像大小设定为(512, 512), 在<code>ugrid = uniform_grid(dshape)</code>中, 输入的dshape是类似(512, 512)的tuple数组, 然后创建(512, 512, 2)的空矩阵, -1维可以理解为[x, y]位置上对应的归一化坐标. 代码的核心语句中<code>c[..., 0] = np.linspace(0, 1, W, dtype=np.float32)</code>会首先创建(512, )的[0-1]的均匀分布采样矩阵, 然后广播至c的第一个整体矩阵, 将一个横向的512向量纵向重复了512次, 生成的可视化结果如下.</p><figure>    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911160535.png" style="width: auto; height: 350px"></figure><p>对于<code>np.expand_dims(np.linspace(0, 1, H, dtype=np.float32), -1)</code>, 其中的在-1维度的expand_dims操作则将原来的(H, )的[0-1]采样向量转换为了(H, 1)的纵向向量, 在广播中则横向复制了512次, 可视化结果如下.</p><figure>    <img src = "https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911163017.png" style="width: auto; height: 350px"></figure><p>最终函数范围的是(512, 512, 2)的grid矩阵(实质作用是用于存储坐标). reduced作用同上, 默认为<code>Ture</code>. 对于函数z, 输入ugrid时进行reshape操作, 等同于将(512, 512, 2)排布的坐标矩阵转换为(512*512, 2)排布的类似一维形式的坐标对列表. 函数<code>z</code>的功能即为通过已经训练的theta函数将全图的坐标进行插值转换.<br><code>U = TPS.u(TPS.d(x, c))</code>计算所有坐标点和给定的源点之间的RBF函数距离, 然后分离<code>theta</code>中的w类和a类, 对于w类根据和为零的约束条件求解出被忽略的$w_0$, 接着根据位移表达式, 组合线性变换部分和非线性变换部分, 获得所有坐标在单一方向(x或y)上的<em>相对位移</em>. </p><p>直接通过z计算得到的结果还是类一维坐标向量的形式, 因此需要通过<code>.reshape(dshape[:2])</code>将之转换为(H, W)的形式, x和y在最后一维拼接获得(H, W, 2)的相对偏移矩阵<code>dgrid</code>, 与初始的<code>ugrid</code>相加即获得最终的目标坐标矩阵<code>grid</code>. 最后, 将归一化的坐标转换为<code>cv2.remap</code>函数可接受的采样图格式, 通过以下<code>tps_grid_to_remap</code>将归一化坐标转换为给定坐标尺寸下的绝对坐标即可, 输出的shape仍为两个(H, W)的矩阵.  </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">tps_grid_to_remap</span>(<span class="params">grid, sshape</span>):</span><br><span class="line">    <span class="string">&#x27;&#x27;&#x27;Convert a dense grid to OpenCV&#x27;s remap compatible maps.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Params</span></span><br><span class="line"><span class="string">    ------</span></span><br><span class="line"><span class="string">    grid : HxWx2 array</span></span><br><span class="line"><span class="string">        Normalized flow field coordinates as computed by compute_densegrid.</span></span><br><span class="line"><span class="string">    sshape : tuple</span></span><br><span class="line"><span class="string">        Height and width of source image in pixels.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns</span></span><br><span class="line"><span class="string">    -------</span></span><br><span class="line"><span class="string">    mapx : HxW array</span></span><br><span class="line"><span class="string">    mapy : HxW array</span></span><br><span class="line"><span class="string">    &#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    mx = (grid[:, :, <span class="number">0</span>] * sshape[<span class="number">1</span>]).astype(np.float32) <span class="comment"># convert normalized x-coordinates [0,1] to source pixel x-indices (width)</span></span><br><span class="line">    my = (grid[:, :, <span class="number">1</span>] * sshape[<span class="number">0</span>]).astype(np.float32)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mx, my</span><br></pre></td></tr></table></figure><p>最后通过<code>cv2.remap(img, mapx, mapy, cv2.INTER_CUBIC)</code>, 即实现了扰动图像的产生. 当选取的坐标对合适, anchor加入较多的情况下可以产生变化细微而逼真的形变图像, 结果如下图所示, 分别展示梵高画像和SAR车辆图像的形变对比图. </p><figure>    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911190710.png" style="width: auto; height: 400px; display: block; margin: 0 auto;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911190941.png" style="width: auto; height: 320px; display: block; margin: 10px auto 0 auto;">    <figcaption style="text-align: center;">TPS形变结果对比图</figcaption></figure><p>当给定的控制点只是简单的单一方向移动时(以网格图像的横向移动为例), TPS算法对图像的形变效果如下图所示. </p><figure>    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250911192313.png" style="width:auto; height:320px"></figure><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>TPS是相当经典而强大的图像warping算法, 后续会简要分析以此为核心的DeCoWa对抗算法和潜在的可能应用方向. </p><h1 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h1><ol><li><a href="https://blog.csdn.net/g11d111/article/details/128641313">Thin Plate Spline: TPS理论和代码详解</a></li><li><a href="https://zhuanlan.zhihu.com/p/227857813">薄板样条插值(Thin Plate Spline)</a></li><li><a href="https://www.jianshu.com/p/2cc189dfbcc5#fn3">Thin Plate Spline</a></li><li><a href="https://shape.polymtl.ca/lombaert/thinplates/">Manual Registration with Thin Plates</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;
TPS图像扭曲算法的代码完整复现与对所有代码的详细分析.
&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="代码笔记" scheme="http://boremycin.github.io/tags/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
    
    <category term="算法复现" scheme="http://boremycin.github.io/tags/%E7%AE%97%E6%B3%95%E5%A4%8D%E7%8E%B0/"/>
    
  </entry>
  
  <entry>
    <title>vsc账号首次远程连接失败的问题与解决</title>
    <link href="http://boremycin.github.io/2025/09/06/code3/"/>
    <id>http://boremycin.github.io/2025/09/06/code3/</id>
    <published>2025-09-06T06:26:30.674Z</published>
    <updated>2025-09-06T07:50:33.024Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">在服务器上的新建账号无法顺利使用vsc的ssh连接</p><br><span id="more"></span></p><h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>前不久在实验室的服务器上创建了一个新的多卡账号, 服务器则是已经通过ssh key连接过在<code>known_hosts</code>中保留指纹信息, 但应该不会影响通过用户名和密码的远程连接方式. 但我在登录过程中遇到了xshell可以正常登录, 但vscode始终显示登录失败的问题. 在vscode的终端输出中显示的内容也只是<em>试图写入的管道不存在</em>, 并同时夹杂着冗长的报错信息. </p><p>但是在<code>xshell</code>和<code>xfpt</code>上都是可以正常通过账号密码登录的(而且这两个软件也会默认创建密钥实现后续的服务器直连, 并且也可以将公钥导出到本地的<code>.ssh</code>路径中). </p><h1 id="尝试解决"><a href="#尝试解决" class="headerlink" title="尝试解决"></a>尝试解决</h1><p>最开始我认为这个问题产生的原因是在113服务器(账号所在服务器)上我已经用vsc和ssh key连接过, 但在新的用户目录下还没有复制对应的公钥. 通过<code>xshell</code>的终端可以进行相应的配置. 由于缺少root权限, 所有的命令行都需要加入<code>sudo</code>前缀. 在最后一步使用nano进入文件后, 直接使用<code>ctrl v</code>粘贴本地的公钥, <code>ctrl o</code>保存文件, 在底部出现确认提示后回车, 随后退出即可.  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在远程服务器上创建ssh文件夹</span></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">mkdir</span> -p ~/.ssh </span><br><span class="line"></span><br><span class="line"><span class="comment">#设置权限, 仅目录所有者可以访问读写</span></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chmod</span> 700 ~/.ssh </span><br><span class="line"></span><br><span class="line"><span class="comment">#进入路径, 创建用于保存公钥的文件</span></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">touch</span> ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="comment">#设置文件权限</span></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chmod</span> 600 ~/.ssh/authorized_keys</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用nano在终端写入</span></span><br><span class="line">nano ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure><p>但操作完成后连<code>xshell</code>也无法连接了, 但<code>xfpt</code>仍能正常连上. 随后我尝试了指定vsc的ssh-remote优先使用账号密码验证, 在配置文件中加入了<code>PreferredAuthentications password</code>. 然而后续测试的结果仍然是无法连接, 但报错的信息减少了, 最终的最初始的报错信息中找到了问题发生的原因.</p><h1 id="问题根源与解决方案"><a href="#问题根源与解决方案" class="headerlink" title="问题根源与解决方案"></a>问题根源与解决方案</h1><p>vsc在远程连接失败后会打印<strong>超长的</strong>报错日志, 但最关键的信息往往在靠前的位置. 在从头分析后发现关键的报错原因如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[12:10:51.465] &gt; <span class="built_in">mkdir</span>: cannot create directory <span class="string">&#x27;/remote-home/zym/.vscode-server&#x27;</span>: Permission denied</span><br><span class="line">&gt; <span class="built_in">chmod</span>: cannot access <span class="string">&#x27;/remote-home/zym/.vscode-server&#x27;</span>: No such file or directory</span><br><span class="line">&gt; Creating the server install <span class="built_in">dir</span> failed...</span><br></pre></td></tr></table></figure><p>显然, 这是因为使用vscode远程连接服务器时需要在服务器的用户目录下创建<code>.vscode</code>文件夹, 然后登录用户并没有root权限导致这个文件夹创建失败, 随后又打印了大量的额外报错信息并最终提示连接失败. 只需要将该目录的权限归属到用户即可, 相应命令如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看当前工作目录</span></span><br><span class="line"><span class="built_in">pwd</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#查看目录拥有者和群组</span></span><br><span class="line"><span class="built_in">ls</span> -ld /remote-home/zym</span><br><span class="line"></span><br><span class="line"><span class="comment">#更改目录拥有者为当前用户</span></span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chown</span> zym:zym /remote-home/zym</span><br><span class="line"></span><br><span class="line"><span class="comment">#更新读写权限</span></span><br><span class="line"><span class="built_in">chmod</span> 755 /remote-home/zym</span><br></pre></td></tr></table></figure><p>最终该问题完美解决, 而且得益于已经写入了公钥, 也已经完成了免密登录的配置.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;在服务器上的新建账号无法顺利使用vsc的ssh连接&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="笔记" scheme="http://boremycin.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="代码笔记" scheme="http://boremycin.github.io/tags/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
    
    <category term="工程开发" scheme="http://boremycin.github.io/tags/%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>docker应用笔记</title>
    <link href="http://boremycin.github.io/2025/09/04/code2/"/>
    <id>http://boremycin.github.io/2025/09/04/code2/</id>
    <published>2025-09-04T12:44:11.221Z</published>
    <updated>2025-09-04T13:13:02.221Z</updated>
    
    <content type="html"><![CDATA[<p><div style="text-align: center;"> 将服务器上的项目迁移到windows上并打包为docker镜像</div><br><span id="more"></span></p><h1 id="Docker应用笔记"><a href="#Docker应用笔记" class="headerlink" title="Docker应用笔记"></a>Docker应用笔记</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>暑期比赛的验收阶段要求将训练和推理环境打包为Docker,借此初步了解了Docker打包的全流程.需要注意的是云服务器或实验室服务器集群的实例本身就已经是某种形式的docker了,所以<strong>无法直接打包服务器中的代码项目</strong>.如果需要打包服务器上的代码,当前比较直接的折中办法是整理好项目的环境依赖,通过conda导出为environment.yml文件(或requirements.txt),将核心代码下载至本地windows环境,并在windows下进行docker的打包.后续的介绍内容均面向windows环境.</p><p>Docker是开源的操作系统虚拟化技术,通过对进程进行封装并与宿主和其他进程之间进行隔离来提高程序的分发和部署效率,被封装的对象被称为<code>容器</code>.相较于传统的虚拟机技术,Docker响应速度更快,对系统资源的调度更高效.同时提供除内核外的完整运行时环境也保证了运行环境之间的一致性.</p><p>在windows环境下,使用Docker主要通过Docker Desktop软件结合WSL,后者为windows提供linux的内核支持.</p><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先下载<a href="https://desktop.docker.com/win/main/amd64/Docker%20Desktop%20Installer.exe">Docker Desktop</a>和<a href="https://wslstorestorage.blob.core.windows.net/wslblob/wsl_update_x64.msi">WSL</a>.现在完成后先检查CUP虚拟化是否已开启.通过任务管理器性能中的CPU部分即可查看,一般会默认开启.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250903205031.png" alt="20250903205031"></p><p>然后还需要开启Hyper-v,Hyper-v是微软开发的虚拟化技术,允许在单一物理机上运行多个虚拟机,在Docker Desktop中被用来创建和管理轻量化虚拟机支持,是Docker在windows上运行的关键组件.在控制面版中打开程序,在<strong>启动或关闭windows功能</strong>部分可以用来管理Hyper-v.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250903205837.png" alt="20250903205837"></p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250903210018.png" alt="20250903210018"></p><p>如果没有对应选型,可以在创建Hyper-V.bat文件,直接写入以下脚本内容后运行.该脚本会遍历系统目录中所有包含Hyper-V名称的.mum文件,保存到临时的txt文件后用dism命令将找到的相关包加入到系统中,即实现Hyper-V的自动安装和启用.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">pushd</span> <span class="string">&quot;%~dp0&quot;</span></span><br><span class="line"><span class="built_in">dir</span> /b %SystemRoot%\servicing\Packages\*Hyper-V*.mum &gt;hyper-v.txt</span><br><span class="line"><span class="keyword">for</span> /f %%i <span class="keyword">in</span> (<span class="string">&#x27;findstr /i . hyper-v.txt 2^&gt;nul&#x27;</span>) <span class="keyword">do</span> dism /online /norestart /add-package:<span class="string">&quot;%SystemRoot%\servicing\Packages\%%i&quot;</span></span><br><span class="line">del hyper-v.txt</span><br><span class="line">Dism /online /enable-feature /featurename:Microsoft-Hyper-V-All /LimitAccess /ALL</span><br></pre></td></tr></table></figure><p>完成上述准备工作,安装好Docker Desktop还需要修改存储路径(默认为C盘),避免由于存储空间不足导致的镜像保存失败.至此,windows环境下的docker准备工作均已完成.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250904184335.png" alt="20250904184335"></p><h2 id="将项目代码打包为镜像"><a href="#将项目代码打包为镜像" class="headerlink" title="将项目代码打包为镜像"></a>将项目代码打包为镜像</h2><h3 id="Dockerfile与-dockerignore"><a href="#Dockerfile与-dockerignore" class="headerlink" title="Dockerfile与.dockerignore"></a>Dockerfile与.dockerignore</h3><p>由于服务器上的conda环境无法直接下载到windows当中,所以需要先导出项目的依赖环境,然后在创建镜像时在Dockerfile中加入环境创建的步骤.conda的环境导出命令如下.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda activate your_env_name</span><br><span class="line">conda <span class="built_in">env</span> <span class="built_in">export</span> --no-builds &gt; environment.yml</span><br></pre></td></tr></table></figure><br>该命令会生成一个<code>environment.yml</code>文件,记录当前环境依赖,同时不包含平台相关的build bash.接着需要构建一个<code>Dockerfile</code>文件,基于miniconda的基础镜像进行环境配置,以本次比赛项目为例,<code>Dockerfile</code>文件内容如下.</p><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 使用 miniconda 基础镜像（内置 conda，比自己安装方便）</span></span><br><span class="line"><span class="keyword">FROM</span> continuumio/miniconda3:latest</span><br><span class="line"><span class="comment"># 2. 设置国内源（换 apt 源）</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">echo</span> <span class="string">&quot;deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye main contrib non-free&quot;</span> &gt; /etc/apt/sources.list \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;deb https://mirrors.tuna.tsinghua.edu.cn/debian-security bullseye-security main contrib non-free&quot;</span> &gt;&gt; /etc/apt/sources.list \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;deb https://mirrors.tuna.tsinghua.edu.cn/debian/ bullseye-updates main contrib non-free&quot;</span> &gt;&gt; /etc/apt/sources.list \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; apt-get update \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; apt-get install -y --no-install-recommends \</span></span><br><span class="line"><span class="language-bash">       git \</span></span><br><span class="line"><span class="language-bash">       build-essential \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">rm</span> -rf /var/lib/apt/lists/*</span></span><br><span class="line"><span class="comment"># 3. 设置工作目录</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"><span class="comment"># 4. 复制 conda 环境文件</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> environment.yml .</span></span><br><span class="line"><span class="comment"># 5. 创建 conda 环境</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> conda <span class="built_in">env</span> create -f environment.yml</span></span><br><span class="line"><span class="comment"># 6. 激活环境 (添加到 PATH)</span></span><br><span class="line"><span class="comment">#   注意：这里要和 environment.yml 里的 name 对应，比如 your_env_name</span></span><br><span class="line"><span class="keyword">ENV</span> PATH /opt/conda/envs/your_env_name/bin:$PATH</span><br><span class="line"><span class="comment"># 7. 复制项目代码</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> . .</span></span><br><span class="line"><span class="comment"># 8. 默认运行脚本</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;python&quot;</span>, <span class="string">&quot;select2.py&quot;</span>]</span></span><br></pre></td></tr></table></figure><p>如果选择基于requirements.txt构建环境,则需要依赖python-slim的轻量化镜像,对应版本的Dockerfile内容如下.<br><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 使用官方 Python slim 镜像</span></span><br><span class="line"><span class="keyword">FROM</span> python:<span class="number">3.10</span>-slim</span><br><span class="line"><span class="comment"># 2. 设置国内源并安装系统依赖</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> <span class="built_in">echo</span> <span class="string">&quot;deb https://mirrors.tuna.tsinghua.edu.cn/debian/ trixie main contrib non-free&quot;</span> &gt; /etc/apt/sources.list \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;deb https://mirrors.tuna.tsinghua.edu.cn/debian-security trixie-security main contrib non-free&quot;</span> &gt;&gt; /etc/apt/sources.list \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">echo</span> <span class="string">&quot;deb https://mirrors.tuna.tsinghua.edu.cn/debian/ trixie-updates main contrib non-free&quot;</span> &gt;&gt; /etc/apt/sources.list \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; apt-get update \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; apt-get install -y --no-install-recommends \</span></span><br><span class="line"><span class="language-bash">       git \</span></span><br><span class="line"><span class="language-bash">       build-essential \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; <span class="built_in">rm</span> -rf /var/lib/apt/lists/*</span></span><br><span class="line"><span class="comment"># 3. 设置工作目录</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> /app</span></span><br><span class="line"><span class="comment"># 4. 复制依赖文件</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> requirements.txt .</span></span><br><span class="line"><span class="comment"># 5. 升级 pip 并安装 Python 依赖</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> pip install --upgrade pip \</span></span><br><span class="line"><span class="language-bash">    &amp;&amp; pip install --no-cache-dir -r requirements.txt</span></span><br><span class="line"><span class="comment"># 6. 复制项目代码（不包括大数据）</span></span><br><span class="line"><span class="keyword">COPY</span><span class="language-bash"> . .</span></span><br><span class="line"><span class="comment"># 7. 默认运行脚本</span></span><br><span class="line"><span class="keyword">CMD</span><span class="language-bash"> [<span class="string">&quot;python&quot;</span>, <span class="string">&quot;select2.py&quot;</span>]</span></span><br></pre></td></tr></table></figure><br>如果在windows上拉取的项目文件中仍然存在不希望打包到镜像中的冗余部分,可以类比git使用<code>.dockerignore</code>进行管理,写入希望掠过的文件或文件夹的相对路径即可.</p><h3 id="打包docker镜像"><a href="#打包docker镜像" class="headerlink" title="打包docker镜像"></a>打包docker镜像</h3><p>在windows的命令行中(Docker Desktop自带)定位到项目文件夹,运行<code>docker build</code>命令即可构建镜像.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker build -t img-name:latest .</span><br></pre></td></tr></table></figure><br><code>-t</code>用以给镜像命名,<code>latest</code>是镜像的标签,用来标记不同的版本.<code>.</code>会令docker在当前目录下查找<code>Dockerfile</code>和其他需要复制的文件.镜像创建成功后并不会有显式的存储路径,需要通过docker命令查看所有的本地镜像,命令如下.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker images</span><br></pre></td></tr></table></figure></p><h3 id="测试镜像"><a href="#测试镜像" class="headerlink" title="测试镜像"></a>测试镜像</h3><p>测试镜像使用<code>docker run</code>命令.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker run -it --name container-name img-name:latest</span><br></pre></td></tr></table></figure><br>其中的<code>-it</code>参数用于创建一个交互模型的运行容器并分配伪终端,<code>--name</code>参数用于指定运行容器的名称.在Docker中镜像(image)只是一个静态的只读模板,包含运行程序需要的所有内容;而容器(container)则是镜像的运行实例,支持<strong>启动</strong>,<strong>停止</strong>和<strong>删除</strong>等操作.在通过<code>run</code>运行docker后,可以通过<code>exit</code>退出,容器状态变为停止,但不会立刻结束.当需要彻底清除容器时,需要单独删除,可以通过命令行后Docker Desktop的可视化交互.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看存在的所有容器</span></span><br><span class="line">docker ps -a</span><br><span class="line"><span class="comment"># 删除指定容器</span></span><br><span class="line">docker <span class="built_in">rm</span> container-name</span><br><span class="line"><span class="comment"># 删除所有已停止的容器</span></span><br><span class="line">docker container prune</span><br></pre></td></tr></table></figure></p><h3 id="导出可分发文件"><a href="#导出可分发文件" class="headerlink" title="导出可分发文件"></a>导出可分发文件</h3><p>docker镜像需要压缩为<code>.tar</code>文件才能被后续分发,对应的导出命令为:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker save -o file-name.tar img-name:latest</span><br></pre></td></tr></table></figure><br>恢复镜像的命令为:<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker load -i file-name.tar</span><br></pre></td></tr></table></figure></p><h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看docker安装</span></span><br><span class="line">docker ps  或者  ps -ef | grep docker</span><br><span class="line"> </span><br><span class="line"><span class="comment">#查看docker服务：</span></span><br><span class="line">systemctl status docke</span><br><span class="line"> </span><br><span class="line"><span class="comment">#设置开启docker服务:</span></span><br><span class="line">systemctl <span class="built_in">enable</span> docker</span><br><span class="line"> </span><br><span class="line"><span class="comment">#启动docker服务：</span></span><br><span class="line">systemctl start docker</span><br><span class="line"> </span><br><span class="line"><span class="comment">#查看mysql镜像</span></span><br><span class="line">docker search mysql</span><br><span class="line"> </span><br><span class="line"><span class="comment">#进入容器：</span></span><br><span class="line">docker <span class="built_in">exec</span> -it xxx bash</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>第一次跑通docker的全流程,在无法直接打包云服务器项目的背景下很难说有多么的简单好用.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;div style=&quot;text-align: center;&quot;&gt; 
将服务器上的项目迁移到windows上并打包为docker镜像
&lt;/div&gt;&lt;br&gt;</summary>
    
    
    
    <category term="笔记" scheme="http://boremycin.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="代码笔记" scheme="http://boremycin.github.io/tags/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
    
    <category term="工程开发" scheme="http://boremycin.github.io/tags/%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>detectron2安装编译</title>
    <link href="http://boremycin.github.io/2025/08/19/code1/"/>
    <id>http://boremycin.github.io/2025/08/19/code1/</id>
    <published>2025-08-19T04:46:23.090Z</published>
    <updated>2025-09-16T03:49:26.056Z</updated>
    
    <content type="html"><![CDATA[<p><div style="text-align: center;"> detectron安装编译时缺少torch依赖</div><br><span id="more"></span></p><p>detectron2是facebook开源的目标检测依赖框架, 诸如sparcer-cnn的目标检测模型均依赖于其进行开发. 然而于诸如opencv-python等一般的python的cv依赖库不同, detectron2多出来了编译的步骤.官方的指导安装步骤是使用git clong源码后在setup.py所在路径下运行以下命令:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -e .</span><br></pre></td></tr></table></figure><p>需要额外注意的是detectron2对torch库的版本依赖较为敏感. 我的CUDA version是12.1,使用的是如下torch版本.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.5.1+cu121 torchvision==0.20.1+cu121 torchaudio==2.5.1+cu121 --extra-index-url https://download.pytorch.org/whl/cu121</span><br></pre></td></tr></table></figure><p>然而在后续的编译过程中反复遇到提示无法在虚拟环境中找到torch依赖的报错, 即使我确实已经正确安装了对应版本的torch库. 初步分析, 产生这个问题的原因在于使用<code>pip install -e .</code>时启动了PEP517隔离环境, 即创建了一个<strong>临时的</strong>,<strong>独立的</strong>虚拟环境, 而这个环境正好将外部的已安装torch的conda环境隔离在外, 根据路径下的<code>pyproject.toml</code>配置构建库函数所依赖的工具并加入到临时环境中. 但即使其中声明了对torch的依赖, 也可能因为境内的网络原因导致安装失败. 我所遇到的具体报错如下所示:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"> File <span class="string">&quot;/tmp/pip-build-env-h48lpsdh/overlay/lib/python3.9/site-packages/setuptools/build_meta.py&quot;</span>, line 317, <span class="keyword">in</span> run_setup</span><br><span class="line">      <span class="built_in">exec</span>(code, locals())</span><br><span class="line">      File <span class="string">&quot;&lt;string&gt;&quot;</span>, line 10, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">      ModuleNotFoundError: No module named <span class="string">&#x27;torch&#x27;</span></span><br><span class="line">      [end of output]</span><br><span class="line">note: This error originates from a subprocess, and is likely not a problem with pip.</span><br><span class="line">error: subprocess-exited-with-error</span><br></pre></td></tr></table></figure><p>对于这种情况, 有以下几种种常规的处理方法:  </p><ul><li>在pip命令后加入后缀<code>--no-build-isolation</code>, 来关闭上述的隔离功能, 使编译过程在conda环境中进行(conda本身已经对环境进行了隔离)</li><li>直接强制使用setup.py进行编译安装. <code>python setup.py build develop</code>, 然而这种操作已不被pip推荐以避免可能的环境混乱.</li><li>使用pip安装已经完成预编译的detectron2库, 命令为<code>pip install &#39;git+https://github.com/facebookresearch/detectron2.git&#39;</code>.</li></ul><p>然而, 前两种方法对上述报错并不奏效, 第三种方法在我所在的网络环境下几乎不可行 (即使我已经更换了诸如githubfast.com的镜像源或使用GitHub官方加速方法). 最后, 我发现了一种手动编译的方法成功解决的了detectron2的安装编译问题, 在定位到detectron2项目目录下, 构建好conda环境后执行以下两步即可.</p><ol><li><code>FORCE_CUDA=1 python setup.py build_ext --inplace</code></li><li><code>python setup.py develop</code></li></ol><p>第1步首先通过<code>FORCE_CUDA=1</code>强制激活CUDA编译流程, 确保CUDA相关的cpp和.cu被正确编译; 接着运行setup.py, 这是标准的python包构建命令, <code>build_ext</code>是其子命令, 作用是告诉<code>setuptools</code>当前在执行构建拓展的操作. <code>--inplace</code>则是<code>build_ext</code>的子选项, 用于声明拓展文件的放置位置, 默认情况下编译完成的拓展文件会被放置于临时的<code>build/</code>目录下, 而<code>inplace</code>则覆盖该默认行为并将编译好的文件放置于于源文件相同的目录中. </p><p>第2步则是用于构建<code>.egg-info</code>文件, 使pip工具能够正确的识别和管理当前的包, 其文件本身是旧格式的Python包元数据(metadata).</p><p>至此, detectron2的编译安装问题就暂时解决了.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;div style=&quot;text-align: center;&quot;&gt; 
detectron安装编译时缺少torch依赖
&lt;/div&gt;&lt;br&gt;</summary>
    
    
    
    <category term="笔记" scheme="http://boremycin.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="代码笔记" scheme="http://boremycin.github.io/tags/%E4%BB%A3%E7%A0%81%E7%AC%94%E8%AE%B0/"/>
    
    <category term="工程开发" scheme="http://boremycin.github.io/tags/%E5%B7%A5%E7%A8%8B%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
  <entry>
    <title>再站在新世界的门前</title>
    <link href="http://boremycin.github.io/2025/08/16/essay1/"/>
    <id>http://boremycin.github.io/2025/08/16/essay1/</id>
    <published>2025-08-16T14:41:12.776Z</published>
    <updated>2025-09-13T13:11:51.335Z</updated>
    
    <content type="html"><![CDATA[<p><p style="text-align: center;">研一正式开始前的一些胡思乱想</p><br><span id="more"></span></p><h1 id="站在新世界的门前"><a href="#站在新世界的门前" class="headerlink" title="站在新世界的门前"></a>站在新世界的门前</h1><p>这是由宋春丹发表在中国新闻周刊的新闻报道, 回顾了复旦77级数学系同学的青年时期的复旦生活和多年以后的人生轨迹. 现将全文摘录, 如下:</p><div class="note default" style="max-height:500px; overflow:auto;">  <pre style="white-space: pre-wrap;">1978年3月第一个星期六的下午，复旦数学系180多位新生坐在600号教学楼的阶梯大教室里，聆听校长、著名数学家苏步青的开学讲话.  这是10年来，首次有人通过高考走进大学校门。“你们要把自己小看10年。但是数学上要出成绩，必须在30岁之前。”苏校长说。数学一班的过硬一听不妙。他是1966届高中生，开学时已年满31周岁。全班年纪最大的他是班上三个“老三届”之一，另外两个是29岁的金山和27岁的李源潮。主持开学典礼的数学系副系主任胡立鹏的讲话更不客气。他说，我们的任务就是通过四年的学习，把你们带到数学的大门口。你们自己看看进得去吧？进得去就进去，进不去看着转行。又说：“你们这一届中，只要有一个人进去，我就心满意足了！其他人都是陪读的，当然陪读也有好处，能够经历数学思维的训练，以后改行随便做什么都可以做得很出色。” 数学二班的陈大康听后不以为然，心想：“进得去吧？”而今，他成了华东师范大学中文系教授。李源潮则在多年后有感而发：“你本来想走进这个房间，结果却走进了另一个房间。” 1978年2月28日，是复旦新生三天报到日的第一天。吴宗敏记得，这天是个阳光明媚的好天气。他本已定好第二天来报到，但按捺不住心痒，第一天就来寝室踩点。他被分到3号楼的223号寝室。寝室向阳，约15平方米，四张上下铺木床，中间是一张长条桌。他到时，已有同学将右侧靠窗的下铺收拾妥当。彼此做了自我介绍，他在左侧靠窗下铺贴上了写有自己名字的纸条，占了一个位置就离开了。第二天，上海市无线电八厂副厂长亲自带队，保卫处处长开着大卡车，工会主席带着十来个工人随行，一路敲锣打鼓把胸戴大红花的吴宗敏送到了学校。他1975年中学毕业后分配到该厂，这次高考，这家100多人的工厂中有包括他在内的4名工人被录取。卡车停在复旦大学3号男生宿舍楼前，工人们拿着他的行李——其实也只有一个小包，簇拥着他上了二楼，七手八脚帮着整理好行李，铺好床铺。22岁的张来武从安徽铜陵来到数学二班报到。他已经想不起来寝室的门牌号，只记得寝室地板的质量不错。因为他是寝室中唯一的外地人，每到周末，就只剩下他一个人，他总把寝室的地擦得干干净净。刚来时，他看到一位室友拿着一本全英文的小说在看，大为惊异，因为自己连英文字母还认不全。一楼的132号寝室里，计算数学班的沈文海坐了整整一天的火车，从北京风尘仆仆而至。他是全寝室最后一位入住者，大家自我介绍一番后，便陷入了沉默。寝室年纪最小的男生李刚打开了收音机，跟随广播里的领读大声念起英文：“Yes, it is. Yes, they are.” 陈晓亮也被分到计算数学班。他很奇怪，因为之前报考的明明是物理系。后来他听说，去杭州的招生老师是数学系主任，他把这个地区所有第一志愿报考复旦的上线考生全部招到了数学系。虽然未能如愿就读物理系，但陈晓亮还是觉得颇为幸运，因为数学系是复旦大学最好的系，考分最高。数学系77级被媒体称为状元班。数学一班的何宁卡是湖南省高考状元，李克难是湖南省第二名，计算数学班的李乐德是上海市高考状元，姚大卫是洛阳市高考状元。4个班中，数学一班平均年龄最小，被称为“小班”。17岁的李克难在小学时跳了两级，汉语拼音还没来得及学会，大学时要补习汉语拼音。15岁的朱一新跳了三级，从崇明农村考入复旦，被媒体称为“最年轻的大学生”。数学二班和计算数学班入校时就各有十多位“老三届”，入学时大都已婚。上学期间时不时传来谁家孩子出生的消息，名字多叫“欢”或“乐”。3月底，一批高考成绩出色但年纪偏大的学生经补招入校，成为走读生。数学一班新招了8人，最大的已36岁；计算数学班新增了7人，全都是30岁。陈晓亮听到大家私下议论，是不是有“走后门”的。他有朋友在上海交通大学，就没有补招的事。但后来发现，班上的走读生都很优秀。其实，这是根据国家的新政补招的。开学后，国家紧急下发了《关于普通高等学校试行招收走读生和增加招生名额的通知》，允许高等学校在完成招生计划的前提下试行招收走读生，并要求注意招收1966届、1967届高中毕业生。5月27日是复旦校庆，下午是各系参观日。新生们对这一天期待已久。  223寝室结伴参观，到计算机楼时，吴宗敏第一次见到了计算机，体积比一个寝室还大。数学系的展览则门庭冷落。几排桌子上，整齐陈列着刊有该系师生论文的国内外著名学术期刊。新生们尚不能理解其中含金量，只觉得呆板沉闷，转了一圈就匆匆离去。  开学第一周，考虑到从前工农兵学员入学时水平参差不齐，数学系决定先对新生做一次摸底考试。考场设在学校大礼堂，彼此间至少隔二三个空座，以免抄袭。考试内容为初等数学。分数当天就下来了，沈文海只考了63分。但他的成绩并不算最差的，最高分也不到90，成绩相对较好的多为“老三届”。数学系决定暂停教学计划，腾出四周时间专门补习初等数学。四周后再次测试，全系基本通过。大学课程正式开始。600号楼是数学系所在地，其中611教室是可以容纳200人的阶梯教室，数学系的公共课大都在这里上。77级是仓促考试和开学的，一开始教材供应跟不上，很多课本还是油印的，第二学期才恢复正常。开学后不久，老师告诫他们，以他们现在的水平，千万不能碰哥德巴赫猜想，“这东西害人的”。数学系常收到声称解决了哥德巴赫猜想的“民科”们的来信，就会把信发给学生，考他们能不能看出问题来。数学分析是公认最难的一门基础课，考验逻辑推导能力，很多同学都喜欢这门课，尤其喜欢听何成奇讲这门课。因为他不但讲出了数学之用，还讲出了数学之美，“让你觉得不仅是进入了一个科学殿堂，也是进入了一个艺术殿堂”。数学分析课辅导老师孙芳烈也是数学一班的第一任班主任，是班上同学公认的大学期间对大家帮助最大的一位老师。在同学的记忆中，她有一颗“母仪之心”，宽爱所有的学生，非常认真负责，一心扑在学生身上。现已83岁的孙芳烈头发花白，思维清晰。她极其低调，一再推辞采访，好不容易接受了，又一再强调，记不清多少事情了。大学班主任跟中小学班主任不同，学生们基本均已成年，有自觉性，能力强，都是自己管理自己。她告诉《中国新闻周刊》，77级是她带过的最优秀的一届学生，很有集体观念，不是只关心自己。她是复旦大学1953级数学系学生，带班时已年过40，工资刚从拿了二十几年的60块钱调到72块钱。她和在浙江大学工作的丈夫长期分居两地，两个孩子小的只有3岁，工作很忙，她只能把孩子全托。她设身处地想，过去自己上学时遇到问题经常找不到老师，就每天下班后都留在学校。为了保证答疑质量，她把《吉米多维奇习题集》从头到尾做了一遍。《吉米多维奇习题集》是数学分析的经典题集，同学们都争相去图书馆排队借阅。书上只有习题，没有答案，不少人解题上瘾。沈文海的下铺黄忠强将做过的习题装订成册，珍藏起来。封面上用正楷写道：“习题之真谛所在，欲知者智穷而后启之。” 在第一学年的数学分析考试中，数学一班有14人得了100分。苏步青连连称赞这个班“不得了”。数学系是苏校长的“宠儿”，77级更是他的骄傲。数学系学子们也均以“校长门生”而自得。苏校长作讲座，有学生迟到，分辩说自己只迟到了一两分钟，他说，根据数学归纳法，等于1时成立、等于2时成立、等于n＋1时成立，那么结论就成立。同理，迟到一分钟不算迟到、迟到两分钟不算迟到，那迟到多久也不算迟到了。经过“文革”时期的停滞，当时数学系只有苏步青和谷超豪两位教授，夏道行是副教授，其余授课老师都是讲师或助教。谷超豪不拿教授工资，拿副教授工资，他说，什么时候夏道行拿了教授工资了，他才拿教授工资。谷超豪有一种老派的绅士派头，待人彬彬有礼，讲课循循善诱。他曾给数学系上过一学期选修课“统一场论”。下课后他总会留下来，和学生聊天交流。陈晓亮记得，有一晚下课后，谷超豪和几个学生一起离开教室，在路口分手时，他停下来，等学生先走了，自己才走。  夏道行教实变函数，考试极难，考前也不给复习范围。考试时，一共只考一道半题目，一道是证明一个书上用了二十多页来证明的定理，半道是送分题。两个小时过了，一直到吃饭，也没人交卷。不过，他严是严，但并不会真的难为大家，还是会让大家及格的。数学系有一门计算机课。当时的计算机没有显示屏和键盘，运算结果只能打印出来。打印纸尺寸大，质地好，能当草稿纸，还能糊墙壁、包书皮、垫抽屉、做信纸。个别学生故意将打印结果排得很稀，原本一张纸就能打印完的用了五六张。大一时，最受追捧的课程并不是专业课，而是政治课“中共党史”。学子们都关心时事政治，老师讲党内“十次路线斗争”讲得声情并茂，阶梯教室座无虚席，下课时全体鼓掌。教了几十年书的老师从没见过这么勤奋的一届学生。校图书馆阅览室一座难求，常有人用书包占位子。很多学生熬夜甚至通宵学习，恨不得一天能当两天用，晚上能当白天用。由于学习高度紧张，开学三个月沈文海就开始失眠，有时通宵不能入睡，满脑子定理习题。直到大学最后一学期，他开始冬泳，失眠才痊愈。入学后的第一个夏天，上海酷热难耐，没有电风扇的寝室里像蒸笼一样。沈文海每隔一阵就要爬下床，跑到水房里，用凉水浑身上下一遍一遍地浇。那个期末，学校决定提前放假，开学后补考期末考试。考虑到学生的身体，校方决定执行“熄灯令”：图书馆晚上9∶30熄灯，教学楼9∶40清场、10点熄灯，宿舍楼10∶40熄灯。  “熄灯令”招致一片反对之声。学生代表找苏步青反映：同学们无法完成课业，希望延长供电时间。苏步青说，来日方长，既要学习，也要健康。不过，熄灯时间被延迟为教室10∶30，寝室11点。于是，熄灯后，宿舍楼就会响起一片“轰隆轰隆”声，那是从寝室往走廊拖板凳的声音。不久，为了争夺走廊灯正下方光线最好的位置，一些学生不等熄灯就用板凳占位。楼前的路灯下也总是站满人，都是数学系的学生，大部分人都在读外语。李源潮后来说，他工作后外语一直不错，很多人以为他出国留学过，其实完全是“路灯底下的外语”。不过，也有学习轻松的。计算数学班的李乐德、姚大卫、顾大维三位“老三届”都已成家，每天下课后都要回家做家务，但成绩却很是出色。陈晓亮学习十分紧张，很好奇这几位老大哥是什么时候做的作业，后来听说是在骑自行车回家的路上就在脑子里做完了。刚开学时大部分人的英语都是从26个字母学起，姚大卫已经能用英文交流。1978年，国家恢复研究生考试。上海交通大学刚设立的研究生院急需人才，李乐德、姚大卫、顾大维被要求报考上交大的研究生，并被录取，只读了3个学期就离开了复旦，一学期后又都赴美留学。每天早上5点，戴晓波准时起床，拎上几个空的暖水瓶，奔赴运动队参加训练。他曾获复旦大学中长跑比赛第三名，从大一起就加入了运动队，一直到七年后从复旦研究生毕业。7点多，他训练完毕，在食堂吃过早餐，给晚起的室友捎上馒头，拎上打好的开水，跑回寝室。他的下铺吴宗敏常常会在第一遍铃打过之后、离上课只有5分钟时才匆匆爬下床，抓起馒头，一边咬一边往教室跑。王向东入学时肝病未愈，面色蜡黄，每天早睡。室友就寝时间不一，免不了互相干扰，但从未有人表示过不满。223寝室的7名男生中，27岁的李源潮是寝室的大哥。因为年龄比其他人大了好几岁，阅历丰富，性格稳重，室友们多敬他几分，不像其他人之间时有玩笑和打闹。当时规定，工作5年以上可以带薪上大学，每次一起在外吃饭，有收入的他总是抢着买单。他爱打桥牌，曾和吴宗敏组队参加学校的桥牌比赛。21岁的戴晓波性格活跃，喜欢创新，由于身处运动队，他的课余生活比起“两耳不闻窗外事”的数学系同窗们算得上丰富多彩，过得不亦乐乎。运动队会发运动服，他的衣服也总是全班最漂亮的。19岁的何宁卡性格成熟，在中学时是学校的团总支书记，善于处理人际关系，是223的寝室长。他对时事很敏感，聊天时，谈起时事新闻和历史，他都如数家珍。21岁的吴宗敏和18岁的王向东是好友，第一学期时一直出双入对，总去图书馆里钻书堆。吴宗敏考试经常第一个交卷，比其他同学能提前半小时；王向东记忆力超群，过目不忘，笔记和作业极为整洁，考试常拿100分，99分都很少见。18岁的陈跃斌喜欢围棋，没事时就打棋谱。16岁的毛怀遂年纪最小，性格却很沉稳，他擅长长跑，曾是西安市长跑比赛第二名。吴宗敏觉得，这也是他们寝室的总体特点：不是冲刺型，而是耐力型。室友间的关系也是，看似平淡，不是称兄道弟，但从没红过一次脸。戴晓波说，当时觉得很简单，回头看才知道很不容易，是一笔最大的财富。宿舍楼二楼中间学生会活动室里，有一台20寸黑白电视机，每天的固定节目是《新闻联播》，偶尔会放《敌营十八年》《大西洋底来的人》《加里森敢死队》《无名英雄》等电视连续剧。每逢重大体育赛事，就到了男生们的节日了。1981年，中国女排参加世界锦标赛，首次夺冠，举国欢腾。复旦的学生们兴奋得不知所措，四五百人叫嚷着涌出校门，直到后半夜才筋疲力竭回校睡觉。也是这一年，在西班牙世界杯足球预选赛上，中国对阵科威特，点球被中国队门将扑出，男生宿舍瞬间沸腾，有人点着褥子挂在窗外，有人举起装满开水的暖壶从窗口摔下。中国队本来有望冲出亚洲，却因沙特故意放水给新西兰而最终失之交臂，男生们揭竿而起，有人甚至点着扫把冲到楼下。那几年，这种“游行”屡见不鲜。223室的男生们虽然也爱看球，但很少参与。吴宗敏说，寝室大多数人的思想好像比较偏中道。每周末，李源潮从家返校，总会光顾书店，买回各类书籍，放在寝室的小书架上。他还从家里带来一台模样特别的小机器，大家才第一次知道，这就是录音机。寝室里开始有交响乐和流行歌曲的旋律飘出，但最主要的用途是学外语。大二时，寝室里有人提议，靠窗和靠门的铺位应该轮换住，于是李源潮和吴宗敏就换到了进门右手边那张床，住上下铺。311寝室里通常都在安静地自习。周末赶上下雨天凉，过硬的妻子会带上衣服来学校接他回家。但她从不进寝室，就在门口等着，如果有人出来就请其帮忙给过硬带个话。寝室里静得“一根针掉在地上都能听见”，她怕敲门会打断大家解题的思路。3号楼的对面是文科生的男生宿舍楼，陈晓亮常常听到楼里传来小提琴声和播放的音乐声，跟3号楼的沉闷静谧对比极为鲜明。大二结束后，最后两届工农兵学员陆续毕业离校，数学系77级男生宿舍升至三楼。3号楼寝室门牌号奇数朝阳，偶数背阴，上海天气潮湿，阴面阳面差别很大。为公平起见，数学系规定，以一年为期，寝室南北互换。于是，吴宗敏等从223寝室搬到了328（后来又搬到了308），沈文海等则从132搬到了333室，和328室是斜对门。跟数学一班相比，计算数学班“老三届”较多，气氛也相对活跃。沈文海作为寝室最早起床的人，开学后不久就被推选为室长，除了每天早上给室友带馒头，还要收发作业本，代领粮票、购物券，打扫卫生。终日泡在图书馆或教学楼里苦读的他很羡慕寝室的几个老大哥——姜叙伦、於崇华、曹沅。他们学习轻松，爱好棋牌，晚上在寝室听着收音机聊着天就把作业做完了，然后就冲着其他寝室吆喝，不一会儿就组成了棋牌局。张来武和安徽同乡的陈晓漫在数学二班成绩出色，被称为“安徽两霸”。张来武记得，大约半年后，开学时摸底测试成绩靠前的“老三届”学习上优势不再明显。他觉得，这可能是因为被耽搁多年，过了学数学的黄金年龄。600号楼611教室不光用来上大课，很多讲座也在这里举行，几乎每周都有一到两个晚上，内容五花八门。在《中国青年》就潘晓的一封来信“人生的路为何越走越窄”展开大讨论前后，数学系学生会请了一位学者来座谈人生的意义。与当时革命化的人生观主旋律不同，这位学者却强调小确幸：“生活的意义就在于每天的上班下班、每天的柴米油盐酱醋茶、每天早晨的送孩子上学、每天晚上的与家人一道共进晚餐、每天夜里临睡前的与家人互道晚安……以及每天的家长里短、每天的解决邻里纠纷、每天的喜怒哀乐等等。” 最受欢迎的是交谊舞讲座，每次都人满为患。中年女老师用一台卡式录放机放起音乐，讲解之余，经常挑两个男生上台，手把手示范，男生们既兴奋又害羞，台下笑声不止。  每周六晚上，学校食堂都挪开桌椅，举行舞会。系里也组织舞会，地点多在600号楼的礼堂。虞锦国擅长交谊舞，有时在寝室里教同学用凳子练习。交际舞讲座沈文海很少参加，但诗歌类讲座几乎场场必到。沈文海在北京44中读书时语文成绩出色，作文经常被当成范文。一次开会，他和李源潮坐在会场最后面，两个喜欢文学的青年开始在笔记本上手谈。李源潮一言不发地写下一串书名，每写一个，如《毁灭》《南行记》《死水微澜》《生死场》等，就询问地看看他，他时而点点头，表示读过或听过，但大多数时候都是一脸惘然。李源潮整整写满了一页，这些书里，他看过的不到十分之一，至少有三分之一没听说过。1981年，复旦大学组建诗社，杨荫稚加入了这个几乎都是文科生的社团。一首《我走近这陌生的一群——给诗社，也给你们》保留至今： 我走近这陌生的一群 因为，我期望 漂浮的心 能靠上一片稳妥的云 我小心地走近这陌生的一群 因为，我害怕 梦幻的薄冰过早地消融 我曾一千次地追求 可许多回 抓住的 只是一片浮云 所以，我来了 走近这陌生的一群 做第一千零一次的追寻 为一颗不安宁的灵魂 寻一片拍岸的惊涛 校学生会组织赛诗会，数学系常由沈文海、杨荫稚、张来武等人组队上阵。张来武不知道是因为自己的诗真的太受欢迎呢，还是因为自己普通话说得不够好，反正他在台上朗诵时，台下总是掌声笑声不断。不过，他很快就认定自己成不了好的诗人，他觉得自己的思维太严谨，缺少诗情画意。他还是数学系的体育健将，大三时在运动会上拿下五项全能第二名，被授以“宝刀不老”的称号，让年纪尚轻的他感觉莫名其妙。数学一班的马进是数学系团总支文艺委员，在系里是少有的多才多艺者。与大部分男生千篇一律的白衬衫、中山裤不同，他穿着时髦，戴金色边框眼镜。他画画功底扎实，擅长拉手风琴，会跳迪斯科，班上男生的迪斯科都是他教的。文艺汇演时，他经常拉手风琴伴奏，集合一批爱唱歌的男生搞小合唱。百科知识竞赛是数学系大出风头的领域。在校学生会举办的竞赛中，计算数学班的宋伟铭夺冠，前十名里数学系占了三个。当时，各种社会思潮风起云涌。伤痕文学、民主竞选、四五英雄、N大美女，各领风骚一阵。路边的板报栏经常张贴着小字报，展开辩论。一天，大家突然发现一篇书法流畅、见地老到的小字报，署名竟是“夏征农”。当时，经常能看到校党委书记夏征农一个人拿副碗筷在食堂打饭，然后坐下来和学生边吃边聊。用同学的话说，当时整个大学里面弥漫着一种处变不惊的氛围，什么事情都可以做，做了什么事情也没什么了不起。这是一个思想解放的场所，包容活跃，自由进取，这在社会上的其他地方是看不到的。毕业时，数学系很多同学决定考研。1981年暑假，沈文海没有回北京，留在学校复习，准备参加大连工学院的研究生考试。李源潮也没有回家，在校复习。他报考的是复旦大学管理科学系数量经济专业研究生。系里希望他不要考研究生，毕业后留在数学系，做分管学生工作的老师，但他没有接受。他一向对经济很感兴趣，十一届三中全会之前就和世经系的王战一起成立了社会经济体制改革研究小组，大四时常去管理系旁听。假期的校园很冷清，两人常常结伴复习。图书馆阅览室和教室大多不开放，寝室里则酷热难耐，他们常在下午去宿舍楼二楼中间的活动室复习。活动室没风时异常闷热，两人一人一把蒲扇，一手摇扇一手翻书，挥汗如雨。每隔半小时，就得去用脸盆打清水回来往地上洒。下午4点后，校园东北角的游泳池开始下午场，两人就结伴去泳池消暑。经过了艰苦复习的这个苦夏，后来让他们觉得什么都无所畏惧了似的。最终，数学系（力学专业除外）共18人读研。戴晓波考取了复旦大学管理科学系数量经济专业研究生，这一专业的5个研究生都来自数学系。吴宗敏报考了本系研究生，看到报考表格中有一项“你是否愿意成为出国预备生”，就顺手打了勾。结果，毕业分配前一个月，他得到通知，学校将选派他到国际著名的联邦德国哥廷根大学数学系留学。作为教育部和国外签订的交流项目，1982年，复旦数学系共有6名学生公派出国留学。吴宗敏和室友王向东去德国，杨平、邓波和雍烱敏去美国，毛裕新去法国。吴宗敏到上海图书馆查阅自己导师Schaback的研究方向，结果发现是自己原来不怎么喜欢的计算数学。谷超豪访问德国时，吴宗敏曾恳请他通过关系帮助换一下专业，但谷超豪认为签好协议再换导师有失尊重，吴宗敏只能接受这一现实。1982年1月底的一个下午，数学系77级全年级180余人聚集在600号楼611阶梯教室，听宣读毕业分配结果。由于此前保密工作做得很严，大部分人事先都不知道结果。教室里十分安静，老师按班级一个一个地念，大家的心都提到了嗓子眼，紧张到手心出汗。“过硬，上海市杨浦高级中学；李源潮，复旦大学管理科学系；杨荫稚，上海旅游专科学院；陈跃斌，上海社科院；陈大康，上海纺织专科学校；周舜培，少儿出版社；李刚，石油部石油勘探开发设计院；沈文海，中央气象局气象科学研究院；陈晓亮，上海电力成套设备研究所…… ” 约两小时后，名单才念完。一散会，大家就纷纷跑向距学校最近的五角场邮局，发电报向家人报信，一时间邮局已经人满为患。毕业前夕，举行了两个婚礼和一个葬礼。举行婚礼的是数学一班的两个老三届，婚礼在俱乐部举行，邀请全班同学参加，每人发一点喜糖。结婚时床不够用，就问过硬家借了一张小钢丝床，这张床至今被过硬的夫人保存在老房子里，她说“一辈子都不会丢掉”。毕业前，力学班班长、25岁左右的慈力远因癌症去世。数学二班班长奚树林、计算数学班班长姜叙伦与系学生会一起，出面筹办了慈力远的追悼会。全年级同学几乎悉数到场。慈力远的老父抑制不住悲痛，突然放声大哭。很多同学随之泪如雨下。年轻生命骤然凋谢带来的震动、告别学生时代的不舍和即将走向社会的忐忑，让沈文海的心情难以言表。一天晚饭后，他路过一栋教学楼，里面正在举行毕业生联欢会，《毕业歌》《年轻的朋友来相会》的歌声忽高忽低地飘出，他无法自持，泪流满面。  1985年，国家提出了“支持留学、鼓励回国、来去自由”的方针，一场史无前例的出国热潮席卷全国，第二年自费留学的人数就突破了10万人。在此前后，数学一班26人相继出国。从此，同学们风流云散。倏忽30年。2010年底，在河南洛阳国企担任老总的数学一班同学赵滨海发出倡议，在毕业30年之际举行同学聚会。戴晓波找到老班长过硬，联系了在上海的几位同学，筹备工作很快提上日程。同学们早已星散全球。杨荫稚利用在上海师范大学旅游学院的优势，请学生通过各种线索寻人。经过一年多的努力，全班46人，联系上了43个，34人参加。杨荫稚还主持收集了大量上学时期的老照片，扫描成电子版，装订成纪念册。这些旧照多是大家当年DIY的作品。2011年12月23日到25日，数学一班同学历史性齐聚上海。一些同学已经退休，年纪最小的也已年过半百，入校时只有17岁的蔡进一已是满头银丝。前两天的活动主要在校外举行，远在美国的赵建民和毛昭林通过大屏幕与大家远程视频，大家还向四十多岁英年早逝的王向东等同学献上哀思。第三天是重返母校的重头戏，李源潮参加。经过协商，时任复旦大学常务副校长陈晓漫、时任科技部副部长张来武等7位数学二班同学，作为外班同学代表，受邀参加了这次聚会。12月25日早晨8点多，在600号楼会议室召开的座谈会上，本来事先安排好了发言的同学和次序，结果说着说着就成了自由发言。11点，座谈会结束，大家到600号楼门前拍合影。如何站位的问题得到了巧妙的解决。毕业前夕，数学一班和老师们就在这座楼前照了一张合影，这次也按原来的站位拍摄。毕业照中的全班46位同学，这次到了34人。照片上原有6位老师——第一任班主任孙芳烈、第二任班主任王芬、教控制理论的李训经、教数学分析的何成奇、教代数拓扑的李元熹、教数学分析的陈天平，这次，除了已经去世的李训经，5位老师悉数到场。李训经的位置则由指导员杨浣明代替。中午，在学校大食堂聚餐，席开四桌。吃饭时，吴宗敏突然宣布了当天的特别节目——这一天是过硬的65周岁生日，黄超成、虞锦国、周伟良的生日也分别在前后一天。4位寿星一起吹灭了生日大蛋糕的蜡烛。过硬在蛋糕上切下第一刀，一时间百感交集，心情不可名状。最后一个节目是参观复旦，木制的老校门早已不再通行，600号楼内的611、602两个教室已不复存在。3号楼男生宿舍还是灰色墙壁的三层小楼，却已变成女生宿舍，门口的牌子上写着“男士止步”。计算数学班的大聚，在2006年毕业25周年之际就已举行。当年在寝室里念着“Yes, it is. Yes, they are.”的15岁小男孩李刚已变成富态的水产公司老板，40多岁就提前退休，姜叙伦一见面差点没认出他来。而姜叙伦自己，这个当年被公认为极有数学天赋和管理才能的老班长，在90年代初放弃十年基业，携全家赴美重新开始，如今在一家小公司做着一名普通的程序员。沈文海现在已是国家气象信息中心总工程师。333寝室7人中，独缺於崇华。2000年12月7日下午，已是复旦大学数学系常务副主任兼博士生导师的於崇华骑自行车去学校讲课，在横穿马路进校门时被一辆小客车撞倒而离世。王元生带来了寝室最后一次聚会时於崇华的录音。沈文海想听又不敢听，犹豫了很久，终究还是放弃了。那是毕业前夕的一天下午，寝室自发搞了一次茶话会。大家围坐谈天，憧憬未来。最后，於崇华冲着一部砖头式录放机喊道：“我们的口号是：十年后计算数学年会上见！” 如今，参加学术年会早已是再寻常不过的事情。甚至于，网上流传着“史上最牛班”的传说。严格地说，这份名单是复旦数学系77级、78级两个年级精英的合集。其中，哈佛大学研究生院院长孟晓犁、牛津大学数学教授陈贵强、普林斯顿大学冠名金融讲座教授范剑青、斯坦福大学数学系教授李骏 、斯坦福大学生物统计系教授陆盈、哥伦比亚大学统计系教授应志良、加州大学戴维斯分校计算机系教授柏兆俊、德意志银行原执行董事徐幼于等均出自78级。77级同学中，计算数学班的姚大卫在加拿大多伦多大学博士毕业后担任哥伦比亚大学工业工程和运筹系教授，曾在哈佛大学和耶鲁大学执教，并在香港中文大学以及清华大学兼任讲座教授。2015年，他当选美国工程院院士，这是美国工程技术领域的最高专业荣誉。数学一班的蔡进一在康奈尔大学获得计算机科学博士学位后，担任威斯康星大学麦迪逊分校计算机系教授，在理论计算机科学领域成就卓著；雍炯敏获美国普渡大学博士学位后回到复旦，成为中国最年轻的数学家之一，后来担任了复旦大学数学系主任，现在在美国佛罗里达大学任教，是国家杰出青年科学基金获得者和“长江学者”特聘教授；马进 1985年从复旦大学数学系硕士毕业，留校两年后赴美就读明尼苏达大学，1992年博士毕业后开始在普渡大学任教，自2007年起在南加州大学任教，在金融数学领域成就斐然。但更多的人选择了转行。尤其平均年龄偏大的数学二班和计算数学班，很少有人继续在专业内做学术。陈大康认为班上的陈晓漫和张来武是为数不多适合从事数学研究的人，但是陈晓漫和张来武后来都担任了行政职务。张来武从复旦数学系毕业后考取北京大学数学系研究生，后在美国纽约州立大学奥本尼分校经济系经济学专业获博士学位。1999年，他被派至宁夏回族自治区任政府主席助理，本来以为只是短暂挂职，没想到不久后西部大开发开始了，他在宁夏一干就是10年。2008年，张来武离任宁夏回族自治区政府副主席，担任了科技部副部长。陈大康自己，更成为数学系的传奇。他在上海纺织专科学校当数学老师期间，致力于用数理统计方法研究《红楼梦》的语言风格。在没有计算机的情况下，他先用一年多时间手工统计了该书中各种字、词出现频率等数据，又经过三个月的计算，推算出《红楼梦》后40回非曹雪芹所作。此后他考入华东师范大学中文系攻读博士学位，后留校从事中国古典文学的教学与研究，曾担任该校中文系主任，现在是终身教授。将数学思想、方法运用于人文科学研究，成了他的独特优势，为学界所瞩目。他的研究风格与一般文科学者不同。他常说，搞研究第一是发现问题，第二是解决问题。写论文就像证明几何题，已知和证明写清即可结束，无需任何矫饰。过硬记得，刚进大学时苏步青对他们说，你们当中应该有不少人到中学当老师，以提高中学的教学质量。没想到，日后这真的成了他的毕生事业。他认为自己不可能出数学成绩了，就专注教学，久而久之，成了上海的名牌数学教师，退休时已是中学最高级别的五级教授职衔。308寝室7人，王向东在德国研究生毕业后担任了一家软件公司的总裁，已去世。毛怀遂在老家西安创业，成了化工公司的老板。陈跃斌在上海市财政局研究所工作。李源潮和何宁卡走了从政的路。戴晓波则横跨学、官、商三道。研究生毕业后，他被分配到上海市计划经济研究所担任副所长；1991年，作为上海住房改革方案组组长，在时任市长朱镕基的领导下，主笔起草了住房制度改革和公积金方案；随后又担任上海市计委长远规划处处长，在时任市长徐匡迪领导下，编制了上海2020年城市规划和国民经济“九五”计划。他还做过风险投资公司老总。现在，他是上海社科院应用经济研究所城市与房地产研究中心主任、研究员，已完成上海2050年发展方向研究。他不爱讲过去，喜欢谈未来。只有吴宗敏一人仍然在数学界坚守。他卸任复旦大学数学系主任一职后，现在是数学学院教授、博士生导师、长江学者特聘教授。当初，他遗憾自己对于未来的规划不够清晰。现在，他会告诉学生，不必太刻意，环境日新月异，每个人都需要终身成长，重要的是，学到理念和方法。因为，只有具备独立之精神、理性之思维，才是真正的登堂入室。  </pre></div><p>大二时第一次在树洞读到这篇文章,后面又三番五次地读完,每次都心情久久不能平复,长叹世事无常,命途难断.我们往往难以决定自己的生活的走向,人生的走向,哪怕当初坚定不移地朝着坐标与目标之间的最短路径前进,走到重点时也可能发现与当初的规划大相径庭,因为在历史与时代的坐标下,我们所看到的所有目标都是时变的;同时,每个人生阶段的走向,又被所有经历过的人生阶段所界定,</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;p style=&quot;text-align: center;&quot;&gt;研一正式开始前的一些胡思乱想&lt;/p&gt;&lt;br&gt;</summary>
    
    
    
    <category term="随笔" scheme="http://boremycin.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
    <category term="随笔" scheme="http://boremycin.github.io/tags/%E9%9A%8F%E7%AC%94/"/>
    
    <category term="杂谈" scheme="http://boremycin.github.io/tags/%E6%9D%82%E8%B0%88/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：SAR Conditional Random Field Attack</title>
    <link href="http://boremycin.github.io/2025/08/11/paper-CRF/"/>
    <id>http://boremycin.github.io/2025/08/11/paper-CRF/</id>
    <published>2025-08-11T13:09:12.000Z</published>
    <updated>2025-08-11T13:33:32.855Z</updated>
    
    <content type="html"><![CDATA[<p><div style="text-align: center;"> 关于论文CRFA的笔记  </div><br><span id="more"></span></p><h1 id="前置知识"><a href="#前置知识" class="headerlink" title="前置知识"></a>前置知识</h1><h2 id="logistic回归模型"><a href="#logistic回归模型" class="headerlink" title="logistic回归模型"></a>logistic回归模型</h2><p>二项(二分类)Logistic回归模型可以理解为参数化的Logistic分布.对于Logistic分布,设X为连续的随机变量,所服从的概率分布函数和概率密度函数如下.  </p><script type="math/tex; mode=display">F(x) = P(X \leq x) = \frac{1}{1+e^{-(x-\mu)/\gamma}} \tag{1}</script><script type="math/tex; mode=display">f(x) = F(x) = \frac{e^{-(x-\mu)/\gamma}}{\gamma(1+e^{-(x-\mu)/\gamma})^2} \tag{2}</script><p>上式中$\mu$为位置参数,$\gamma &gt; 0$为形状参数.分布函数的图像是一条Sigmoid曲线,满足中心对称,在中心附近的增长速度快,在两端的增长速度慢.同时在DNN模型中Logistic函数也会和Sigmoid激活函数之间形成天然的联系.而当作为二分类模型时,logistic分类模型会对输入变量X进行0或1的标签Y划分,通过有监督学习的方法估计模型的参数.对应的表达式如下.</p><script type="math/tex; mode=display">P(Y=1|x)=\frac{exp(w \cdot x + b)}{1 + exp(w \cdot x + b)} \tag{3}</script><script type="math/tex; mode=display">P(Y=0|x) = \frac{1}{1 + exp(w \cdot x + b)} \tag{4}</script><figure>  <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/d0f755e55f9da868d9da7d21e06fd007.png" alt="图像描述">  <figcaption style="text-align: center;">Logistic分布的概率密度函数和分布函数图像</figcaption></figure><p>在DNN分类模型中,不妨将二分类场景下模型最后一层输出设定为logit值,满足 $logit(z)=\mathbf{w} \cdot \mathbf{x}$.不难发现logit取值涵盖整个实数域,而我们希望找到一种转换函数,使模型的因变量变为分布于[0-1]的概率,同时logit的取值越大,标签Y为1的概率越大;logit的取值越小, 标签Y为0的概率越大(即为1的概率越小,同时这也表明了目标映射函数必须使关于0中心对称的).</p><p>这里需要引入几率(Odds)的概念,并与概率(probability)的概念相对照.如果简单地将概率理解为采样空间中目标事件的占比(需满足非负性,规范性和可列可加性),那么几率则是目标事件与非目标事件的比值.对应几率比的表达式如下.几率是概率的函数,对事件的发生更加敏感,而对于0/1分类模型,事件发生和不发生应当时对称的,即将标签0打为标签1时,模型参数应当是一直而无趋向性的.</p><script type="math/tex; mode=display">oddsRatio(p) = \frac{p}{1-p} \tag{5}</script><figure>    <img src=https://raw.githubusercontent.com/boremycin/ImageBed/master/3b21a1ffdd65657d152103fb7cdfdbf7.png>    <figcaption style="text-align: center;">oddsRatio和取对数的log oddsRatio</figcaption></figure><p>观察函数分布可以发现,将几率取对数后就是理想的中心对称图像,函数的自变量为[0-1]的分布,因变量覆盖整个实数域.因此,将其转置即可实现从logit到概率的映射,即有如下推导过程.</p><script type="math/tex; mode=display">logit(z) = \mathbf{w} \cdot \mathbf{x} = log(\frac{p}{1-p}) \tag{6}</script><script type="math/tex; mode=display">p = \frac{exp(logit(z))}{1 + exp(logit(z))} \tag{7}</script><p>从式(7)不难发现,模型概率p即满足一个logistic分布,同时这也是一个sigmoid函数.因此,logistic回归模型可以视为一个Linear线性层和一个sigmoid激活函数叠加的简单DNN模型.对于多分类任务,则可以将sigmoid推广至softmax,即sigmoid是softmax的特例.</p><p>不过在论文正文中,只简单用到了Logistic回归模型对Feature Vector进行前景和背景的二分类操作.</p><h2 id="条件随机场-Conditonal-Random-Field"><a href="#条件随机场-Conditonal-Random-Field" class="headerlink" title="条件随机场(Conditonal Random Field)"></a>条件随机场(Conditonal Random Field)</h2><p>条件随机场(CRF)可以简单理解为用于结构化预测的概率图模型,模型输入序列X,输出标签序列Y.表达式如下,其中的$f_k$为特征函数,$\lambda_k$为相应权重,$Z(X)$为归一化因子.</p><script type="math/tex; mode=display">P(Y|X) = \frac{1}{Z(X)}exp(\sum_i \sum_k \lambda_k f_k (y_{i-1},y_i,X,i) \tag{8}</script><script type="math/tex; mode=display">Z(X) = \sum_{Y'}exp(\sum_i \sum_k \lambda_k f_k(y'_{i-1},y'_i,X,i))  \tag{9}</script><p>对于训练完成的CRF模型,给定输入X后可以找出最可能的标签序列Y.</p><h1 id="创新点总结"><a href="#创新点总结" class="headerlink" title="创新点总结"></a>创新点总结</h1><p>本文不同于常见的聚焦于SAR图像分类模型的对抗样本设计,而是提出了针对SAR目标检测模型的攻击方法.对于SAR图像而言,不同像素之间存在着明显的上下文信息相关性(目标-目标,目标-背景等等),影响了目标检测模型的决策过程.该论文首次提出通过CRF干扰SAR目标像素和背景像素之间的内在联系,设计了代表目标-背景区域特征向量差异性的语义信息损失,结合目标检测模型已有的目标损失,回归损失和分类损失进行loss最大优化,通过尝试最大化迭代过程前后目标-背景区域的特征向量的CRF差异实现攻击.</p><h1 id="方法介绍"><a href="#方法介绍" class="headerlink" title="方法介绍"></a>方法介绍</h1><p>结合式(8)和式(9),CFR的表达式可以理解为通过基于Gibbs分布的概率分布与能量函数之间的关系,即式(8)的指数表达式部分可以理解为负能量函数,能量越低,对应标签分配y的可能性越大,而取负能量表达形式的原因是物理学中低能量状态出现的概率更高,对应的简化表达式为:</p><script type="math/tex; mode=display">P(Y|X) = \frac{1}{Z(X)}exp(-E(Y,X)) \tag{6}</script><p>因此,在从CRF视角下的目标检测任务即为最小化能量函数.CRFA的后续设计也基于能量函数展开(同时符合loss上升实现对抗的优化方向).论文从一元序列(unary)和成对序列(pairwise)两个角度出发设计了相应的特征函数,分别为$\phi_i$和$\phi_{ij}$,相应的表达式如下.</p><script type="math/tex; mode=display">\phi_i(y_i,X) = -ln(P(y_i = l_k | X)) \tag{11}</script><script type="math/tex; mode=display">\phi_{ij}(y_i,y_j,X) = \begin{cases}0 & \ if \  x_i = x_j \\ 1 + \theta \cdot \frac{exp(-\| x_i - x_j \|^2)}{\| i - j  \|^2} & \ otherwise\end{cases} \tag{12}</script><p>对于式(12)的pairwise表达项,论文解释为当输入的两个像素有相似的纹理或色彩特征并被分类为不同类别时,二元项的值更大,最终的能量表达式为:</p><script type="math/tex; mode=display">E(Y|X) = \sum_{i \in V} \phi_i(y_i,X) + \gamma \sum_{i \in V , \ j \in N_i} \phi_{ij}(y_i,y_j,X)  \tag{13}</script><p>论文提出,图像加入扰动前后的E的差别应该尽可能大,最终的伪代码流程和攻击示意图如下所示.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/23837804083de5ae3fde456c0481d7ca.png" alt="23837804083de5ae3fde456c0481d7ca"></p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>论文使用SSDD数据集训练了YOLOv3,SSD和Faster RCNN三种目标检测网络,在白盒攻击场景下与不同对抗算法比较的实验结果如下,可以看出作者在实验中并未将同一种攻击算法在所有模型上都进行测试.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/6bf8a3a38e6f172bdf91bc77d955b575.png" alt="6bf8a3a38e6f172bdf91bc77d955b575"></p><p>此外,作者还以替代模型的形式测试了CRFA的对抗样本攻击迁移性,结果表明算法展现了一定的攻击迁移能力,但并不显著,统计结果如下图所示.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/e76cc1f82da3a2b16d4ae0e358ede7f9.png" alt="e76cc1f82da3a2b16d4ae0e358ede7f9"></p><p>同时作者也进行了部分消融实验,在此仅展示不同损失项叠加的结果,说明了引入的额外损失项确实促进了算法的攻击效果.下图中√表示加入的损失函数项,以第二行作为基础逐渐累加.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/10cb1b63c167cc8908351c78cd5bcfaa.png" alt="10cb1b63c167cc8908351c78cd5bcfaa"></p><h1 id="总结与回顾"><a href="#总结与回顾" class="headerlink" title="总结与回顾"></a>总结与回顾</h1><p>CRFA结合SAR图像的成像机理,首次从前景-背景的图像成像结果特点出发,选择介于CRF工具引导模型损失函数在梯度上升迭代的过程中破坏图像的前景-背景分布相似性,从而达到更好的攻击效果.但从原理上看,对抗样本的效果下限仍是由梯度迭代上升操作本身进行保证的,同时实验部分没有做到对抗方法的统一,缺少了说服力.</p><p>最后,也必须说明这篇文章的成稿质量低到了令人发指的程度.SAR领域的AI文章读起来云里雾里让人难受并不少见,但这篇读到如鲠在喉的效果还是稀缺的.出现的问题如下:</p><ul><li>变量定义时缺少主语或引用不当</li></ul><figure>  <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250811201213.png" style="width: 100%; height: 6%; object-fit: cover;"></figure><figure>  <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250811201329.png" style="width: 100%; height: 6%; object-fit: cover;"></figure><ul><li>相关变量缺少定义说明</li></ul><figure style="text-align: center;">  <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250811201348.png" style="width: 100%; height: 6%; object-fit: cover;"></figure><ul><li>CRF具体的训练过程没有明确说明,以及伪代码部分依赖RPN训练CRF,没有说明如何迁移到YOLOv3的单阶段模型上.</li></ul><figure style="text-align: center;">  <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/65f551d265932555e29c33f64c00f312.png" style="width: 100%; height: 6%; object-fit: cover;"></figure><ul><li>指二为三:声明有三个迭代终止情况,但只列举了两条.</li></ul><figure style="text-align: center;">  <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/20250811201752.png" style="width: 100%; height: 6%; object-fit: cover;"></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;div style=&quot;text-align: center;&quot;&gt; 
关于论文CRFA的笔记  
&lt;/div&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="对抗样本" scheme="http://boremycin.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记:DeCoWa</title>
    <link href="http://boremycin.github.io/2025/06/10/paper-decowa/"/>
    <id>http://boremycin.github.io/2025/06/10/paper-decowa/</id>
    <published>2025-06-10T05:43:14.000Z</published>
    <updated>2025-06-10T05:45:18.582Z</updated>
    
    <content type="html"><![CDATA[<p><div style="text-align: center;"> 关于DeCoWa的论文笔记</div><br><span id="more"></span></p><h1 id="发表去向"><a href="#发表去向" class="headerlink" title="发表去向"></a>发表去向</h1><p>论文收录于AAAI2024</p><h1 id="创新点总结"><a href="#创新点总结" class="headerlink" title="创新点总结"></a>创新点总结</h1><p>针对不同类型模型（例如CNN模型和ViT模型）之间的架构差异，作者从对抗样本攻击迁移性出发，提出了基于图像变换增强的对抗攻击方法。该方法在现有的MI-FGSM攻击方法的基础上，从图像形态变换的角度出发继续攻击效果的增强。本文提出的攻击方法名为Deformation-Constrained Warping Attack(DeCoWA)，该方法通过对输入图像（或类似音频，视频数据）进行弹性变换（elastic deformation）来获取丰富的全局信息并避免图像语义信息损失.</p><h1 id="相关背景"><a href="#相关背景" class="headerlink" title="相关背景"></a>相关背景</h1><h2 id="CNN与ViT的架构差异"><a href="#CNN与ViT的架构差异" class="headerlink" title="CNN与ViT的架构差异"></a>CNN与ViT的架构差异</h2><p>一般认为,CNN架构模型近似于高通滤波器,会捕捉图像的高频信息作为判断特征,而ViT架构(即Transformer模型)则近似于低通滤波器,会捕捉图像的全局信息.在论文的开始部分,作者利用ResNet50和DeiT-B模型进行了简单实验初步验证了以上观点.结果如下图.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/4f53d165b9c6ad72fa4c49ce6a9999ef" alt="4f53d165b9c6ad72fa4c49ce6a9999ef"></p><p>对于左图实验,作者对完整图像进行patch划分,并对其中的部分图像进行模糊操作.结果表明随着被模糊图块占比的增加,CNN模型出现了明显的性能降低而ViT模型仍能保持较高的准确率;对于右图实验,作者则是重复局部patch来构建被测试图像,结果显示ViT模型的分类准确率的下降程度比CNN模型更加显著.</p><h2 id="迁移对抗攻击"><a href="#迁移对抗攻击" class="headerlink" title="迁移对抗攻击"></a>迁移对抗攻击</h2><p>在ViT模型被广泛应用之前，对抗攻击迁移性主要聚焦于CNN模型之间。经典的迁移性提升对抗算法的设计往往从<strong>动量增强</strong>、<strong>方差调整（variance tuning，如引入对损失函数的方差控制）</strong>、<strong>知识先验（knowledge-based）</strong>和<strong>基于生成模型</strong>等角度出发进行设计。其中作者特别提到了基于梯度的动量增强的迁移攻击算法：MIM算法在FGSM的基础上加入动量项来保持梯度方向；SI-NI-FGSM算法继续讲动量项设置为Nestero项并加入了方差调整。   </p><p>对于SI-NI-FGSM的具体说明如下：NI值Nesterov Iterative，该方法将加入扰动的目标由第t轮的图像$x_t$变更为了包含累计梯度信息的新变量$x_t + \alpha \cdot \mu \cdot g_t$，其中$g_t$为第t轮的累计梯度动量。使用Nesterov Accelerated Gradient（NAG）的动机是该方法可以加快攻击收敛速度并在损失函数分布空间中跳出局部最优，获得更有效的扰动矩阵。$g_t$的迭代公式如下。  </p><script type="math/tex; mode=display">g_(t+1) = \mu \cdot g_t + \nabla_x J (x_t + \alpha \cdot \mu \cdot g_t,y_{true})</script><p>上式中$J$为损失函数,$y_{true}$为图像真实标签.在更新$x_t$时思想与FGSM相同,只是梯度方向采用$g_t$作为替代.对于方差修正的部分,则采用了尺度不变法(Scale-Invariant Method，SIM).即对于输入图像x，生成m个不通过尺度的图像$x_1’,x_2’,…,x_m’$后在每个缩放版本的图像上利用当前迭代的临时版本计算梯度$x_t$计算相应的梯度(加入尺度对齐处理),最后对所有缩放尺度上的梯度求平均或求和，即获得最终的聚合梯度。</p><p>SI-NI-FGSM算法结合了 Nesterov 加速梯度在优化过程中的稳定性和“向前看”能力，以及尺度不变性在不同尺度上生成鲁棒扰动的能力，使得生成的对抗性样本在攻击其他黑盒模型时表现出显著更高的成功率。减少了对抗性样本对白盒模型特定特征的“过拟合”。有效提高了针对CNN模型的对抗攻击可迁移性。</p><p>随着基于Transformer的ViT模型不断发展,研究者也开始关注针对ViT模型的可迁移黑盒攻击.其中较为前沿的有ATA(Architecture-Oriented Transferable Attacking)和TGR(Token Gradient regularization)等.以ATA算法为例,该算法重点关注了不同架构模型间的梯度差异,同时利用多个替代模型进行梯度更新,对应的梯度更新公式如下所示.  </p><script type="math/tex; mode=display">g_{t+1} = \mu \cdot g_t + \frac{\nabla_x \mathcal{L}(\hat{x_t},y) + \lambda \cdot \nabla_x\mathcal{D}(f_1(\hat{x_t}),f_2(\hat{x_t}))}{\|\nabla_x \mathcal{L}(\hat{x_t},y) + \lambda \cdot \nabla_x\mathcal{D}(f_1(\hat{x_t}),f_2(\hat{x_t}))\|_1}</script><p>上式中$\mathcal{L}$是标准的交叉熵损失, $\mathcal{D}$是散度度量函数(如KL散度),$\lambda$是平衡二者的超参数,$f_1$和$f_2$分别代表CNN架构和ViT架构的模型.</p><p>此外,模型还加入了结构感知的随机降噪模块(Structure-aware Random Denoise, SRD),该模块的出发点是不同架构模型在深层特征上的差异较大,但在模型浅层空间中同一样本对应的特征(如边缘,纹理等)则往往较为接近.SRD模块首先在对抗样本迭代过程中加入随机降噪的操作,其中降噪的强度,概率分布和参数随机变化(如高斯滤波,均值滤波或中值滤波等),该降噪操作可以提升对抗样本的泛化性能,避免对单一模型的梯度信息过拟合;其次进行结构保持,当认为不同模型都会关注图像的边缘和轮廓等区域时,通过避免对该区域的降噪处理来保持对抗样本的攻击性能,实现方法为用诸如Canny或Sobel等标准边缘检测算子提取掩码矩阵,在降噪过程中保留前后差值矩阵,向降噪后的图像加入掩码部分的差值即实现边缘区域的信息复原,然后进行上述的梯度更新和对抗扰动迭代过程.</p><h1 id="核心算法"><a href="#核心算法" class="headerlink" title="核心算法"></a>核心算法</h1><p>本文实现对抗迁移的核心是在替代模型上在对抗样本的迭代过程中加入图像变换过程,选择的参考算法是TPS算法.TPS是经典的非线性差值和图像变形算法,输入一组源点(包含x,y坐标),输出对应的目标点.该函数的通常变换形式如下.  </p><script type="math/tex; mode=display">f(x,y)= a_1 + a_2x + a_3y + \sum_{i=1}^N w_i \cdot U(\|(x,y)-(x_i,y_i)\|)</script><p>上式中的$a_1,a_2,a_3$为仿射变换的参数,$w_i$为控制非线性扭曲的权重.$U(r) = r^2log(r)$,为TPS基函数,N为变换点的个数.</p><p>DeCoWa在TPS的基础上,针对源点选择具有随机性的问题进行改进,加入了自适应控制策略,原始的坐标点选择向量为$\xi$,从正态分布中随机采样获得,变换函数为$T_v$,则结合对抗场景的目标坐标点选择向量应使替代模型的损失函数取最小,以减少图像全局语义信息的损失.对应优化目标表达式如下. </p><script type="math/tex; mode=display">\hat{\xi} = \arg \min_\xi \mathcal{L}(\mathcal{S_\theta}(T_v(x_t^{adv};\xi)),y)</script><p>上式中$S_\theta$为替代模型,y为真实标签.结合迭代更新的反向传播过程,设置学习率为$\beta$,对应的更新公式如下:</p><script type="math/tex; mode=display">\hat{\xi} = \xi - \beta \cdot \nabla_\xi (\mathcal{L}(\mathcal{S_\theta}(T_v(x_t^{adv};\xi)),y))</script><p>论文给出了对应的更新过程示意图和图像变换结果示意图,如下图所示.结果表明,当前的变化算法可以增加对抗样本的局部特征信息并同时不破坏全局语义信息.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/6d2055ecd7d3542dd5f27088f00b423f" alt="6d2055ecd7d3542dd5f27088f00b423f"></p><p>结合MI-FGSM后的非动量梯度计算公式如下:</p><script type="math/tex; mode=display">\hat{g}_{t+1} = \frac{1}{N} \sum_{j=0}^{N}\nabla_{x_t^{adv}}\mathcal{L}(\mathcal{S_\theta}(T_{dc}(x_t^{adv};\hat{\xi_j})),y)</script><p>上式中N为图像进行扭曲变换的次数.结合动量信息后的梯度计算公式如下:</p><script type="math/tex; mode=display">g_{t+1} = \mu \cdot g_t + \frac{\hat{g}_{t+1}}{\|\hat{g}_{t+1}\|_1}</script><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>作者针对多种场景(如视频,语音)下的跨模型攻击进行了相应测试,这里演示图像分类场景下的DeCoWa算法迁移攻击性能.  </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/7d6798d7674105261d236f78cbf9a74c" alt="7d6798d7674105261d236f78cbf9a74c"></p><p>此外,通过Grad-CAM工具比较不同对抗算法对同一张图像的热力图结果,实验结果如下,可以看出DeCoWa算法扩大了模型的注意力区域并减少了不同模型间的注意分布差异,从而实现更理想的跨模型迁移性能.</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/fb5016c85a9d9216bde20c6626920415" alt="fb5016c85a9d9216bde20c6626920415"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>后续可以结合SAR的频率域背景杂波分布和目标成像特征,探索基于频率域的SAR图像可迁移变换并引入梯度动量的思想.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;div style=&quot;text-align: center;&quot;&gt; 
关于DeCoWa的论文笔记
&lt;/div&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="对抗样本" scheme="http://boremycin.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
    <category term="迁移攻击" scheme="http://boremycin.github.io/tags/%E8%BF%81%E7%A7%BB%E6%94%BB%E5%87%BB/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：GAN和pixel2pixel</title>
    <link href="http://boremycin.github.io/2025/03/25/paper5-GAN/"/>
    <id>http://boremycin.github.io/2025/03/25/paper5-GAN/</id>
    <published>2025-03-25T07:01:18.000Z</published>
    <updated>2025-03-25T07:05:46.193Z</updated>
    
    <content type="html"><![CDATA[<p><div style="text-align: center;"> GAN模型相关的两篇笔记</div><br><span id="more"></span></p><h1 id="GAN系列论文"><a href="#GAN系列论文" class="headerlink" title="GAN系列论文"></a>GAN系列论文</h1><h2 id="Generative-Adversarial-Nets"><a href="#Generative-Adversarial-Nets" class="headerlink" title="Generative Adversarial Nets"></a>Generative Adversarial Nets</h2><h3 id="发表去向"><a href="#发表去向" class="headerlink" title="发表去向"></a>发表去向</h3><p>Ian Goodfellow发表在NIPS2014的首篇GAN生成模型论文</p><h3 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h3><p>GAN网络有生成网络G和判别网络D组成。与依赖Markov链的图像生成方法不同，GAN网络以MLP为主干，通过G拟合目标数据集分布空间的概率密度函数参数，通过D判别生成图像是否可以被认为原始空间中的图像，以此达到理想的图像生成效果。与以往的VAEs（variational autoencoders）模型不同，GAN从可见单元中提取差分信息，因此无法对离散分布数据建模；VAE从隐藏单元中提取差分信息，因此不能处理离散隐变量。此外，GAN模型的训练过程不同于以往分类模型的损失函数最小值优化问题，而是一种<strong>minimax game</strong>，即对生成器G进行最小值优化，对判别器D进行最大值优化，且优化过程可以设定为同时进行。对数学结果进行分析可知，模型会在优化方程的鞍点处（saddle point）停止。 </p><h3 id="模型数学推导与算法流程"><a href="#模型数学推导与算法流程" class="headerlink" title="模型数学推导与算法流程"></a>模型数学推导与算法流程</h3><p>生成器G在模型中相当于对数据集分布的概率密度函数进行参数拟合，当真实数据分布满足$\mathbf{z}\sim p_z$，G则代表待优化的概率密度函数$p_g$，对应的优化目标为数据的概率分布$p_{data}$。判别器D的构造为MLP，输出代表概率的单一标量，其作用为判定输出图像<strong>x</strong>来自$p_{data}$而非$p_g$的概率。在训练过程中，同时对D进行maximum训练，使来自训练集的真实图像和来自G的生成图像与其对应标签的预测概率最大；对G进行minimum训练，使$log(1-D(G(z)))$的值最小。最终，模型训练时的损失函数(正文中成为价值函数，value function)表达式如下。  </p><script type="math/tex; mode=display">\min_{G}\max_{D}V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[logD(x)] + \mathbb{E}_{z \sim p_{z}(z)}[log(1 - D(G(z)))]</script><p>对于作者所提出的value function，将其表达简化为$f(x) = a\log(x) + b\ log(1-x)$,当取极值时x的取值为a/a+b，带入value function表达式后最终的优化结果如下。  </p><script type="math/tex; mode=display">D_{G}^{*} = \frac{p_{data}(x)}{p_{data}(x) + p_g(x)},\ when\ G\ fixed</script><p>至此可以完成对D的优化，将优化后的D表达式带入value function，可以得出对G的minimum优化表达式，如下所示。  </p><script type="math/tex; mode=display">C(G) = \max_{D}V(G,D) = \mathbb{E_{x \sim p_{data}(x)}}[log\frac{p_{data}(x)}{p_{data}(x) + p_g(x)}] + \mathbb{E_{x \sim p_{g}}}[log\frac{p_{g}(x)}{p_{data}(x) + p_g(x)}]</script><p>当$p_{data} = p_{g}$时上式有对应最小值解，对应结果如下。  </p><script type="math/tex; mode=display">C(G) =  -log(4) + KL(p_{data} \parallel \frac{p_{data} + p_g}{2}) + KL(p_{g} \parallel \frac{p_{data} + p_g}{2})</script><p>上式中的KL为KL散度，用于描述两个概率分布（如Q(x)和P(x)）的相似程度。KL散度是非对称的，也不满足三角不等式，不是严格一样上的距离度量。离散分布对应表达形式如下。若为连续变量改为积分形式即可。</p><script type="math/tex; mode=display">KL(Q \parallel P) = \sum_i Q(i)log(\frac{Q(i)}{P(i)})</script><p>最终模型的算法流程如下所示。 </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/276141da0f7a87ff85d1993cdd71af11" alt="276141da0f7a87ff85d1993cdd71af11"></p><h2 id="pixel2pixel"><a href="#pixel2pixel" class="headerlink" title="pixel2pixel"></a>pixel2pixel</h2><h3 id="发表去向-1"><a href="#发表去向-1" class="headerlink" title="发表去向"></a>发表去向</h3><p>收录于CVPR2017</p><h3 id="创新点总结"><a href="#创新点总结" class="headerlink" title="创新点总结"></a>创新点总结</h3><p>pixel2pixel方法对于在生成器G和判别器D上都对以往的GAN网络进行了改进。对于G，作者主要进行了网络改进，将MLP架构更换为引入了skip connection的UNet架构；对于D，作者在现有的cGAN（condition GAN）研究基础上，探究了向判别器损失函数加入额外条件项对生成效果的改进作用。  </p><h3 id="主要方法-1"><a href="#主要方法-1" class="headerlink" title="主要方法"></a>主要方法</h3><h4 id="生成器G"><a href="#生成器G" class="headerlink" title="生成器G"></a>生成器G</h4><p>以往GAN网络中的生成器在噪声向量z和标签图像y中形成map映射，即$G:z \rightarrow y$，而cGAN在生成来源中加入了被观察图像x和随机噪声向量z，即$G: \{x,z\} \rightarrow y$。对于cGAN的对应value function如下。优化目标同样是对G取min，对D取max。  </p><script type="math/tex; mode=display">L_{cGAN} = \mathbb{E}_{x,y}[logD(x,y)] + \mathbb{E}_{x,z}[log(1 - D(x,G(x,z)))]</script><p>已有的研究表明向优化目标中加入传统的损失函数（如L2范数），不会对D的判别功能造成影响，但可以使生成结果在加入损失函数的物理意义角度更加接近真实标签。作者通过实验得出使用L1范数相较于L2范数可以降低生成图像的模糊效果。对应的目标函数形式如下。</p><script type="math/tex; mode=display">G^{*} = \arg \min_{G}\max_{D}L_{cGAN}(G,D) + \lambda \mathcal{L}_{L1}(G)</script><script type="math/tex; mode=display">\mathcal{L}_{L1}(G) = \mathbb{E}_{x,y,z}[\|{y - G(x,z)}\|_1]</script><p>对于映射输入中的图像x和随机噪声z，二者都是必要的。缺少随机噪声z后生成器G会出现过拟合的情况，生成结果会是确定性的（deterministic）而无法对训练数据以外图像分布实现理想的拟合效果。但在作者的实验中，加入噪声的策略没有显示出明显的作用——G在训练过程中会忽略加入的噪声z。因此作者提出以dropout的形式加入噪声，即在G的隐藏层中应用dropout，以随机丢弃部分神经元的方法影响G的输出分布，从而间接引入噪声。与直接向x中叠加z相比，这种噪声是一种隐式噪声且作用于网络内部。  </p><p>此外，作者发现模型的生成结果往往缺少随机性（针对相同输入x的对应输出也高度相同），而并未在论文中明确造成这种现象的原因。</p><p>对于生成器G的架构选择，以往的cGAN方法以Encoder-Decoder架构为主，但在例如image colorization等任务中，输出图像和输出图像会共享较多的语义信息（如目标位置，轮廓或图像的纹理特征等）。因此作者采用了加入了skip connection的UNet架构来避免原有架构中的bottle-neck结构造成的信息损失。相应的结构示意图如下。</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/014a9cfdbe75df7a28ca330232a1ea4f" alt="014a9cfdbe75df7a28ca330232a1ea4f"></p><h4 id="判别器D"><a href="#判别器D" class="headerlink" title="判别器D"></a>判别器D</h4><p>实验结果表明cGAN中引入的L2 loss（以及L1 loss）会在输出结果中捕捉到更多低频信息，但同时引入更明显的模糊效果（高频信息损失）。为提高高频信息的正确性，作者提出了一种PatchGAN的判别器D架构，从输出图像对的$N \times N$patch角度进行fake-real判别，并以卷积的方式遍历整个图像，将所有结果的平均值作为D的最终输出。后续实验发现参数N可以远小于原有的图像尺寸，在减少模型参数，加快运算速度的同时仍可以实现理想的生成效果。相应消融实验结果如下。</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/88c558f32b552c0505b7f28c44e0a2ea" alt="88c558f32b552c0505b7f28c44e0a2ea"></p><p>PatchGAN架构判别器D会将输出图像视作Markov random field，假设patch尺度下的不同pixel存在独立性。因此，该判别器可以实现图像纹理或风格的判定。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>作者从多个角度进行了完备且充分的实验验证pixel2pixel-GAN的转换有效性，并探究了不同的参数设置和不同的任务场景对模型生成结果的影响，在此不再赘述。仅展示不同损失函数的引入对模型输出的结果影响，对应实验结果如下。  </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/8cad28bfdf3e84477708bad4f514fb27" alt="8cad28bfdf3e84477708bad4f514fb27"></p><h2 id="系列总结"><a href="#系列总结" class="headerlink" title="系列总结"></a>系列总结</h2><p>从GAN到pixel2pixel，生成网络的性能和应用场景都得到了极大的拓展。结合当前的SAR仿真-实测对抗攻击场景，可以从G的损失函数引入角度出发进行攻击角度的设置和优化，例如原有方法从图像对相似的角度引入了L1范数，而攻击场景可以将对应的约束对象替换为图像对在待攻击目标模型或者替代模型中的梯度一致性，通过此类约束，在后续应用基于梯度信息的攻击时可以生成更有效的对抗扰动。  </p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;div style=&quot;text-align: center;&quot;&gt; 
GAN模型相关的两篇笔记
&lt;/div&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：《Towards Transferable Adversarial Attacks with Centralized Perturbation》</title>
    <link href="http://boremycin.github.io/2025/03/04/paper4/"/>
    <id>http://boremycin.github.io/2025/03/04/paper4/</id>
    <published>2025-03-04T04:23:15.000Z</published>
    <updated>2025-03-04T04:32:48.686Z</updated>
    
    <content type="html"><![CDATA[<p><div style="text-align: center;"> 关于《Towards Transferable Adversarial Attacks with Centralized Perturbation》的论文笔记</div><br><span id="more"></span></p><h1 id="发表去向"><a href="#发表去向" class="headerlink" title="发表去向"></a>发表去向</h1><p>论文最终被AAAI2024接收</p><h1 id="预备知识"><a href="#预备知识" class="headerlink" title="预备知识"></a>预备知识</h1><h2 id="YCbCr色彩空间"><a href="#YCbCr色彩空间" class="headerlink" title="YCbCr色彩空间"></a>YCbCr色彩空间</h2><p>本论文的主要操作都基于将彩色图像RGB矩阵转换为YCrCb矩阵，并针对不同通道进行处理。YCrCb色彩空间常用于数码视频成像系统，其中的Y通道代表图像的明亮度（luma），Cb和Cb通道则反映图像的色度（chroma）信息。</p><figure>  <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/00d10b652a6f6818acd38a6894f23312" alt="00d10b652a6f6818acd38a6894f23312">  <figcaption>图1. RGB与YCrCb色彩空间展示</figcaption></figure><p>由RGB至YCC的色彩空间转换可分为模拟信号转换和数字信号转换，在此只关注后者的转换关系。根据标准ITU-R BT.601，对应的转换公式如下：  </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/a0263dfbd33ff729f9521bdcd4134b8f" alt="a0263dfbd33ff729f9521bdcd4134b8f"></p><h2 id="图像DCT变换"><a href="#图像DCT变换" class="headerlink" title="图像DCT变换"></a>图像DCT变换</h2><p>论文的主要创新点在于将对图像的扰动施加过程转换到频率域中进行，同时作者使用了离散余弦变换（DCT）而非离散傅里叶变换（DFT）来实现此过程。与傅里叶变换相比DCT只使用了实数进行变换，一个N=8的变换矩阵图如下图所示。该矩阵是实矩阵，有正交且不对称的特性。DCT被应用于JPEG压缩编码当中，可图像视觉信息损失较少的有损压缩。</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/a04fbede927547029c5d35ba0cb0bc19" alt="a04fbede927547029c5d35ba0cb0bc19"></p><p>DCT的变换公式如下所示：</p><script type="math/tex; mode=display">s(x,u) = \alpha(u)cos(\frac{(2x+1)u\pi}{2N})</script><script type="math/tex; mode=display">\alpha(u) = \begin{cases} \sqrt{1/N} & & \text{if } u = 0 \\\sqrt{2/N} & & \text{if } u =1,2,...,N-1 \end{cases}</script><p>对于二维离散图像矩阵，由灰度矩阵转换为频率矩阵的转换公式如下：</p><script type="math/tex; mode=display">T(u,v) = \sum_{x=0}^{N-1}\sum_{y=0}^{N-1}f(x,y)s(x,y,u,v)</script><script type="math/tex; mode=display">s(x,y,u,v) = s_1(x,u) s_2(y,v)</script><h1 id="摘要总结"><a href="#摘要总结" class="headerlink" title="摘要总结"></a>摘要总结</h1><p>当前的主流对抗样本会在图像灰度矩阵上施加全局扰动，往往导致对抗样本对梯度来源模型的过拟合而降低对抗样本的可迁移性。向主要的图像目标区域加入模型相关性弱的扰动是提升对抗有效性的关键，但在空域中限制扰动范围被证明对对抗样本的迁移性提升不足。对于上述情况，论文作者提出了基于频率域的梯度扰动优化对抗样本设计，使产生的对抗扰动可以降低对来源模型的过拟合，提升对抗样本的迁移性能并可以扰动多种模型防御方法，最大限度地保留攻击有效性。    </p><p>作者的主要创新点如下：</p><ol><li>设计了基于DCT的共享频率分解算法，通过频域系数的量化处理消除多余扰动，使频率域的扰动限定在中心频率当中，从而避免对梯度源模型的过拟合。</li><li>实现了量化处理矩阵的并行优化，确保了与每一步模型预测结果的对齐。</li></ol><h1 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h1><h2 id="频率分解（Frequency-Decomposition）"><a href="#频率分解（Frequency-Decomposition）" class="headerlink" title="频率分解（Frequency Decomposition）"></a>频率分解（Frequency Decomposition）</h2><p>假定输入图像为8bit的（3，224，224）图像X，首先将RGB图像转换为YCC图像，然后对每个channel进行DCT频率变换；接着进行block化操作将图像矩阵转化为多个8*8的blocks，将（B，C，W，H）的图像转化为（B，C，W，H/64，8，8）；然后对每一个频率矩阵进行量化操作（quantization），引入量化矩阵Qs来除去多余的频率系数；最后进行blocks的合并，使输入矩阵变为原始维度，然后进行IDCT操作获取分解后的图像X。完成以上流程后即实现对图像的频率分解，对应流程图如下所示。  </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/3de78e58ed30b6b0cb57d31fa92d8fa5" alt="3de78e58ed30b6b0cb57d31fa92d8fa5"></p><p>对于上述的量化矩阵Qs，以Y通道为例其量化处理过程如下：</p><script type="math/tex; mode=display">B_Y = blockify(DCT(Y))</script><script type="math/tex; mode=display">B_Y^{'} = B_Y\odot Q_Y</script><p>不同于JPEG压缩过程中的量化矩阵，本文的量化矩阵定义如下：</p><script type="math/tex; mode=display">Q = (q_{ij}) \subseteq \{0，1\}^{m\times m}</script><p>Q矩阵初始化为单位向量<strong>1</strong>，在每一轮的量化过程中不重要的频率参数均被自动清楚，使得扰动主要添加在DNN预测中更加关键的区域。对于扰动矩阵$\delta_t$，通过下式的优化过程可以将其进行中心化产生新的扰动$\delta_t^{‘}$。</p><script type="math/tex; mode=display">\delta_t^{'} = \mathcal{K}(\delta_t;Q_t)</script><p>上式中的$\mathcal{K}$即为频率分解和量化过程，每一轮迭代的Qt在该过程固定。  </p><h2 id="优化量化矩阵Qt"><a href="#优化量化矩阵Qt" class="headerlink" title="优化量化矩阵Qt"></a>优化量化矩阵Qt</h2><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/2243d7411336bcb596664f85678d0c12" alt="2243d7411336bcb596664f85678d0c12"></p><p>施加扰动产生对抗样本的整体流程如上图所示。在stage2中完成对Qt的更新过程。更新的主要思想是将该轮次的对抗样本输出到源模型中，通过设计的loss损失函数实现Qt的更新。更新的主要思路是利用源模型输出梯度矩阵的变换程度，矩阵Q在每一轮优化$x_t^{adv}$后应当使得模型输出真实标签置信度降低。对应的损失函数公式如下：  </p><script type="math/tex; mode=display">\arg\max_{Q_t}\mathcal{J}(\mathcal{K}(x_t^{adv};Q_t),y)</script><p>其中的损失$\mathcal{J}$是模型输出的交叉熵损失。作者认为Qt的更新过程一方面使Qt始终准确反映频率参数矩阵对模型预测准确性的影响；另一方面源模型的梯度矩阵在更新过程中会不断累加，从而提升泛化性能。模型算法流程如下所示。</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/5a630fd839119e7b1f2c446c0bd1433d" alt="5a630fd839119e7b1f2c446c0bd1433d">  </p><p>在更新Qt的反向传播过程中，优化结果用m*m的矩阵P表示，对于YCC三通道每个通道有不同的P和相应的优化系数$0\leq r\leq 1$，对于Y，Cr，Cb分别对应为（0.9，0.05，0.05）。更新公式如下所示。  </p><script type="math/tex; mode=display">Q = R(P;r) = \begin{cases} 1, & \text{where }  P_{ij} \geq \rho \\0 & \text{otherwise } \end{cases}</script><p>上式中$\rho$为相应通道矩阵的1-r分位数。此外，作者使用了直通估计器（STE）避免了二值化矩阵Qt的非连续性导致的梯度消失。</p><p>结合上述更新过程可以看出，作者在更新量化矩阵部分主要保留了Y通道的信息，而略去了大部分的Cr和Cb通道的图像信息，即认为图像的明亮度信息会对模型准确判别的影响更大，且在不同模型之间具备通用性。 </p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><h2 id="实验准备"><a href="#实验准备" class="headerlink" title="实验准备"></a>实验准备</h2><p>论文使用NIPS2017的Adversarial Learning Challenge数据集，包含1000张ImageNet的图像；训练了ResNet50，VGG-19和Inception-v3作为source model；采用了不同基于梯度的FGSM类攻击方法作为基准攻击方法，包括MI、DI、TI、VMI和SI-NI-FGSM；最后作者还额外设计模型防御和对抗训练等场景验证扰动矩阵的泛化攻击鲁棒性。 </p><h2 id="攻击迁移性结果"><a href="#攻击迁移性结果" class="headerlink" title="攻击迁移性结果"></a>攻击迁移性结果</h2><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/092c9072871f675b4042166c65e98a90" alt="092c9072871f675b4042166c65e98a90"></p><p>在黑盒场景下对于ResNet、VGG和Dense等架构的模型本论文方法对生成扰动的攻击迁移性提升起到了一定的促进作用，但对于ConvNeXt和ViT架构模型，原始FGSM方法的攻击效果和相应的迁移性提升表现都不理想。</p><h2 id="对防御方法的攻击效果"><a href="#对防御方法的攻击效果" class="headerlink" title="对防御方法的攻击效果"></a>对防御方法的攻击效果</h2><p>对于以JPEG和Bit-depth reduction为代表的滤波器防御方法，本文方法的攻击结果如下。</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/dd6ef8b49a9eae203550c9fa33a6dd00" alt="dd6ef8b49a9eae203550c9fa33a6dd00">  </p><p>对于对抗训练的防御方法，对应结果如下。  </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/185981e0411bc1727b31dc04eadea665" alt="185981e0411bc1727b31dc04eadea665"> </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本论文提出了基于频率域的迁移攻击增强方法，搭建了完整的频率域攻击和优化流程，并在FGSM为代表的梯度攻击方法上实现了理想的攻击效果，后续可以借鉴该论文的思路进一步探索频率域的攻击手段。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;div style=&quot;text-align: center;&quot;&gt; 
关于《Towards Transferable Adversarial Attacks with Centralized Perturbation》的论文笔记
&lt;/div&gt;&lt;br&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="对抗样本" scheme="http://boremycin.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：Similarity to Availability</title>
    <link href="http://boremycin.github.io/2025/02/21/paper3-s2a/"/>
    <id>http://boremycin.github.io/2025/02/21/paper3-s2a/</id>
    <published>2025-02-21T01:50:35.000Z</published>
    <updated>2025-02-21T01:59:44.928Z</updated>
    
    <content type="html"><![CDATA[<div style="text-align: center;"> 关于《Similarity to Availability: Synthetic Data Assisted SAR Target Recognition via Global Feature Compensation》的论文笔记</div>  <span id="more"></span><h1 id="论文去向"><a href="#论文去向" class="headerlink" title="论文去向"></a>论文去向</h1><p>该论文被TAES2025收录 </p><h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><h2 id="核心创新点"><a href="#核心创新点" class="headerlink" title="核心创新点"></a>核心创新点</h2><p>本论文针对SAMPLE仿真实测数据集，基于当前SAR图像仿真数据（以CAD方法仿真获得）与实测数据相比质量不高，仿真与实测数据的统计分布具有明显差异这一问题，提出了改进GAN网络的GFC-Net网络，使该网络可以学习实测数据的全局信息和背景信息并以此提升输入的仿真数据的质量，在提升仿真图像与实测图像相似度的同时进一步提升了仿真图像的有效性，可以视为仿真图像的图像质量提升插件。  </p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>作者借鉴GAN网络的思想设计了生成器（Generator）和鉴别器（Discriminator），对于生成器部分采用了UNet架构进行图像生成并将其中的卷积模块更换为Transformer模块并改进为W-MMA（window multihead mix-attention），使UNet可以更好地捕捉到全局信息。针对SAR实测图像背景信息提出了BFC（background feature compensation）模块；对于判别器则采用了PatchGAN网络和二进制判别器来获取最终的判别损失。网络的架构示意图如下。   </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/6994907c74197863081df7dab3d8a562" alt="6994907c74197863081df7dab3d8a562">   </p><h3 id="BFC"><a href="#BFC" class="headerlink" title="BFC"></a>BFC</h3><p>在图像生成过程中，BFC模块的主要操作是将图像切割为多个blocks并在blocks之间建立跳跃连接（skip connections），以更好地区分出图像中的背景和目标。这些连接可以在仿真图像到实测图像的转换过程中转换low-level的纹理信息，同时仿真图像的原有语义信息，使转换后的图像不易失真。作者采用了swin transformer blocks（STBs）和UNet架构作为信息提取模块，以实现转换后的图像有更接近实测数据的背景纹理，UNet的主要优点在于使用更少的参数和训练数据便可以产生更高质量的分割掩膜。</p><div style="display: flex; justify-content: space-between;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/b5ee202f2a4c05c376989546c1508d87" alt="Image 1" style="max-width: 48%; height: auto;">    <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/79c3f91316bb983b49a4f8f60b60cfc9" alt="Image 2" style="max-width: 48%; height: auto;"></div>  <h3 id="W-MMA"><a href="#W-MMA" class="headerlink" title="W-MMA"></a>W-MMA</h3><p>尽管SAR实测数据和仿真数据在统计分布上有所不同，但二者存在结构特征的高度相似（例如图像中目标所在位置，和背景的语义关系），而注意力机制可以更好地学习到这些特征。W-MMA模块则是通过Transformer架构提高生成器对相关信息的学习效果。  </p><p>W-MMA将C*H*W的特征图分割为M*M的小窗，然后在小窗上分别进行操作。一方面，计算两对应窗之间的交叉注意力值，另一方面将两对应窗进行拼接成2*M*M的特征窗图并计算其自注意力值。相应的结果会被拼接并投射为C*H*W矩阵格式。  </p><p>对于输入的特征图$f_1$和$f_2$，M-WWA将其分割为各N个窗后首先计算$f_1$中各窗的自注意力值，计算公式如下： </p><script type="math/tex; mode=display">Self-Attention(Q,K,V) = softmax(\frac{QK^T}{\sqrt{d_k}} + B)V</script><p>接着计算$f_1$和$f_2$对应小窗的交叉注意力值，计算方法与上式相似但$W_Q$矩阵选自$f_1$而$W_K$与$W_V$矩阵选自$f_2$。此外，为进行多头操作，多头注意力函数会同时进行交叉注意力计算来获取不同位置和表征子空间的信息，对应计算公式如下：</p><script type="math/tex; mode=display">MultiHead(Q,K,V) = Concat(head_1,...,head_n)W^0</script><script type="math/tex; mode=display">head_i = Attention(QW_i^Q,KW_i^K,VW_i^V)</script><p>以上两注意力结果分别包含了特征图本身的特征信息和不同特征图同位置的对应信息，分别记为$O_1$和$O_2$，将二者拼接并重新映射到输入特征图维度，即可获得W-MMA模块的最终结果。在W-MMA模块中，实测图像的特征作为q来计算与之最接近的仿真图像特征。将W-MMA替换W-MSA即获得新的CSTB（cross STB）模块，用于最终的GAN生成器中，相应结构如下。</p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/4ec5d7dd68eebdeb3761912aeb9f9e83" alt="4ec5d7dd68eebdeb3761912aeb9f9e83"></p><h3 id="GFC-Net"><a href="#GFC-Net" class="headerlink" title="GFC-Net"></a>GFC-Net</h3><p>GFC-Net的具体网络架构如下图所示。对于其中的判别器，PatchGAN的作用为在patch尺度对图像进行惩罚判别，尝试却分N*N的图像patch是来自实测数据还是仿真数据，而二进制判别器更关注全局尺度。综合两者的判别器损失函数如下：</p><script type="math/tex; mode=display">Loss_D = \lambda Loss_{patchGAN} + (1 - \lambda) Loss_{binary}</script><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/6742160bc472a198df6abeade970ee73" alt="6742160bc472a198df6abeade970ee73">  </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>该方法提出了使用改进的GAN网络提升仿真数据质量，使之在下游任务中有更好的表现，更加贴近于真实数据。对于结合SAR仿真数据的对抗样本设计也可以从此角度出发，通过引入类似的translator提升基于仿真数据的对抗样本对由真实数据训练得到的目标网络的攻击性能。</p>]]></content>
    
    
    <summary type="html">&lt;div style=&quot;text-align: center;&quot;&gt; 
关于《Similarity to Availability: Synthetic Data Assisted SAR Target Recognition via Global Feature Compensation》的论文笔记
&lt;/div&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="SAR" scheme="http://boremycin.github.io/tags/SAR/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：One-Pixel-Attack</title>
    <link href="http://boremycin.github.io/2025/02/14/paper2-One-Pixel-Attack/"/>
    <id>http://boremycin.github.io/2025/02/14/paper2-One-Pixel-Attack/</id>
    <published>2025-02-14T10:35:18.000Z</published>
    <updated>2025-02-14T10:44:22.962Z</updated>
    
    <content type="html"><![CDATA[<div style="text-align: center;"> 关于One-Pixel-Attack的论文笔记  </div>  <span id="more"></span><h1 id="论文去向"><a href="#论文去向" class="headerlink" title="论文去向"></a>论文去向</h1><p>收录于2019年IEEE Transactions on Evolutionary Computation</p><h1 id="论文思路和方法"><a href="#论文思路和方法" class="headerlink" title="论文思路和方法"></a>论文思路和方法</h1><p>在One-Pixel Attack方法之前，诸如FGSM和JSMA等经典方法均产生全局扰动，该方法则着眼于限制扰动像素的个数并从该角度出发进行了CNN架构的VGG16、AllConv和AlexNet等经典架构的攻击机理解释。该方法的核心在于采用差分进化算法（Differential Evolution，DE）实现被扰动像素位置和RGB值的选取，从而摆脱了对待攻击模型架构与参数等白盒信息的依赖并不再要求模型的可微性，实现了对模型的黑盒攻击。</p><h2 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h2><p>对抗样本生成的数学表达可简化如下：  </p><script type="math/tex; mode=display">\mathop{\text{maximize}}\limits_{e(\mathbf{x})^*}  \ f_{adv}(\mathbf{x} + e(\mathbf{x}))</script><script type="math/tex; mode=display">subject\ to \ ||e(\mathbf{x})|| \leq L</script><p>其中 $\mathbf{x}$ 为输入到分类器的图像向量，所有的像素点视为离散的维度坐标，例如10*10*1的图像可看作1*100的向量变量。$e(\mathbf{x})$则表示对应的全局扰动向量，L表示该向量的最大扰动长度。在One-Pixel方法中，相应的数学表达如下：</p><script type="math/tex; mode=display">\mathop{\text{maximize}}\limits_{e(\mathbf{x})^*}  \ f_{adv}(\mathbf{x} + e(\mathbf{x}))</script><script type="math/tex; mode=display">subject\ to \ ||e(\mathbf{x})||_0 \leq d</script><p>上式中的d取较小的实数值，用来表示被更改的输入图像矩阵坐标轴的数量（即被选中的像素值个数）。除单像素攻击场景外，作者还实验了3像素和5像素攻击。此外，像素改变的程度不再加以限制，在图像较小时单点像素的更改已经实现了人眼可察觉的效果。  </p><p>在确定了图像扰动个数后，对于扰动位置和扰动程度的选择，作者采用了DE方法。该方法作为一种被较早提出的遗传优化算法，具有方法简单、对目标系统的信息需求少和全局最优解搜索能力强等优势。对于其中的第二点，One-Pixel方法在攻击时只需要访问模型对输入的分类结果软标签即可。</p><p>在One-Pixel Attack中，作者首先设定了n=400的候选点作为初代种群，每代的优化过程如下：  </p><script type="math/tex; mode=display">x_i(g+1)\ = x_{r1}(g) + F(x_{r2}(g) - x_{r3}(g))</script><script type="math/tex; mode=display">r1 \neq r2 \neq r3</script><p>上式中的 $x_i$ 为候选结果中的一个元素，包含坐标对和RGB值，r1-r3为随机数，F为缩放因子并设定为0.5，其作用为对r2和r3对应元素取差值平均，g则为进化代际指数。每一个候选结果在产生后便于其对应祖先结果进行比较并保留较好的结果。以CIFAR-10数据集为例，初始候选点坐标选取服从U（1，32）的均值分布，RGB值服从N（$\mu$=124,$\sigma$=127）的高斯分布。  </p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/9a48a4cc61207b23f63fcb63da7ade13" alt="9a48a4cc61207b23f63fcb63da7ade13">  </p><p>作者针对四种分类模型，测试了有目标攻击、无目标攻击分类准确率和置信度（COnfidence）结果，其中的置信度定义为对每一个成功的扰动，计算所有的目标类软标签概率，再除以成功扰动的总数，作为成功攻击的平均概率置信度值。   </p><p>考虑到One-Pixel方法的扰动像素数量，对于相对简单的模型该方法展现了较好的攻击效果，但对如AlexNet等较为复杂的模型攻击效果出现明显下降，且对更加复杂的分类器或检测器模型该方法的攻击鲁棒性有待探究。</p><p>此外作者还对不同模型和实验设置开展了多项消融实验，在此不再赘述。  </p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>该论文较早探索了限制扰动区域的攻击效果，但由于扰动范围较小并未获取较为理想的攻击结果，DE方法相较于Grad-CAM等融合模型信息获取图像关键位置的方法在攻击机理上仍有可挖掘的空间。   </p>]]></content>
    
    
    <summary type="html">&lt;div style=&quot;text-align: center;&quot;&gt; 
关于One-Pixel-Attack的论文笔记  
&lt;/div&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="对抗样本" scheme="http://boremycin.github.io/tags/%E5%AF%B9%E6%8A%97%E6%A0%B7%E6%9C%AC/"/>
    
  </entry>
  
  <entry>
    <title>论文笔记：SARATRX</title>
    <link href="http://boremycin.github.io/2025/01/14/paper1-saratrx/"/>
    <id>http://boremycin.github.io/2025/01/14/paper1-saratrx/</id>
    <published>2025-01-14T13:01:25.000Z</published>
    <updated>2025-01-14T13:14:13.811Z</updated>
    
    <content type="html"><![CDATA[<div style="text-align: center;"> 关于《SARATR-X: Towards Building A Foundation Model for SAR Target Recognition》的论文笔记  </div>  <span id="more"></span>  <h1 id="发表去向"><a href="#发表去向" class="headerlink" title="发表去向"></a>发表去向</h1><p>论文最终被TIP2025接收  </p><h1 id="主要内容"><a href="#主要内容" class="headerlink" title="主要内容"></a>主要内容</h1><h2 id="论文摘要总结"><a href="#论文摘要总结" class="headerlink" title="论文摘要总结"></a>论文摘要总结</h2><p>论文从SAR-ATR任务的基座模型（foundation models，FMs）缺失这一现状出发，提出了融合现有光学图像模型（HiViT），加入针对SAR图像任务优化的pre-train方法，通过无标签的自监督学习方式（self supervise learning，SSL）实现了较为理想的ATR性能。文中的ATR任务包括了SAR图像分类和SAR目标检测两大主流任务。此外，论文的另一主要贡献是整理了当前主流的SAR分类和检测数据集来作为基座模型SSL训练的预训练数据集（预训练过程不加入标签），并进行了大量的实验验证了多个主流模型的相应性能。  </p><p>个人理解的论文不足之处在于没有对模型本身有任何创新，大部分的贡献在于光学光学前沿方法的调研、整合和针对SAR图像downstream任务的微调。  </p><p>在模型规模仍与光学基座大模型有较大差距，模型微调过程中SAR训练数据量明显不足等问题外，该论文仍然是SAR基座大模型的第一篇相关研究，同时也成功实现了通过一个模型解决分类和检测两个SAR-ATR任务。   </p><h2 id="pre-train方法"><a href="#pre-train方法" class="headerlink" title="pre-train方法"></a>pre-train方法</h2><h3 id="自监督学习"><a href="#自监督学习" class="headerlink" title="自监督学习"></a>自监督学习</h3><p>设计针对SAR图像的SSL方法的难度要比针对光学图像更高，主要的原因是SAR图像成像过程中会受到斑点噪声（speckle noise）的影响，该噪声会导致图像中纹理和边缘信息的失真；此外SAR图像的语义信息也远不及光学图像丰富和独立。因此，作者提出SAR图像的SSL预训练的首要任务是提高图像质量。  </p><p>此外，针对ViT模型，做作者使用MIM（Masked Image Modeling）作为SSL方法。MIM通过对输入图像进行部分遮掩（mask），要求模型从未遮掩的部分中学习和推断被遮掩部分的信息，以此捕获图像的深层特征。MIM中模型被要求重建的内容被称为目标特征（Target Features），可以设置为原始的输入图像像素值，也可根据下游任务的特点与属性设置更高级特征。引导信号（Guided Signal）由目标特征生成，用于指导模型优化。  </p><p>在MIM中引导信号的选择上以往的论文已经由较为充分的研究和比较，主要的目标特征包括了CannyEdge、HOG、Haar-like，SAR-HOG和SAR-SIFT。本文作者则在以往工作SAR-JEPA的基础上，选择了较为简单的MGFs（Mutil-scale Gradient Features）来抑制斑点噪声与提取目标信息。    </p><p>由于SAR的成像机理，图像中的目标会包含乘性散点噪声（multiplicative speckle noise），导致目标，尤其是金属目标，周围会出现不同方向的强弱间纹理。因此，在特定区域中施加梯度比率（gradient ratio）可以提升模型的稳定性。  </p><h3 id="数据集准备"><a href="#数据集准备" class="headerlink" title="数据集准备"></a>数据集准备</h3><p>对于预训练过程，作者进行实验对比了只使用SAR数据集进行预训练与在光学大规模数据集开源数据集ImageNet上预训练后的权重基础之上在基于SAR数据集进行预训练两种情况。实验表明后者会有更好的表现。鉴于作者选用的基础模型为Transformer架构，使用ImageNet（包含约140万张图像，场景和类别相较于作者构建的SAR图像数据集都更加丰富）预训练权重来初始化可以实现注意力head分布更加分散，使模型的bottom layer学习的到信息更加多元并避免SAR图像中的斑点噪声对模型早期训练的影响。    </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/36b1b01fea8e48a46797142bca56c208" alt="36b1b01fea8e48a46797142bca56c208">  </p><figcaption style="text-align: center;">论文中列举的SAR数据集</figcaption>    <h3 id="MGF方法说明"><a href="#MGF方法说明" class="headerlink" title="MGF方法说明"></a>MGF方法说明</h3><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/34f876795c0d2fb37da23878d995cb6f" alt="34f876795c0d2fb37da23878d995cb6f">  </p><figcaption style="text-align: center;">论文二阶段预训练示意图</figcaption>  <script type="math/tex; mode=display">MGF = concat(G_{m1},G_{m2},G_{m3})</script><p>论文中采用的MGF方式是独立的数字图像处理方法，与HiViT模型没有任何关系，只挖掘了SAR图像中的信息。其中 $G_{mi}$ 之间的区别是对应的scale-kernel的大小 $\tau$ 不同，分别为9，13，17。从图中最左侧可以看出，选取 $M_i(1)$ 会提取横向信息；选取 $M_i(3)$ 会提取纵向信息,同理2，4会提取不同对角线方向的信息。M下标值只是说明从图像的不同起点构建卷积核。通过输入图像和4张固定的卷积核就能够获取图像中的梯度信息（不是图像在模型中的梯度信息！）  </p><p>假定输入的图像为单通道图像且进行填零操作，对于整个图像矩阵（不放设为m*n）而言，每个像素点坐标(a,b)都有对应的 $M_i(j)$ ，因此矩阵R、G大小均为均为m*n。  </p><p>即便MGF的思想和方法出现时间较早，其仍在论文中实现了满意的效果。    </p><h2 id="部分实验结果"><a href="#部分实验结果" class="headerlink" title="部分实验结果"></a>部分实验结果</h2><p>这里主要关注实验中的分类任务结果。对于分类任务，论文作者进行了1-shot，2-shot和5-shot等小样本实验，同时进行了SOC和EOC等操作来验证模型的泛化性能。</p><table>  <tr>    <td style="text-align: center;">      <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/52a6415778a08092ff2279bca8cf9d8c" alt="论文中列举的SAR数据集" style="max-height: 300px; width: auto;"">      <figcaption>实验结果1</figcaption>    </td>    <td style="text-align: center;">      <img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/9a0cb400ea8da3882a6027a0376c3420" alt="论文二阶段预训练示意图" style="max-height: 300px; width: auto;"">      <figcaption>实验结果2</figcaption>    </td>  </tr></table>  <p>实验结果显示文中的FM已经有了较好的SAR图像分类泛化性能</p>]]></content>
    
    
    <summary type="html">&lt;div style=&quot;text-align: center;&quot;&gt; 
关于《SARATR-X: Towards Building A Foundation Model for SAR Target Recognition》的论文笔记  
&lt;/div&gt;</summary>
    
    
    
    <category term="论文笔记" scheme="http://boremycin.github.io/categories/%E8%AE%BA%E6%96%87%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
    <category term="论文" scheme="http://boremycin.github.io/tags/%E8%AE%BA%E6%96%87/"/>
    
    <category term="SAR" scheme="http://boremycin.github.io/tags/SAR/"/>
    
  </entry>
  
  <entry>
    <title>建站记录</title>
    <link href="http://boremycin.github.io/2025/01/09/hello-world/"/>
    <id>http://boremycin.github.io/2025/01/09/hello-world/</id>
    <published>2025-01-09T13:42:14.262Z</published>
    <updated>2025-01-13T07:54:59.855Z</updated>
    
    <content type="html"><![CDATA[<p><div style="text-align: center;"> 这里简要记录我如何使用<a href="https://hexo.io/">Hexo</a>开源框架和<a href="https://github.com/next-theme/hexo-theme-next">NexT</a>主题结合<a href="https://pages.github.com/">Github Pages</a>搭建了第一个个人博客</div><br><span id="more"></span></p><h2 id="他山之石"><a href="#他山之石" class="headerlink" title="他山之石"></a>他山之石</h2><p>Hexo和Next都是相当成熟的博客模板并对中文有较好的支持，此外中文互联网上也有大量且详细的博客搭建指导贴。在这里就不再赘述从0到1的博客搭建与部署细节，而是列出对自己帮助较大，且更新日期较新的建站说明。    </p><ul><li><a href="https://blog.csdn.net/jj6666djdbbd/article/details/129321783">CSDN建站贴</a>，细致清晰！  </li><li><a href="https://hexo-next.readthedocs.io/zh-cn/latest/hexo/base/%E5%88%9D%E8%AF%86Hexo/">Hexo-Next中文文档</a>，不确定是否为官方文档，但涵盖了大部分内容的说明。</li><li><a href="https://zxblog.eu.org/posts/5a75/">高级说明</a>，Next博客博主的建站记录贴，记录了更多的高阶优化内容。  </li><li><a href="https://michaelxoxo.github.io/2019/05/19/hexo-blog-full-note/">博客说明</a>，另一位博主的hexo-next建站记录贴，额外给出了许多hexo博客写作技巧。</li><li><a href="https://hexo.io/docs/configuration.html">Hexo</a>与<a href="https://theme-next.js.org/docs/getting-started/">NexT</a>官方文档，最权威的建站参考资料。</li></ul><h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><p>在上述帖子的基础上博客搭建的大部分过程都非常顺利，但仍然有一些相对麻烦的部分，在此进行单独记录。  </p><h3 id="npm换源"><a href="#npm换源" class="headerlink" title="npm换源"></a>npm换源</h3><p>我在使用npm下载hexo时及时挂了梯子依然会很慢很卡，在换了对应的镜像源后丝滑解决问题！powershell换源脚本如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查询当前使用的下载源</span></span><br><span class="line">npm get registry  </span><br><span class="line"><span class="comment">#更换为淘宝源  </span></span><br><span class="line">npm config <span class="built_in">set</span> registry https://registry.npmmirror.com/</span><br></pre></td></tr></table></figure><h3 id="NexT主题下载与命名"><a href="#NexT主题下载与命名" class="headerlink" title="NexT主题下载与命名"></a>NexT主题下载与命名</h3><p>通过<code>git clone</code>下载相应仓库代码后，仓库名称为<code>hexo-theme-next</code>，设置hexo的config中的theme项为next后无法正确显示主题，这种情况下需要将该文件夹重命名为<code>next</code>，更改之后就可以正常调用NexT主题了。</p><h3 id="GitHub-Pages部署过程中的username设置问题"><a href="#GitHub-Pages部署过程中的username设置问题" class="headerlink" title="GitHub Pages部署过程中的username设置问题"></a>GitHub Pages部署过程中的username设置问题</h3><p>我原本GitHub的username为<code>YM-ZHANG-Adv</code>，在以此为基础进行GitHub Pages部署的结果就是创建的仓库为<code>YM-ZHANG-Adv.github.io</code>，而实际浏览器读取的地址为<code>ym-zhang-adv.github.io</code>，虽然这个问题可能有解决的办法，但我还是不想再多花时间解决这个问题，而且自己的第一个username确实蠢得离谱。后续就用了一个自己在其他平台用了好久的username<code>Boremycin</code>来当作GitHub的username，一通改名之后顺利完成了博客网站部署。  </p><p>另外，改username是一件非常需要谨慎考虑的事情，很多依赖github username但容易被遗忘的链接都需要重写，例如我作为图床的PicGO。  </p><h3 id="Markdown中的Latex渲染"><a href="#Markdown中的Latex渲染" class="headerlink" title="Markdown中的Latex渲染"></a>Markdown中的Latex渲染</h3><p>Hexo的原生Latex数学公式渲染器并不友好，与常用的md数学语言相比差距较大，因此需要重新设置相应的渲染器和部署文件。以下内容基本按照<a href="https://readmengk90.github.io/2024/12/15/Hexo+Next%E7%9A%84Typora%E6%95%B0%E5%AD%A6%E5%85%AC%E5%BC%8F%E6%98%BE%E7%A4%BA%E9%97%AE%E9%A2%98/">该博客贴</a>进行优化。  </p><p>首先，Hexo默认使用”hexo-renderer-marked”引擎渲染网页，该引擎会把一些特殊的markdown符号转换为相应的html标签，比如在markdown语法中，下划线 <code>_</code> 代表斜体，会被渲染引擎处理为<code>&lt;em&gt;</code>标签。因为类Latex格式书写的数学公式下划线 <code>_</code> 表示下标，有特殊的含义，如果被强制转换为<code>&lt;em&gt;</code>标签，那么MathJax引擎在渲染数学公式的时候就会出错。例如$x_i$在开始被渲染的时候，处理为 $x<em>i</em>$，这样MathJax引擎就认为该公式有语法错误，因为不会渲染。类似的语义冲突的符号还包括 <code>*</code>, <code>&#123;</code>, <code>&#125;</code>, <code>\</code>等。  </p><p>通过更换markdown中的latex渲染引擎可以轻易地解决该问题，这次采用hexo-renderer-kramed引擎。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save <span class="comment">#卸载原引擎，否侧可能会出现错误</span></span><br><span class="line">npm install hexo-renderer-kramed --save</span><br></pre></td></tr></table></figure><p>但hexo-renderer-kramed引擎也存在行内冲突的问题，因此需要对相应的js文件做进一步修改，目标文件的路径为<code>blog/node_modules\kramed\lib\rules\inline.js</code>，更改其中的部分代码。</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//  escape: /^\\([\\`*&#123;&#125;\[\]()#$+\-.!_&gt;])/,</span></span><br><span class="line">  <span class="attr">escape</span>: <span class="regexp">/^\\([`*\[\]()#$+\-.!_&gt;])/</span></span><br><span class="line"><span class="comment">//  em: /^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/,</span></span><br><span class="line">  <span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span></span><br></pre></td></tr></table></figure><p>最后，在主题中开启mathjex开关，进入到<code>theme/next/_config.yml</code>，将mathjax项的默认false改为true即可。  </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">MathJax</span></span><br><span class="line">mathjax:</span><br><span class="line">    enable: true</span><br><span class="line">    per_page: true</span><br></pre></td></tr></table></figure><p>完成上述配置后，需要在post的md文件开头手动选择是否开启该文章的latex渲染。鉴于可能会需要写较多的数学公式，因此希望每个文章都默认渲染，将上述yml配置文件中math选项的per_page改为true即可实现，源代码中也给出了相应的注释说明。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">math:</span><br><span class="line">    # Default (true) will load mathjax / katex script on demand.</span><br><span class="line">    # That is it only render those page which has `mathjax: true` in Front-matter.</span><br><span class="line">    # If you set it to false, it will load mathjax / katex srcipt EVERY PAGE.</span><br><span class="line">    per_page: false</span><br></pre></td></tr></table></figure><h3 id="实现文章置顶"><a href="#实现文章置顶" class="headerlink" title="实现文章置顶"></a>实现文章置顶</h3><p>原始的主题模板提供了<code>sticky</code>的置顶功能，但再三摸索后仍没有成功实现，因此选择改动模板实现文章置顶功能。  </p><p>首先，需要更改hexo的generate插件，卸载原有的index插件并安装新插件，脚本如下：  </p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm uninstall hexo-generator-index --save</span><br><span class="line">npm install hexo-generator-index-pin-top --save</span><br></pre></td></tr></table></figure><p>接着，修改<code>blog/node_modules/hexo-generator-index/lib/generator.js</code>中的内容，可直接复制替换的完整代码如下：  </p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&#x27;use strict&#x27;</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">var</span> pagination = <span class="built_in">require</span>(<span class="string">&#x27;hexo-pagination&#x27;</span>);</span><br><span class="line"></span><br><span class="line"><span class="variable language_">module</span>.<span class="property">exports</span> = <span class="keyword">function</span>(<span class="params">locals</span>) &#123;</span><br><span class="line">  <span class="keyword">var</span> config = <span class="variable language_">this</span>.<span class="property">config</span>;</span><br><span class="line">  <span class="keyword">var</span> posts = locals.<span class="property">posts</span>.<span class="title function_">sort</span>(config.<span class="property">index_generator</span>.<span class="property">order_by</span>);</span><br><span class="line"></span><br><span class="line">  posts.<span class="property">data</span> = posts.<span class="property">data</span>.<span class="title function_">sort</span>(<span class="keyword">function</span>(<span class="params">a, b</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span>(a.<span class="property">top</span> &amp;&amp; b.<span class="property">top</span>) &#123;</span><br><span class="line">          <span class="keyword">if</span>(a.<span class="property">top</span> == b.<span class="property">top</span>) <span class="keyword">return</span> b.<span class="property">date</span> - a.<span class="property">date</span>;</span><br><span class="line">          <span class="keyword">else</span> <span class="keyword">return</span> b.<span class="property">top</span> - a.<span class="property">top</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(a.<span class="property">top</span> &amp;&amp; !b.<span class="property">top</span>) &#123;</span><br><span class="line">          <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">if</span>(!a.<span class="property">top</span> &amp;&amp; b.<span class="property">top</span>) &#123;</span><br><span class="line">          <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">else</span> <span class="keyword">return</span> b.<span class="property">date</span> - a.<span class="property">date</span>;</span><br><span class="line">  &#125;);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">var</span> paginationDir = config.<span class="property">pagination_dir</span> || <span class="string">&#x27;page&#x27;</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="title function_">pagination</span>(<span class="string">&#x27;&#x27;</span>, posts, &#123;</span><br><span class="line">    <span class="attr">perPage</span>: config.<span class="property">index_generator</span>.<span class="property">per_page</span>,</span><br><span class="line">    <span class="attr">layout</span>: [<span class="string">&#x27;index&#x27;</span>, <span class="string">&#x27;archive&#x27;</span>],</span><br><span class="line">    <span class="attr">format</span>: paginationDir + <span class="string">&#x27;/%d/&#x27;</span>,</span><br><span class="line">    <span class="attr">data</span>: &#123;</span><br><span class="line">      <span class="attr">__index</span>: <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>然后，需要在主题模板中添加置顶标志，修改<code>/blog/themes/next/layout/_macro/post.swig</code>文件，在<code>&lt;div class=&quot;post-meta&quot;&gt;</code>标签下插入如下代码：  </p><p><img src="https://raw.githubusercontent.com/boremycin/ImageBed/master/f4bb7b9b0d5953981c43dd5704cb22a7" alt="f4bb7b9b0d5953981c43dd5704cb22a7"></p><p>完成上述步骤后，只需在每篇文章的Front-matter中加入<code>top: true</code>或<code>top: x</code>，即可实现文章的置顶。  </p><h2 id="回顾与总结"><a href="#回顾与总结" class="headerlink" title="回顾与总结"></a>回顾与总结</h2><div class="note [default] [icon]">            <p>我为什么需要一个博客？ </p>          </div>  <p>这个博客可能是本应一年前就该完成的工作，当时还是大三的自己已经收藏了不知道多少个大佬的博客，也懵懵懂懂写了一些代码，自以为可以像完成课程PJ一样完成这么一个项目，但一开始就踏入了<code>Django</code>的无底洞，几乎是从零开始手搓前端，这显然不是我希望的结果，于是只能将其搁置。过了大三大四两学期整整一年，才想起来自己还曾经有这么个念想，于是在寒假马不停蹄地把这个博客搭了起来（当然绝大部分是依赖现有的成熟模板）。可能这就是<em>念念不忘，必有回响</em> 吧。  </p><p>此外，我也希望通过这个博客改改自己好高骛远，眼高手低的毛病，后面读了什么论文或者跑了什么代码和算法，都会在这个博客上留下记录，而不是在脑子里过一遍了事，这个博客既是一个公开的笔记，也是一种督促。  </p><p>再从功利的角度出发，有一个不断更新的技术博客本身就是互联网求职的加分项，加分权重依照博客维护者的年龄或者学历层次逐渐递减。抛开内容不谈，一个完成度高的高中生博客给人的印象深刻程度一定是远远大于一个博士研究生的博客的，所以希望抓住自己本科的尾巴，把自己的博客搭起来，这样就可以在简历里大肆宣扬<del>本科阶段建立个人博客并不断维护</del>。    </p><p>最后如果真的要回答搭建博客到底对自己的学习，对当下的任务有什么促进的话，那显然是没有的。不过自己还是本科，这就是所有的<strong>自由而无用</strong>的事情发生的充分理由。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;div style=&quot;text-align: center;&quot;&gt; 
这里简要记录我如何使用&lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;开源框架和&lt;a href=&quot;https://github.com/next-theme/hexo-theme-next&quot;&gt;NexT&lt;/a&gt;主题结合&lt;a href=&quot;https://pages.github.com/&quot;&gt;Github Pages&lt;/a&gt;搭建了第一个个人博客
&lt;/div&gt;&lt;br&gt;</summary>
    
    
    
    <category term="笔记" scheme="http://boremycin.github.io/categories/%E7%AC%94%E8%AE%B0/"/>
    
    
    <category term="建站" scheme="http://boremycin.github.io/tags/%E5%BB%BA%E7%AB%99/"/>
    
    <category term="笔记" scheme="http://boremycin.github.io/tags/%E7%AC%94%E8%AE%B0/"/>
    
  </entry>
  
</feed>
